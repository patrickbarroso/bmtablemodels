{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DETR Table Detection\n",
        "https://huggingface.co/TahaDouaji/detr-doc-table-detection\n",
        "\n",
        "https://arxiv.org/abs/2005.12872\n"
      ],
      "metadata": {
        "id": "12dSZ0KsBIWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções Pré-processamento para MAIN"
      ],
      "metadata": {
        "id": "Uj5n8iIR0cdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uVHIX_mHBL3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88be4a6-4b12-449e-a9cf-325731c00e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install transformers torch torchvision matplotlib pytesseract\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!pip install timm\n",
        "!pip install pdf2image\n",
        "!pip install pymupdf\n",
        "!apt-get install poppler-utils\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "hL18HoELBN19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bibliotecas\n",
        "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
        "import torch\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from huggingface_hub import hf_hub_download\n",
        "import fitz\n",
        "import os\n",
        "import pandas as pd\n",
        "from pdf2image import convert_from_path\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv\n",
        "from tqdm.auto import tqdm\n",
        "import cv2\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from pytesseract import Output\n",
        "import pytesseract\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import shutil\n",
        "import pytz\n",
        "import random\n",
        "from datetime import datetime\n",
        "try:\n",
        "    from PIL import Image #esse problema de import da classe Image deve ser considerado para demais modelos\n",
        "except ImportError:\n",
        "    import Image\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "EZTt2a960qW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#caminho dos certificados para analise\n",
        "CERT_PATH = \"/content/drive/MyDrive/DataSets/Certificados/In/\"\n",
        "\n",
        "#caminho de saida para geração das anotações das tabelas\n",
        "OUT_PATH = \"/content/drive/MyDrive/DataSets/Certificados/Out/Test/\"\n",
        "\n",
        "#diretorio dos arquivos do GT - Ground Truth (REFERENCIA para comparacao das tabelas)\n",
        "GT_PATH = \"/content/drive/MyDrive/DataSets/Certificados/Out/GT/\"\n",
        "\n",
        "#diretorio de escrita de arquivos\n",
        "DIROUT_DETR = \"/content/drive/MyDrive/DataSets/Certificados/Out/DETR/\""
      ],
      "metadata": {
        "id": "uZOzEdcX0sWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#funcao para varrer os arquivos PDF dos laboratórios nas pastas e retorna um dataframe\n",
        "#! o método está bem lento (deve verificar o motivo posteriormente)\n",
        "def InfoPDF(pathPDF):\n",
        "\n",
        "  lenPages = 0\n",
        "  doc = fitz.open(pathPDF)\n",
        "\n",
        "  lenPages = len(doc)\n",
        "  for pagina in doc:\n",
        "    isText = bool(pagina.get_text())\n",
        "    break\n",
        "\n",
        "  typeArq = [\"TEXT\" if isText else \"IMAGE\"]\n",
        "\n",
        "  return typeArq, lenPages\n",
        "\n",
        "def listFiles_OLD(CERT_PATH):\n",
        "\n",
        "  dfArq = pd.DataFrame()\n",
        "  dirs = [nome for nome in os.listdir(CERT_PATH) if os.path.isdir(os.path.join(CERT_PATH, nome))]\n",
        "\n",
        "  i = 0\n",
        "  for dir in dirs:\n",
        "\n",
        "    #coletando dados do diretorio (id, laboratorio)\n",
        "    lstDir = dir.split(\"_\")\n",
        "\n",
        "    if(len(lstDir)==3):\n",
        "\n",
        "      for file in os.listdir(os.path.join(CERT_PATH, dir)):\n",
        "\n",
        "        #é arquivo PDF\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "\n",
        "          pathFile = CERT_PATH + dir + \"/\" + file\n",
        "          dfArq.at[i,\"LAB\"] = lstDir[2]\n",
        "          dfArq.at[i,'PATH'] = pathFile\n",
        "\n",
        "          typeArq, qtdPages = InfoPDF(pathFile)\n",
        "          dfArq.at[i,'TYPE'] = typeArq\n",
        "          dfArq.at[i,'QTDPAGES'] = qtdPages\n",
        "\n",
        "          i = i + 1\n",
        "\n",
        "  return dfArq\n",
        "\n",
        "def listFiles(CERT_PATH, LAB_PATH):\n",
        "\n",
        "  dfArq = pd.DataFrame()\n",
        "\n",
        "  if LAB_PATH is None:\n",
        "    dirs = [nome for nome in os.listdir(CERT_PATH) if os.path.isdir(os.path.join(CERT_PATH, nome))]\n",
        "  else:\n",
        "    dirs = [LAB_PATH]\n",
        "\n",
        "  i = 0\n",
        "  for dir in dirs:\n",
        "\n",
        "    #coletando dados do diretorio (id, laboratorio)\n",
        "    lstDir = dir.split(\"_\")\n",
        "    print(\"lstDir \", lstDir);\n",
        "\n",
        "    if(len(lstDir)==3):\n",
        "\n",
        "      for file in os.listdir(os.path.join(CERT_PATH, dir)):\n",
        "\n",
        "        #é arquivo PDF\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "\n",
        "          pathFile = CERT_PATH + dir + \"/\" + file\n",
        "          print(\"pathFile \", pathFile);\n",
        "          dfArq.at[i,\"LAB\"] = lstDir[2]\n",
        "          dfArq.at[i,'PATH'] = pathFile\n",
        "\n",
        "          typeArq, qtdPages = InfoPDF(pathFile)\n",
        "          dfArq.at[i,'TYPE'] = typeArq\n",
        "          dfArq.at[i,'QTDPAGES'] = qtdPages\n",
        "\n",
        "          i = i + 1\n",
        "\n",
        "  return dfArq\n",
        "\n",
        "#dfArq = listFiles(CERT_PATH)\n",
        "\n",
        "def deleteFiles2(dirpath):\n",
        "  # Obtém a lista de arquivos no diretório\n",
        "  files = os.listdir(dirpath)\n",
        "\n",
        "  # Itera sobre os arquivos e os remove\n",
        "  for file in files:\n",
        "    filepath = os.path.join(dirpath, file)\n",
        "    if os.path.isfile(filepath):\n",
        "      os.remove(filepath)\n",
        "\n",
        "def is_image_by_extension(file_path):\n",
        "  image_extensions = ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'webp']  # Adicione outras extensões se necessário\n",
        "  file_extension = file_path.lower().split('.')[-1]\n",
        "  return file_extension in image_extensions\n",
        "\n",
        "def is_image(file_path):\n",
        "    try:\n",
        "        # Tenta abrir o arquivo como uma imagem\n",
        "        Image(file_path)\n",
        "        return True\n",
        "    except IOError:\n",
        "        # Se não for possível abrir como uma imagem, retorna False\n",
        "        return False\n",
        "\n",
        "def pdf_page_to_png(pdf_path, page_number, output_path):\n",
        "  # Convertendo a página do PDF para uma lista de imagens\n",
        "  images = convert_from_path(pdf_path, first_page=page_number, last_page=page_number)\n",
        "\n",
        "  # Salvando a imagem como PNG\n",
        "  images[0].save(output_path, 'PNG')\n",
        "\n",
        "class MaxResize(object):\n",
        "  def __init__(self, max_size=800):\n",
        "      self.max_size = max_size\n",
        "\n",
        "  def __call__(self, image):\n",
        "      width, height = image.size\n",
        "      current_max_size = max(width, height)\n",
        "      scale = self.max_size / current_max_size\n",
        "      resized_image = image.resize((int(round(scale*width)), int(round(scale*height))))\n",
        "\n",
        "      return resized_image\n",
        "\n",
        "def get_cell_coordinates_by_row(table_data):\n",
        "  # Extract rows and columns\n",
        "  rows = [entry for entry in table_data if entry['label'] == 'table row']\n",
        "  columns = [entry for entry in table_data if entry['label'] == 'table column']\n",
        "\n",
        "  # Sort rows and columns by their Y and X coordinates, respectively\n",
        "  rows.sort(key=lambda x: x['bbox'][1])\n",
        "  columns.sort(key=lambda x: x['bbox'][0])\n",
        "\n",
        "  # Function to find cell coordinates\n",
        "  def find_cell_coordinates(row, column):\n",
        "      cell_bbox = [column['bbox'][0], row['bbox'][1], column['bbox'][2], row['bbox'][3]]\n",
        "      return cell_bbox\n",
        "\n",
        "  # Generate cell coordinates and count cells in each row\n",
        "  cell_coordinates = []\n",
        "\n",
        "  for row in rows:\n",
        "      row_cells = []\n",
        "      for column in columns:\n",
        "          cell_bbox = find_cell_coordinates(row, column)\n",
        "          row_cells.append({'column': column['bbox'], 'cell': cell_bbox})\n",
        "\n",
        "      # Sort cells in the row by X coordinate\n",
        "      row_cells.sort(key=lambda x: x['column'][0])\n",
        "\n",
        "      # Append row information to cell_coordinates\n",
        "      cell_coordinates.append({'row': row['bbox'], 'cells': row_cells, 'cell_count': len(row_cells)})\n",
        "\n",
        "  # Sort rows from top to bottom\n",
        "  cell_coordinates.sort(key=lambda x: x['row'][1])\n",
        "\n",
        "  return cell_coordinates\n",
        "\n",
        "def aumentar_qualidade_e_contraste(imagem_path, fator_contraste, fator_brilho):\n",
        "  # Carregar a imagem\n",
        "  imagem = cv2.imread(imagem_path)\n",
        "\n",
        "  # Converter a imagem para o espaço de cores LAB (Luminância, Azul, Vermelho)\n",
        "  lab = cv2.cvtColor(imagem, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "  # Separar os canais L, A, B\n",
        "  l, a, b = cv2.split(lab)\n",
        "\n",
        "  # Aplicar o aumento de contraste na imagem L (luminância)\n",
        "  l = cv2.add(l, fator_brilho)\n",
        "  l = cv2.multiply(l, fator_contraste)\n",
        "\n",
        "  # Mesclar novamente os canais LAB\n",
        "  lab = cv2.merge((l, a, b))\n",
        "\n",
        "  # Converter a imagem de volta para o espaço de cores BGR\n",
        "  imagem_contraste = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "  return imagem_contraste"
      ],
      "metadata": {
        "id": "kb_WvFey00kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNÇÕES DE MANIPULACAO DE IMAGENS E ARQUIVOS\n",
        "\n",
        "#ler dados da celula\n",
        "def apply_ocr(cell_coordinates):\n",
        "    # let's OCR row by row\n",
        "    data = dict()\n",
        "    max_num_columns = 0\n",
        "    for idx, row in enumerate(tqdm(cell_coordinates)):\n",
        "      row_text = []\n",
        "      for cell in row[\"cells\"]:\n",
        "        # crop cell out of image\n",
        "        cell_image = np.array(cropped_table.crop(cell[\"cell\"]))\n",
        "        # apply OCR\n",
        "        result = reader.readtext(np.array(cell_image))\n",
        "        if len(result) > 0:\n",
        "          # print([x[1] for x in list(result)])\n",
        "          text = \" \".join([x[1] for x in result])\n",
        "          row_text.append(text)\n",
        "\n",
        "      if len(row_text) > max_num_columns:\n",
        "          max_num_columns = len(row_text)\n",
        "\n",
        "      data[idx] = row_text\n",
        "\n",
        "    #print(\"Max number of columns:\", max_num_columns)\n",
        "\n",
        "    # pad rows which don't have max_num_columns elements\n",
        "    # to make sure all rows have the same number of columns\n",
        "    for row, row_data in data.copy().items():\n",
        "        if len(row_data) != max_num_columns:\n",
        "          row_data = row_data + [\"\" for _ in range(max_num_columns - len(row_data))]\n",
        "        data[row] = row_data\n",
        "\n",
        "    return data\n",
        "\n",
        "#similaridade de strings conhecido como \"Distância de Levenshtein\"\n",
        "def calcPercSimStrings(str1, str2):\n",
        "\n",
        "  #retirando quebra de linhas da string\n",
        "  str1 = str1.replace(\"\\n\", \" \")\n",
        "  str2 = str2.replace(\"\\n\", \" \")\n",
        "\n",
        "  tamanho_str1 = len(str1)\n",
        "  tamanho_str2 = len(str2)\n",
        "\n",
        "  matriz = [[0] * (tamanho_str2 + 1) for _ in range(tamanho_str1 + 1)]\n",
        "\n",
        "  for i in range(tamanho_str1 + 1):\n",
        "    matriz[i][0] = i\n",
        "\n",
        "  for j in range(tamanho_str2 + 1):\n",
        "    matriz[0][j] = j\n",
        "\n",
        "  for i in range(1, tamanho_str1 + 1):\n",
        "    for j in range(1, tamanho_str2 + 1):\n",
        "        if str1[i - 1] == str2[j - 1]:\n",
        "            custo_substituicao = 0\n",
        "        else:\n",
        "            custo_substituicao = 1\n",
        "        matriz[i][j] = min(matriz[i - 1][j] + 1,       # Deletar\n",
        "                            matriz[i][j - 1] + 1,       # Inserir\n",
        "                              matriz[i - 1][j - 1] + custo_substituicao)  # Substituir\n",
        "\n",
        "  distancia = matriz[tamanho_str1][tamanho_str2]\n",
        "  maximo_tamanho = max(tamanho_str1, tamanho_str2)\n",
        "\n",
        "  similaridade = 0\n",
        "  if maximo_tamanho > 0:\n",
        "    similaridade = (maximo_tamanho - distancia) / maximo_tamanho\n",
        "  #print (\" similaridade entre {0} e {1}: {2}\".format(str1, str2, similaridade * 100))\n",
        "  return similaridade * 100\n",
        "\n",
        "#funcao que compara o valor de duas listas e calcula a media do percentual de similaridade entre eles\n",
        "#(para calcular o valor do bbox das tabelas e células das tabelas)\n",
        "\n",
        "def calcPercSimValueLists(lista1, lista2):\n",
        "  if len(lista1) != len(lista2):\n",
        "      print(\"calcPercSimValueLists, listas de tamanhos diferentes, lista1=\",lista1,\"/ lista2 = \",lista2)\n",
        "      raise ValueError(\"As listas devem ter o mesmo comprimento.\")\n",
        "\n",
        "  percSim = [ (1 / (1 + (abs(num1 - num2)))) * 100 for num1, num2 in zip(lista1, lista2)]\n",
        "\n",
        "\n",
        "  return sum(percSim) / len (percSim)\n",
        "\n",
        "#(para calcular o percentual de similaridade entre dois números\n",
        "def calcPercSimValueNums(num1, num2):\n",
        "\n",
        "  percSim = (1 / (1 + (abs(num1 - num2)))) * 100\n",
        "  #print (\" similaridade entre os numeros {0} e {1}: {2}\".format(num1, num2, percSim))\n",
        "  return percSim\n",
        "\n",
        "#coletar os arquivos de acordo com premissas (prefixo e sufixo)\n",
        "def getFilesByPrefix(path, prefix, sufix):\n",
        "  lstFiles = []\n",
        "  for fileName in os.listdir(path):\n",
        "      if prefix in fileName and fileName.endswith(sufix):\n",
        "          lstFiles.append(fileName)\n",
        "  return lstFiles\n",
        "\n",
        "#coletar no arquivo do GT dados das tabelas de uma determinada pagina\n",
        "def getListFilesGTInfo(curfile, page, path):\n",
        "\n",
        "  prefix = curfile + \"|\" + str(page)\n",
        "  sufix = \"_INFO.info\"\n",
        "  lstTables = getFilesByPrefix (path, prefix, sufix)\n",
        "\n",
        "  return lstTables\n",
        "\n",
        "def getGTInfo(tableID, curfile, page, path):\n",
        "\n",
        "  prefix = tableID + \"|\" + curfile + \"|\" + str(page)\n",
        "  sufix = \"_INFO.info\"\n",
        "  lstTables = getFilesByPrefix (path, prefix, sufix)\n",
        "\n",
        "  return lstTables\n",
        "\n",
        "#coletar as informacoes da tabela do arquivo _INFO e retornar para uma lista\n",
        "def getListTablesInfo(path, listFiles):\n",
        "\n",
        "  listTablesInfo = []\n",
        "  for fileName in listFiles:\n",
        "    with open(path + fileName, 'r') as file:\n",
        "      conteudo = file.read()\n",
        "      listTablesInfo.append(eval(conteudo))\n",
        "\n",
        "  return listTablesInfo\n",
        "\n",
        "# verificar se a estrutura dos dois dicionários INFO são similares\n",
        "def checkDimensionINFO(dicTableGT, dicTable):\n",
        "\n",
        "  msgErro = \"0 - SUCESSO\"\n",
        "  isIdentical = True\n",
        "\n",
        "  #se o tamanho das chaves dos dicionários são diferentes\n",
        "  if(dicTableGT.keys() != dicTable.keys()):\n",
        "    print(\"Dicionários não possuem o mesmo indice\")\n",
        "    msgErro = \"1 - Dicionários não possuem o mesmo indice\"\n",
        "    isIdentical = False\n",
        "\n",
        "  #se não tiver a mesma dimensao já descarta\n",
        "  if dicTableGT[\"DIMENSION\"] != dicTable[\"DIMENSION\"]:\n",
        "    msgErro = \"2 - Dicionários não possuem a mesma dimensão\"\n",
        "    isIdentical = False\n",
        "\n",
        "  #se o tamanho das colunas HEAD e FIRST_LINE não batem\n",
        "  if(len(dicTableGT[\"HEAD\"]) != len(dicTable[\"HEAD\"])):\n",
        "    print(\"Dicionários não possuem o mesmo tamanho da chave HEAD\")\n",
        "    msgErro = \"3 - Dicionários não possuem o mesmo tamanho da chave HEAD\"\n",
        "    isIdentical = False\n",
        "\n",
        "  if(len(dicTableGT[\"FIRST_LINE\"]) != len(dicTable[\"FIRST_LINE\"])):\n",
        "    msgErro = \"4 - Dicionários não possuem o mesmo tamanho da chave FIRST_LINE\"\n",
        "    isIdentical = False\n",
        "\n",
        "  #return msgErro, isSimilar\n",
        "  return isIdentical\n",
        "\n",
        "#calcular similaridade da string das células dos dicionários INFO de mesma dimensão\n",
        "#(para determinar se dois dicionarios INFO sao similares)\n",
        "def getPercSimTablesINFO(dicTableGT, dicTable , percTolerancia):\n",
        "\n",
        "  i = 0\n",
        "  for textTableGT in dicTableGT[\"HEAD\"]:\n",
        "    textTable = dicTable[\"HEAD\"][i]\n",
        "    if (calcPercSimStrings(textTableGT, textTable) < percTolerancia):\n",
        "      print(\"Tolerancia entre string \", textTableGT, \"e \", textTable,  \" menor que \", percTolerancia)\n",
        "      return False\n",
        "    i+=1\n",
        "\n",
        "  i = 0\n",
        "  for textTableGT in dicTableGT[\"FIRST_LINE\"]:\n",
        "    textTable = dicTable[\"FIRST_LINE\"][i]\n",
        "    if (calcPercSimStrings(textTableGT, textTable) < percTolerancia):\n",
        "      print(\"Tolerancia entre string \", textTableGT, \"e \", textTable,  \" menor que \", percTolerancia)\n",
        "      return False\n",
        "    i+=1\n",
        "\n",
        "  return True\n",
        "\n",
        "\n",
        "#verifica se a dimensão dos dicionários INFO possuem tamanhos parecidos (maximo 1 de diferença)\n",
        "def isAlmostSimilarINFO(dicTableGT, dicTable):\n",
        "\n",
        "  #devem possuir a mesma quantidade de linhas e quantidade semelhante de colunas (maximo no modulo 1)\n",
        "\n",
        "  #colunas\n",
        "  qtdColGT = len(dicTableGT[\"HEAD\"])\n",
        "  qtdColTab = len(dicTable[\"HEAD\"])\n",
        "\n",
        "  #linhas\n",
        "  qtdLinhasGT = dicTableGT[\"DIMENSION\"].split(\"X\")[0]\n",
        "  qtdLinhas = dicTable[\"DIMENSION\"].split(\"X\")[0]\n",
        "\n",
        "  if abs(qtdColGT-qtdColTab) <=1 and qtdLinhasGT == qtdLinhas:\n",
        "    return True\n",
        "\n",
        "  return False\n",
        "\n",
        "#verificar maior valor na lista\n",
        "def maiorValor(lista):\n",
        "\n",
        "  maiorValor = 0\n",
        "  for item in lista:\n",
        "    if item > maiorValor:\n",
        "        maiorValor = item\n",
        "\n",
        "  return maiorValor\n",
        "\n",
        "#verificar possivel similaridade no cabeçalho dos dicionários INFO\n",
        "def checkAVGSimilaritiesINFO(dicTableGT, dicTable, percTolerancia):\n",
        "\n",
        "  qtdColGT = len(dicTableGT[\"HEAD\"])\n",
        "  qtdColTab = len(dicTable[\"HEAD\"])\n",
        "\n",
        "  arrSim = []\n",
        "  arrSummary = []\n",
        "  limit = 4\n",
        "\n",
        "  #verificar similaridade nas 3 primeiras colunas\n",
        "  for i in range(qtdColTab):\n",
        "\n",
        "    for j in range(qtdColGT):\n",
        "      strTab = dicTable[\"HEAD\"][i]\n",
        "      strGT = dicTableGT[\"HEAD\"][j]\n",
        "      arrSim.append(calcPercSimStrings(strTab, strGT))\n",
        "\n",
        "    arrSummary.append(maiorValor(arrSim))\n",
        "    arrSim = []\n",
        "\n",
        "    if(i>= limit):\n",
        "      break\n",
        "\n",
        "  #print(arrSummary)\n",
        "  avgPerc = sum(arrSummary) / len(arrSummary)\n",
        "  return avgPerc >= percTolerancia\n",
        "\n",
        "\n",
        "#verificar possivel similaridade no cabeçalho dos dicionários FIRST_LINE\n",
        "def checkAVGSimilarities2INFO(dicTableGT, dicTable, percTolerancia):\n",
        "\n",
        "  qtdColGT = len(dicTableGT[\"FIRST_LINE\"])\n",
        "  qtdColTab = len(dicTable[\"FIRST_LINE\"])\n",
        "\n",
        "  arrSim = []\n",
        "  arrSummary = []\n",
        "  limit = 3\n",
        "\n",
        "  #verificar similaridade nas 3 primeiras colunas\n",
        "  for i in range(qtdColTab):\n",
        "\n",
        "    for j in range(qtdColGT):\n",
        "      strTab = dicTable[\"FIRST_LINE\"][i]\n",
        "      strGT = dicTableGT[\"FIRST_LINE\"][j]\n",
        "      arrSim.append(calcPercSimStrings(strTab, strGT))\n",
        "\n",
        "    arrSummary.append(maiorValor(arrSim))\n",
        "    arrSim = []\n",
        "\n",
        "    if(i>= limit):\n",
        "      break\n",
        "\n",
        "  #print(arrSummary)\n",
        "  avgPerc = sum(arrSummary) / len(arrSummary)\n",
        "  return avgPerc >= percTolerancia\n",
        "\n",
        "def getDicTableInfo(labName, curfile, page, qtdlinhas, qtdcolunas, bbox, head, firstLine):\n",
        "  dicTableInfo = {}\n",
        "\n",
        "  dicTableInfo[\"LAB\"] = labName\n",
        "  dicTableInfo[\"FILE\"] = curfile\n",
        "  dicTableInfo[\"PAGE\"] = page\n",
        "  dicTableInfo[\"TABLEID\"] = \"TBD\"\n",
        "  dicTableInfo[\"DIMENSION\"] = str(qtdlinhas) + \"X\" + str(qtdcolunas)\n",
        "  dicTableInfo[\"BBOX\"] = bbox\n",
        "  dicTableInfo[\"HEAD\"] = head\n",
        "  dicTableInfo[\"FIRST_LINE\"] = firstLine\n",
        "\n",
        "  return dicTableInfo\n",
        "\n",
        "def SaveDicTableInfo (filePath, dicTableInfo):\n",
        "\n",
        "  strFile = \"{\"\n",
        "  lenDic = len(dicTableInfo.items())\n",
        "  #print(lenDic)\n",
        "  i = 0\n",
        "  for chave, valor in dicTableInfo.items():\n",
        "    vir = \",\" if i < lenDic-1 else \"\"\n",
        "    if type(valor) == str:\n",
        "      strFile += \"'\" + str(chave)+ \"':'\" + str(valor) + \"'\" + vir + \"\\n\"\n",
        "    else:\n",
        "      strFile += \"'\" + str(chave) + \"':\" + str(valor) + vir + \"\\n\"\n",
        "    i = i + 1\n",
        "\n",
        "  strFile += \"}\"\n",
        "  with open(filePath, 'w') as arquivo:\n",
        "    arquivo.write(strFile)\n",
        "\n",
        "#numero de ocorrencias de um numero em uma lista\n",
        "def numTimes(list, num):\n",
        "\n",
        "  cont = 0\n",
        "  for valor in list:\n",
        "    if valor == num:\n",
        "      cont+=1\n",
        "\n",
        "  return cont\n",
        "\n",
        "def numDecimals(list):\n",
        "\n",
        "  cont = 0\n",
        "  for valor in list:\n",
        "    if isDecimal(valor):\n",
        "      cont+=1\n",
        "\n",
        "  return cont\n",
        "\n",
        "def numNotDecimals(list):\n",
        "\n",
        "  cont = 0\n",
        "  for valor in list:\n",
        "    if not isDecimal(valor):\n",
        "      cont+=1\n",
        "\n",
        "  return cont\n",
        "\n",
        "def isDecimal(valor):\n",
        "  try:\n",
        "\n",
        "    if valor == \"NAN\" or valor == \"nan\" or valor is None:\n",
        "      return False\n",
        "    else:\n",
        "      valor = valor.replace(\",\",\".\")\n",
        "      float(valor)\n",
        "      return True\n",
        "  except ValueError:\n",
        "      return False\n",
        "\n",
        "def noteTokensHTML(df):\n",
        "\n",
        "  lsthead = []\n",
        "  lsttd = []\n",
        "  hasheader = False\n",
        "  hasdata = False\n",
        "  dict_tokens_html = {}\n",
        "  lsthead\n",
        "\n",
        "  #percorre o dataframe para construir a estrutura html de colunas\n",
        "  for i in range(len(df)):\n",
        "    primeiraColuna = True\n",
        "    j = 0\n",
        "    for column in df.columns:\n",
        "\n",
        "        value = \"NAN\"\n",
        "        #print(\"noteTokensHTML i, j \", i,\" \",j)\n",
        "        if not df.empty and not pd.isna(df.at[i, column]):\n",
        "          # Value exists, access it\n",
        "          value = str(df.at[i, column])\n",
        "          value = value.replace(\"'\",\"\")\n",
        "          value = value.replace(\"\\\\\",\"\")\n",
        "\n",
        "        #primeira linha sao os cabeçalhos\n",
        "        if i == 0:\n",
        "          hasheader = True\n",
        "          lsthead.append(\"<td>\")\n",
        "          #lsthead.append(value) #apenas para teste, comentar depois\n",
        "          lsthead.append(\"</td>\")\n",
        "        else:\n",
        "          hasdata = True\n",
        "          if(primeiraColuna):\n",
        "            primeiraColuna = False\n",
        "            #a partir da 3a linha fecha a linha anterior </tr>\n",
        "            if(j==0 and i > 1):\n",
        "              lsttd.append(\"</tr>\")\n",
        "            lsttd.append(\"<tr>\")\n",
        "          lsttd.append(\"<td>\")\n",
        "          #lsttd.append(value) #apenas para teste, comentar depois\n",
        "          lsttd.append(\"</td>\")\n",
        "\n",
        "        primeiraColuna = False\n",
        "        j = j + 1\n",
        "\n",
        "  if(hasheader):\n",
        "    lsthead.insert(0,\"<thead>\")\n",
        "    lsthead.insert(1,\"<tr>\")\n",
        "    lsthead.append(\"</tr>\")\n",
        "    lsthead.append(\"</thead>\")\n",
        "\n",
        "  if(hasdata):\n",
        "    lsttd.insert(0,\"<tbody>\")\n",
        "    lsttd.append(\"</tbody>\")\n",
        "\n",
        "  #se a estrutura tiver completa, adiciona no dicionario tokens\n",
        "  if(hasheader and hasdata):\n",
        "    lsthead.extend(lsttd)\n",
        "  else:\n",
        "    dict_tokens_html = {\"tokens\": \"vazio\"}\n",
        "    lsthead.extend(dict_tokens_html)\n",
        "\n",
        "  lsthead.insert(0,\"<table>\")\n",
        "  lsthead.extend(\"</table>\")\n",
        "  return lsthead\n",
        "\n",
        "\n",
        "def noteListTokensBbox(cell_coordinates, lstData, lstTableRef):\n",
        "  list_tokens_bbox = []\n",
        "\n",
        "  #dimensao dos dados\n",
        "  qtdRowData = len(lstData)\n",
        "  qtdColData = len(lstData[0])\n",
        "  print(\"dimensao Data {0} x {1} \".format(qtdRowData, qtdColData))\n",
        "\n",
        "  #carregando BBOX de cada celula por linha para uma lista\n",
        "  qtdlinhasBbox = len(cell_coordinates)\n",
        "  qtdcolunasBbox = len(cell_coordinates[0][\"cells\"])\n",
        "  print(\"dimensao Bbox {0} x {1} \".format(qtdlinhasBbox, qtdcolunasBbox))\n",
        "\n",
        "\n",
        "  i = 0\n",
        "  for row in cell_coordinates:\n",
        "    j = 0\n",
        "    for bbox in row[\"cells\"]:\n",
        "      x1 = round(bbox[\"cell\"][0])\n",
        "      y1 = round(bbox[\"cell\"][1])\n",
        "      x2 = round(bbox[\"cell\"][2])\n",
        "      y2 = round(bbox[\"cell\"][3])\n",
        "\n",
        "      if(lstTableRef is None):\n",
        "        dict_tokens_bbox = {'tokens': list(lstData[i][j]), 'bbox': [x1, y1, x2, y2]}\n",
        "      else:\n",
        "        dict_tokens_bbox = {'tokens': list(lstData[i][j]), 'bbox': [x1 - lstTableRef[0], y1 - lstTableRef[1], x2 - lstTableRef[0], y2 - lstTableRef[1]]}\n",
        "      list_tokens_bbox.append(dict_tokens_bbox)\n",
        "      j+=1\n",
        "    i+=1\n",
        "\n",
        "  return list_tokens_bbox\n",
        "\n",
        "def noteListTokensBbox(lstBbox, df, lstTableRef):\n",
        "  list_tokens_bbox = []\n",
        "\n",
        "  #dimensao dos dados\n",
        "  qtdRowDf = 0 if df.empty else df.shape[0]\n",
        "  qtdColDf = 0 if df.empty else df.shape[1]\n",
        "  print(\"noteListTokensBbox - dimensao df {0} x {1} \".format(qtdRowDf, qtdColDf))\n",
        "\n",
        "  #carregando BBOX de cada celula por linha para uma lista\n",
        "  #qtdlinhasBbox = 0 if len(lstBbox) == 0 else len(lstBbox)\n",
        "  #qtdcolunasBbox = 0 if len(lstBbox) == 0 and len(lstBbox[0]) else len(lstBbox)\n",
        "\n",
        "  for i in range(qtdRowDf):\n",
        "    for j in range(qtdColDf):\n",
        "      #bbox = lstBbox[i][j]\n",
        "      #x1 = round(bbox[0])\n",
        "      #y1 = round(bbox[1])\n",
        "      #x2 = round(bbox[2])\n",
        "      #y2 = round(bbox[3])\n",
        "      x1 = np.nan\n",
        "      y1 = np.nan\n",
        "      x2 = np.nan\n",
        "      y2 = np.nan\n",
        "\n",
        "      value = \"NAN\"\n",
        "      if not df.empty and not pd.isna(df.at[i, j]):\n",
        "        # Value exists, access it\n",
        "        value = str(df.at[i, j])\n",
        "\n",
        "      if(lstTableRef is None):\n",
        "        dict_tokens_bbox = {'tokens': list(value), 'bbox': [x1, y1, x2, y2]}\n",
        "      else:\n",
        "        dict_tokens_bbox = {'tokens': list(value), 'bbox': [x1, y1, x2, y2]}\n",
        "        #dict_tokens_bbox = {'tokens': list(df.at[i,j]), 'bbox': [x1 - lstTableRef[0], y1 - lstTableRef[1], x2 - lstTableRef[0], y2 - lstTableRef[1]]}\n",
        "      list_tokens_bbox.append(dict_tokens_bbox)\n",
        "\n",
        "  return list_tokens_bbox\n",
        "\n",
        "def printMetaDados(dicMetaData):\n",
        "\n",
        "  strout = []\n",
        "  strout.append(\"{ \\n\")\n",
        "\n",
        "  if \"filename\" in dicMetaData:\n",
        "    strout.append(\"filename: '\" + str(dicMetaData[\"filename\"]) + \"',\\n\")\n",
        "  else:\n",
        "    strout[0] = \"chave filename não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if \"split\" in dicMetaData:\n",
        "    strout.append(\"split: '\" + str(dicMetaData[\"split\"]) + \"',\\n\")\n",
        "  else:\n",
        "    strout[0] = \"chave split não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if \"imgid\" in dicMetaData:\n",
        "    strout.append(\"'imgid': \" + str(dicMetaData[\"imgid\"]) + \",\\n\")\n",
        "  else:\n",
        "    strout[0] = \"chave imgid não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if \"html\" in dicMetaData:\n",
        "\n",
        "    strout.append(\"--INICIO HTML \\n\")\n",
        "    strout.append(\"'html': \\n {\")\n",
        "\n",
        "    if \"cells\" in dicMetaData[\"html\"] and \"structure\" in dicMetaData[\"html\"]:\n",
        "\n",
        "      if isinstance(dicMetaData[\"html\"][\"cells\"], list ) and isinstance(dicMetaData[\"html\"][\"structure\"], list ):\n",
        "\n",
        "        #varrendo o conteudo da lista dicMetaData[\"html\"][\"cells\"]\n",
        "        #que contem as duas sublistas tokens e bbox\n",
        "        strout.append(\"--INICIO CELLS \\n\")\n",
        "        strout.append(\"'cells': [\\n\")\n",
        "        i = 0\n",
        "        for arrcells in dicMetaData[\"html\"][\"cells\"]:\n",
        "\n",
        "           #print da estrutura dos dicionarios tokens e bbox\n",
        "           tokens =  arrcells[\"tokens\"]\n",
        "           bbox =  arrcells[\"bbox\"]\n",
        "           comma = \",\"\n",
        "           if i == len(dicMetaData[\"html\"][\"cells\"]) -1:\n",
        "            comma = \"\"\n",
        "           else:\n",
        "            comma = \",\"\n",
        "\n",
        "           strout.append(\"      {'tokens': \" + str(tokens) + \", 'bbox': \" + str(bbox) + \"}\" + comma + \" \\n\")\n",
        "           i = i + 1\n",
        "\n",
        "        strout.append(\"] --FIM CELLS\\n\")\n",
        "\n",
        "        #print da estrutura do dicionario structure\n",
        "        if(dicMetaData[\"html\"] is not None and dicMetaData[\"html\"][\"structure\"] is not None):\n",
        "          #strout.append(\"      'structure': [\" + str(\"','\".join(dicMetaData[\"html\"][\"structure\"])) + \"' \\n\")\n",
        "          strout.append(\"      'structure': ['\" + str(\"','\".join([x for x in dicMetaData[\"html\"][\"structure\"] if x is not None])) + \"' \\n\")\n",
        "\n",
        "        else:\n",
        "          strout.append(\"      'structure': ['None'] \\n\")\n",
        "\n",
        "\n",
        "        strout.append(\"] --FIM STRUCTURE \\n\")\n",
        "\n",
        "      else:\n",
        "        strout[0] = \"chave html/cells ou structure não existe na estrutura\"\n",
        "        return strout\n",
        "\n",
        "    strout.append(\"} --FIM HTML\\n\")\n",
        "  else:\n",
        "    strout[0] = \"chave html não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  strout.append(\"} \\n\")\n",
        "  return strout\n",
        "\n",
        "def saveAnnotationFile(dicMetaData, dirout , numPage):\n",
        "\n",
        "  vecArq = dicMetaData[\"filename\"].split(\"/\")\n",
        "\n",
        "  nomeArq = \"\"\n",
        "  if(len(vecArq)>0):\n",
        "\n",
        "    tableID = dicMetaData[\"imgid\"].replace(\"'\",\"\")\n",
        "    filename = vecArq[len(vecArq)-1].split(\".\")[0]\n",
        "    labname = vecArq[len(vecArq)-2].split(\".\")[0]\n",
        "\n",
        "    nomeArq = tableID + \"|\" + filename + \"|\" + str(numPage) + \"_METADADOS.mtd\"\n",
        "    print(\"gravando arquivo \", nomeArq)\n",
        "\n",
        "    strout = printMetaDados(dicMetaData)\n",
        "\n",
        "    labDirOut = dirout + \"/\" + labname + \"/\"\n",
        "    if not os.path.exists(labDirOut):\n",
        "      os.makedirs(labDirOut)\n",
        "\n",
        "    with open(labDirOut + nomeArq, 'w') as arquivo:\n",
        "      for linha in strout:\n",
        "            arquivo.write(linha)\n",
        "\n",
        "def getPos(lst, key):\n",
        "\n",
        "  for k, item in enumerate(lst):\n",
        "    if item == key:\n",
        "      return k\n",
        "\n",
        "  return -1\n",
        "\n",
        "#FUNCAO DE VERIFICA SE EXISTE O BUG DE TRUNCAR O VALOR ∞\n",
        "def isBUGInfinito(dicTable, dicTableGT):\n",
        "\n",
        "  char = \"\"\n",
        "  posInf = getPos(dicTableGT[\"FIRST_LINE\"], \"∞\")\n",
        "  posInfV = getPos(dicTableGT[\"FIRST_LINE\"], \"V\")\n",
        "\n",
        "  #possui valor ∞ na tabela? segue análise\n",
        "  if posInf >-1:\n",
        "    char = \"∞\"\n",
        "    print(\"Possui valor ∞ na tabela, posInf\", posInf)\n",
        "    #2 - possuem o mesmo valor de dimensao em DIMENSION\n",
        "    if dicTable[\"DIMENSION\"] == dicTableGT[\"DIMENSION\"]:\n",
        "      print(\"Valor DIMENSION iguais\")\n",
        "      #4 primeiras colunas das duas tabelas possuem o mesmo valor?\n",
        "      print(\"checkAVGSimilarities2INFO >=60 perc? \",checkAVGSimilarities2INFO(dicTableGT, dicTable, 80))\n",
        "      if checkAVGSimilarities2INFO(dicTableGT, dicTable, 60):\n",
        "        print(\"Quatro primeiras colunas similares\")\n",
        "        #4 - chave HEAD tem o tamanho um a menos que GT\n",
        "        if len(dicTable[\"HEAD\"]) == len(dicTableGT[\"HEAD\"])-1 and len(dicTable[\"FIRST_LINE\"]) == len(dicTableGT[\"FIRST_LINE\"])-1:\n",
        "          #dicTable[\"FIRST_LINE\"].insert(posInf, \"∞\")\n",
        "          return True, posInf, \"∞\"\n",
        "\n",
        "  #possui valor ∞ na tabela? segue análise\n",
        "  if posInfV >-1:\n",
        "    char = \"V\"\n",
        "    print(\"Possui valor V na tabela, posInf\", posInf)\n",
        "    #2 - possuem o mesmo valor de dimensao em DIMENSION\n",
        "    if dicTable[\"DIMENSION\"] == dicTableGT[\"DIMENSION\"]:\n",
        "      print(\"Valor DIMENSION iguais\")\n",
        "      #4 primeiras colunas das duas tabelas possuem o mesmo valor?\n",
        "      print(\"checkAVGSimilarities2INFO >=60 perc? \",checkAVGSimilarities2INFO(dicTableGT, dicTable, 80))\n",
        "      if checkAVGSimilarities2INFO(dicTableGT, dicTable, 60):\n",
        "        print(\"Quatro primeiras colunas similares\")\n",
        "        #4 - chave HEAD tem o tamanho um a menos que GT\n",
        "        if len(dicTable[\"HEAD\"]) == len(dicTableGT[\"HEAD\"])-1 and len(dicTable[\"FIRST_LINE\"]) == len(dicTableGT[\"FIRST_LINE\"])-1:\n",
        "          #dicTable[\"FIRST_LINE\"].insert(posInf, \"∞\")\n",
        "          return True, posInfV, \"V\"\n",
        "\n",
        "  return False, -1, \"\"\n",
        "\n",
        "import glob\n",
        "\n",
        "def deleteFiles(dir, ext):\n",
        "  # Obter todos os arquivos com a extensão especificada\n",
        "  files = glob.glob(os.path.join(dir, f'*.{ext}'))\n",
        "\n",
        "  # Remover cada arquivo encontrado\n",
        "  for file in files:\n",
        "      try:\n",
        "          os.remove(file)\n",
        "          print(f\"Arquivo {file} removido com sucesso.\")\n",
        "      except OSError as e:\n",
        "          print(f\"Erro ao remover o arquivo {file}: {e}\")\n",
        "\n",
        "def printHTML2(lst, tipo): #com TAB\n",
        "  # tipo: RAW (cru) ou PRETTY (html com identações)\n",
        "\n",
        "  strout = \"\"\n",
        "  #strres = ''.join(lst)\n",
        "  strres = ''.join([str(x) for x in lst])\n",
        "  if tipo == \"PRETTY\":\n",
        "    soup = bs(strres, 'html.parser')\n",
        "    strout = soup.prettify()\n",
        "    # Substituir espaços por TAB\n",
        "    strout = strout.replace(\"  \", \"\\t\")\n",
        "  else:\n",
        "    strout = strres\n",
        "\n",
        "  return strout\n",
        "\n",
        "def printElementMetaData(dicMetaData, elem):\n",
        "\n",
        "  strout = []\n",
        "  strout.append(\"[\")\n",
        "\n",
        "  if not \"filename\" in dicMetaData:\n",
        "    strout[0] = \"chave filename não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if not \"split\" in dicMetaData:\n",
        "    strout[0] = \"chave split não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if not \"imgid\" in dicMetaData:\n",
        "    strout[0] = \"chave imgid não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if \"html\" in dicMetaData:\n",
        "    if \"cells\" in dicMetaData[\"html\"] and \"structure\" in dicMetaData[\"html\"]:\n",
        "      if isinstance(dicMetaData[\"html\"][\"cells\"], list ) and isinstance(dicMetaData[\"html\"][\"structure\"], list ):\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        if(elem == \"BBOX\"):\n",
        "          #strout.append(\"{\")\n",
        "          for arrcells in dicMetaData[\"html\"][\"cells\"]:\n",
        "\n",
        "            tokens =  arrcells[\"tokens\"]\n",
        "            bbox =  arrcells[\"bbox\"]\n",
        "            comma = \",\"\n",
        "            if i == len(dicMetaData[\"html\"][\"cells\"]) -1:\n",
        "              comma = \"\"\n",
        "            else:\n",
        "              comma = \",\"\n",
        "\n",
        "            strout.append(\"{'tokens': \" + str(tokens) + \", 'bbox': \" + str(bbox) + \"}\" + comma + \" \\n\")\n",
        "            i = i + 1\n",
        "\n",
        "        elif(elem == \"HTML_PRETTY\"):\n",
        "          strout.append(printHTML2(dicMetaData[\"html\"][\"structure\"], \"PRETTY\"))\n",
        "\n",
        "        else:\n",
        "          html = \"'\"\n",
        "          html = html + \"','\".join([element for element in dicMetaData[\"html\"][\"structure\"] if element]) + \"'\"\n",
        "          strout.append(html)\n",
        "\n",
        "      else:\n",
        "        strout[0] = \"chave html/cells ou structure não existe na estrutura\"\n",
        "        return strout\n",
        "\n",
        "  else:\n",
        "    strout[0] = \"chave html não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  strout.append(\"]\")\n",
        "  return strout\n",
        "\n",
        "def saveElementMetadata(dicMetaData, elem, dirout, numPage):\n",
        "\n",
        "  vecArq = dicMetaData[\"filename\"].split(\"/\")\n",
        "  #print(vecArq)\n",
        "  nomeArq = \"\"\n",
        "  if(len(vecArq)>0):\n",
        "\n",
        "    tableID = str(dicMetaData[\"imgid\"]).replace(\"'\",\"\")\n",
        "    filename = vecArq[len(vecArq)-1].split(\".\")[0]\n",
        "    labname = vecArq[len(vecArq)-2].split(\".\")[0]\n",
        "\n",
        "    nomeArq = tableID + \"|\" + filename + \"|\" + str(numPage) + \"_\" + elem + \".\" + elem.lower()\n",
        "    strout = printElementMetaData(dicMetaData, elem)\n",
        "\n",
        "    labDirOut = dirout + \"/\" + labname + \"/\"\n",
        "\n",
        "    print(\"Arquivo de anotação \", elem, \" gerado = \",  nomeArq)\n",
        "    with open(labDirOut + nomeArq, 'w') as arquivo:\n",
        "      for linha in strout:\n",
        "          arquivo.write(linha)\n",
        "\n",
        "#retorna o tableID de maior similiaridade entre os dicionarios GT de comparacao\n",
        "def getMaiorSimilaridade(dic, listasGT):\n",
        "\n",
        "  lstDicRes = []\n",
        "\n",
        "  firstLineCopy = copy.deepcopy(dic[\"FIRST_LINE\"])\n",
        "  firstLineAux = firstLineCopy\n",
        "\n",
        "  for dicGT in listasGT:\n",
        "\n",
        "    lstRes = []\n",
        "\n",
        "    #ajusta caso necessário a dimensao entre as tabelas\n",
        "    #print(\"a tratar...\", dic[\"FIRST_LINE\"])\n",
        "    #print(type(dic[\"FIRST_LINE\"]))\n",
        "\n",
        "    print(\"getMaiorSimilaridade, FIRST_LINE GT\", dicGT[\"FIRST_LINE\"])\n",
        "    print(\"getMaiorSimilaridade, FIRST_LINE ANALISE\", firstLineAux)\n",
        "    for i in range(len(dicGT[\"FIRST_LINE\"])):\n",
        "\n",
        "      lenDicGT = len(dicGT[\"FIRST_LINE\"])\n",
        "\n",
        "      firstLineAux = ajustColList(firstLineAux, len(dicGT[\"FIRST_LINE\"]))\n",
        "      lenDic = len(firstLineAux)\n",
        "\n",
        "      if lenDic != lenDicGT: #tamanhos diferentes, retornar vazio\n",
        "        return \"\"\n",
        "\n",
        "      str1 = str(firstLineAux[i])\n",
        "      str2 = str(dicGT[\"FIRST_LINE\"][i])\n",
        "      strDec1 = str1.replace(\",\", \".\").strip()\n",
        "      strDec2 = str2.replace(\",\", \".\").strip()\n",
        "\n",
        "      #se os valores forem numeros converter para float para calcular similiaridade com maior exatidao\n",
        "      if (isDecimal(strDec1) and isDecimal(strDec2)):\n",
        "        #print(\" Similaridade entre dois numeros \", strDec1, \" e \", strDec2, \" = \", round(calcPercSimValueNums(float(strDec1), float(strDec2)), 2))\n",
        "        lstRes.append(round(calcPercSimValueNums(float(strDec1), float(strDec2)), 2))\n",
        "      #no caso de string\n",
        "      else:\n",
        "        #print(\" Similaridade entre duas strings \", strDec1, \" e \", strDec2, \" = \", round(calcPercSimStrings(str1, str2),4))\n",
        "        lstRes.append(round(calcPercSimStrings(str1, str2),2))\n",
        "\n",
        "      firstLineAux = firstLineCopy\n",
        "\n",
        "    lstDicRes.append({\"TABLEID\": dicGT[\"TABLEID\"], \"RESULT\":lstRes})\n",
        "\n",
        "  #verificando maior media\n",
        "  tableId = \"\"\n",
        "  maiorMedia = 0\n",
        "  for dicRes in lstDicRes:\n",
        "    avg = sum(dicRes[\"RESULT\"]) / len(dicRes[\"RESULT\"])\n",
        "    print(\"Media \", avg)\n",
        "    if avg > maiorMedia:\n",
        "      tableId = dicRes[\"TABLEID\"]\n",
        "      maiorMedia = avg\n",
        "\n",
        "  return tableId\n",
        "\n",
        "def ajustColList(lst1, qtdColsRef):\n",
        "\n",
        "  # Calcula o número de colunas de cada lista\n",
        "  #num_cols_lstRef = len(lstRef)\n",
        "  num_cols_lst1 = len(lst1) if lst1 else 0\n",
        "\n",
        "  # Se lst1 tiver menos colunas que lstRef, preenche com NaN\n",
        "  if num_cols_lst1 < qtdColsRef:\n",
        "    # Calcula o número de colunas a serem adicionadas\n",
        "    num_cols_adicionais = qtdColsRef - num_cols_lst1\n",
        "    # Preenche lst2 com NaN nas novas colunas\n",
        "\n",
        "    #print(\"num_cols_adicionais \", num_cols_adicionais)\n",
        "    for i in range(num_cols_adicionais):\n",
        "      lst1.append(\"NAN\")\n",
        "\n",
        "  #se lst1 tiver mais coluna que lstRef, remove as colunas adicionais de lst1\n",
        "  elif num_cols_lst1 > qtdColsRef:\n",
        "    # Calcula o número de colunas a serem removidas\n",
        "    num_cols_adicionais = num_cols_lst1 - qtdColsRef\n",
        "    for i in range(num_cols_adicionais):\n",
        "      if (len(lst1)>0):\n",
        "        del(lst1[len(lst1)-1])\n",
        "\n",
        "  return lst1\n",
        "\n",
        "#funcao para ajustar uma lista de acordo com a quantidade de linhas e colunas de referencia\n",
        "# se tiver a mais linhas ou colunas, adiciona, se tiver menos, remove\n",
        "def ajustList(lst1, qtdRowsRef, qtdColsRef):\n",
        "\n",
        "  qtdRows = len(lst1)\n",
        "\n",
        "  #lsteste = eval(strdata)\n",
        "  lst1_ajust = []\n",
        "\n",
        "  #1 - ajustando as colunas\n",
        "  for lstRow in lst1:\n",
        "    lst1_ajust.append(ajustColList(lstRow, qtdColsRef))\n",
        "\n",
        "  #1 - ajustando as linhas\n",
        "  #se precisar adicionar linhas\n",
        "  if(qtdRowsRef > qtdRows):\n",
        "    qtdLinhasAdicionais = qtdRowsRef - qtdRows\n",
        "    for i in range(qtdLinhasAdicionais):\n",
        "      if len(lst1_ajust) >0 and len(lst1_ajust[0]) >0:\n",
        "        lst1_ajust.insert(len(lst1_ajust), [\"NAN\" for _ in range(len(lst1_ajust[0]))])\n",
        "  #se precisar remover linhas adicionais\n",
        "  elif(qtdRows > qtdRowsRef):\n",
        "    qtdLinhasAdicionais = qtdRows - qtdRowsRef\n",
        "    for i in range(qtdLinhasAdicionais):\n",
        "      del(lst1_ajust[len(lst1_ajust)-1])\n",
        "\n",
        "  return lst1_ajust\n",
        "\n",
        "#funcao para ajustar a estrutura cell_coordinates em relacao a referencia para possibilitar a comparacao e geracao de estatisticas\n",
        "def ajustCellCord (cellCord, qtdRowsRef, qtdColsRef):\n",
        "\n",
        "  lstCoord = [999999, 999999, 999999, 999999] #nova lista de coordenadas\n",
        "  dicNewCol = {'column': lstCoord, 'cell': lstCoord} #uma nova coluna (celula)\n",
        "  #nova linha da tabela\n",
        "  newLine =  \"{'row': [99999, 99999, 99999, 99999], \\\n",
        "              'cells': [], \\\n",
        "              'cell_count': 0}\"\n",
        "  dicNewLine = eval(newLine)\n",
        "\n",
        "  #verificando dimensao da estrutura atual\n",
        "  qtdRows = 0\n",
        "  qtdCols = 0\n",
        "  if cellCord is not None and len(cellCord) >0:\n",
        "    qtdRows = len(cellCord)\n",
        "    qtdCols = len(cellCord[0][\"cells\"])\n",
        "  #qtdRows = len(cellCord)\n",
        "  #qtdCols = len(cellCord[0][\"cells\"])\n",
        "\n",
        "  #adicionando estrutura inicial para cada quantidade de colunas de referencia\n",
        "  for i in range(qtdColsRef):\n",
        "    dicNewLine[\"cells\"].insert(i,dicNewCol)\n",
        "\n",
        "  #adiciona para cada coluna adicional necessária\n",
        "  if qtdCols < qtdColsRef:\n",
        "    qtdColAdicionais = qtdColsRef - qtdCols\n",
        "\n",
        "    print(\"Adicionando coluna, qtd = \", qtdColAdicionais)\n",
        "    for i in range(qtdColAdicionais):\n",
        "      for row in cellCord:\n",
        "        row['cells'].append(dicNewCol)\n",
        "\n",
        "  #removendo uma coluna para cada adicional\n",
        "  elif qtdCols > qtdColsRef:\n",
        "    qtdColAdicionais = qtdCols - qtdColsRef\n",
        "\n",
        "    print(\"Removendo coluna, qtd = \", qtdColAdicionais)\n",
        "    for i in range(qtdColAdicionais):\n",
        "      for row in cellCord:\n",
        "        row['cells'] = row['cells'][:-1]\n",
        "\n",
        "  #adiciona linha para cada linha adicional necessária\n",
        "  if qtdRows < qtdRowsRef:\n",
        "    qtdRowAdicionais = qtdRowsRef - qtdRows\n",
        "\n",
        "    print(\"Adicionando linha, qtd = \", qtdRowAdicionais)\n",
        "    for i in range(qtdRowAdicionais):\n",
        "      cellCord.append(dicNewLine)\n",
        "\n",
        "  #removendo linha para cada linha adicional necessária\n",
        "  elif qtdRows > qtdRowsRef:\n",
        "    qtdRowAdicionais = qtdRows - qtdRowsRef\n",
        "\n",
        "    print(\"Removendo linha, qtd = \", qtdRowAdicionais)\n",
        "    #removendo linha para cada adicional\n",
        "    for i in range(qtdRowAdicionais):\n",
        "      del(cellCord[len(cellCord)-1])\n",
        "\n",
        "  return cellCord\n",
        "\n",
        "def temRepeticoes(lista):\n",
        "    return len(lista) != len(set(lista))\n",
        "\n",
        "#ajustar df2 para incrementar mais linhas ou colunas em relacao a referencia (df1)\n",
        "def ajustDataframe(df1, df2):\n",
        "\n",
        "  # Verifica se o número de colunas de df2 é menor que o de df1\n",
        "  if df2.shape[1] < df1.shape[1]:\n",
        "      # Calcula quantas colunas precisam ser adicionadas\n",
        "      num_cols_adicionais = df1.shape[1] - df2.shape[1]\n",
        "      # Adiciona as colunas extras em df2 preenchidas com NaN\n",
        "      for i in range(num_cols_adicionais):\n",
        "          df2[f'C{i+1}'] = np.nan\n",
        "  #neste caso remove as colunas adicionais\n",
        "  elif df2.shape[1] > df1.shape[1]:\n",
        "    # Calcula quantas colunas precisam ser removidas\n",
        "    num_cols_remover = df2.shape[1] - df1.shape[1]\n",
        "    # Remove as colunas extras em df2 da direita para a esquerda\n",
        "    df2 = df2.iloc[:, :-num_cols_remover]\n",
        "\n",
        "  # Verifica se o número de linhas de df2 é menor que o de df1\n",
        "  if df2.shape[0] < df1.shape[0]:\n",
        "      # Calcula quantas linhas precisam ser adicionadas\n",
        "      num_linhas_adicionais = df1.shape[0] - df2.shape[0]\n",
        "      # Adiciona as linhas extras em df2 preenchidas com NaN\n",
        "      linhas_extras = pd.DataFrame(index=[f'L{i+1}' for i in range(num_linhas_adicionais)],\n",
        "                                    columns=df2.columns)\n",
        "      df2 = pd.concat([df2, linhas_extras])\n",
        "  #neste caso remove as linhas adicionais\n",
        "  elif df2.shape[0] > df1.shape[0]:\n",
        "    # Calcula quantas linhas precisam ser removidas\n",
        "    num_linhas_remover = df2.shape[0] - df1.shape[0]\n",
        "    # Remove as linhas extras em df2 de baixo para cima\n",
        "    df2 = df2.iloc[:-num_linhas_remover, :]\n",
        "\n",
        "  return df2\n",
        "\n",
        "import copy\n",
        "\n",
        "#ajustar df2 para incrementar mais linhas ou colunas em relacao a referencia (df1)\n",
        "def ajustDataframe(df1, qtdLinhasRef, qtdColunasRef):\n",
        "\n",
        "  dfAux = copy.deepcopy(df1)\n",
        "  # Verifica se o número de colunas de df1 é menor que qtdColunasRef\n",
        "  if df1.shape[1] < qtdColunasRef:\n",
        "      # Calcula quantas colunas precisam ser adicionadas\n",
        "      num_cols_adicionais = qtdColunasRef - df1.shape[1]\n",
        "      # Adiciona as colunas extras em df1 preenchidas com NaN\n",
        "      #print(\"adicionando qtd colunas\", num_cols_adicionais)\n",
        "      ultIndice = len(dfAux.columns) - 1\n",
        "      for i in range(num_cols_adicionais):\n",
        "          if (i==0):\n",
        "            novoIndice = ultIndice + 1\n",
        "          else:\n",
        "            novoIndice = ultIndice + (i+1)\n",
        "          #dfAux[f'C{i+1}'] = np.nan novoIndice\n",
        "          dfAux[novoIndice] = np.nan\n",
        "  #neste caso remove as colunas adicionais\n",
        "  elif qtdColunasRef < df1.shape[1]:\n",
        "    # Calcula quantas colunas precisam ser removidas\n",
        "    num_cols_remover = df1.shape[1] - qtdColunasRef\n",
        "    # Remove as colunas extras em df1 da direita para a esquerda\n",
        "    dfAux = dfAux.iloc[:, :-num_cols_remover]\n",
        "\n",
        "  # Verifica se o número de linhas de df1 é menor que qtdLinhasRef\n",
        "  if df1.shape[0] < qtdLinhasRef:\n",
        "      # Calcula quantas linhas precisam ser adicionadas\n",
        "      num_linhas_adicionais = qtdLinhasRef - df1.shape[0]\n",
        "      # Adiciona as linhas extras em df1 preenchidas com NaN\n",
        "      #linhas_extras = pd.DataFrame(index=[f'L{i+1}' for i in range(num_linhas_adicionais)],\n",
        "                                    #columns=df1.columns)\n",
        "      #print(\"adicionando qtd linhas\", num_linhas_adicionais)\n",
        "      ultIndice = len(dfAux.columns) - 1\n",
        "      linhas_extras = pd.DataFrame(index=[f'{ultIndice + i + 1}' for i in range(num_linhas_adicionais)],\n",
        "                             columns=dfAux.columns)\n",
        "      dfAux = pd.concat([dfAux, linhas_extras])\n",
        "  #neste caso remove as linhas adicionais\n",
        "  elif qtdLinhasRef < df1.shape[0]:\n",
        "    # Calcula quantas linhas precisam ser removidas\n",
        "    num_linhas_remover = df1.shape[0] - qtdLinhasRef\n",
        "    # Remove as linhas extras em df1 de baixo para cima\n",
        "    dfAux = dfAux.iloc[:-num_linhas_remover, :]\n",
        "\n",
        "  #normalizando indices\n",
        "  dfAux = dfAux.reset_index(drop=True)\n",
        "\n",
        "  return dfAux\n",
        "\n",
        "#funcao para converter PDF para png\n",
        "def pdf_page_to_png(pdf_path, page_number, output_path):\n",
        "  # Convertendo a página do PDF para uma lista de imagens\n",
        "  images = convert_from_path(pdf_path, first_page=page_number, last_page=page_number)\n",
        "\n",
        "  # Salvando a imagem como PNG\n",
        "  images[0].save(output_path, 'PNG')\n",
        "\n",
        "\n",
        "#funcao para retirar cabeçalhos adicionais\n",
        "def normalizeDataframe(dfTable):\n",
        "\n",
        "  dfAux = copy.deepcopy(dfTable)\n",
        "\n",
        "  for i in dfTable.index.tolist():\n",
        "\n",
        "    line = dfTable.iloc[i].tolist()\n",
        "    #print(\"i = \", i, \" line = \", line)\n",
        "\n",
        "    #se for entre a primeira e quinta linha (possiveis cabeçalhos)\n",
        "    if (i>=0 and i<=5):\n",
        "\n",
        "      #possui valores nulos, vazios ou não numéricos (cabeçalho)\n",
        "      if( numTimes(line, None) > 3 or numDecimals(line)<=1 ):\n",
        "        print(\"i = \",i, \" É CABEÇALHO\" )\n",
        "        print(\"line do cabeçalho = \",line )\n",
        "        #verificar se na proxima linha possui valores numéricos (neste caso não deve remover - possivel cabeçalho)\n",
        "        prox = i + 1\n",
        "        if(prox < len(dfTable)):\n",
        "          proxLine = dfTable.iloc[prox].tolist()\n",
        "          print(\"i = \",i, \"prox = \", prox, \"proxLine = \", proxLine)\n",
        "          if(numDecimals(proxLine)>2):\n",
        "            #print(\"NÃO REMOVER CABEÇALHO DO, \", i, \" , pois PROX possui valores numericos , qtd decimals(prox) =\", numDecimals(proxLine))\n",
        "            #dfAux.loc[len(dfAux)] = line\n",
        "            continue\n",
        "          else:\n",
        "            #print(\"REMOVER LINHA \", i, \"pois PROX não possui valores numericos , qtd decimals(prox) = \", numDecimals(proxLine))\n",
        "            dfAux = dfAux.drop(i, inplace=False)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  #normalizando indices\n",
        "  dfAux = dfAux.reset_index(drop=True)\n",
        "\n",
        "  return dfAux\n",
        "\n",
        "#funcao para retirar cabeçalhos adicionais\n",
        "def normalizeDataframe2(dfTable):\n",
        "\n",
        "  dfAux = copy.deepcopy(dfTable)\n",
        "\n",
        "  j = 0\n",
        "\n",
        "  idLastHead = 0\n",
        "  for i in dfTable.index.tolist():\n",
        "\n",
        "    line = dfTable.iloc[i].tolist()\n",
        "    #print(\"i = \", i, \" line = \", line)\n",
        "\n",
        "    #possui valores numericos (remover todos cabeçalhos pra tras)\n",
        "    if(numDecimals(line)> 4 and i >0):\n",
        "      #print(\"i = \",i, \" É INICIO PÓS CABEÇALHO, REMOVER TUDO PRA TRÁS\" )\n",
        "      #print(\"LINE \", line )\n",
        "      idLastHead = i\n",
        "      break\n",
        "\n",
        "  if idLastHead > 0:\n",
        "    for i in range(idLastHead -1):\n",
        "      #print(\"removendo linha \", i)\n",
        "      dfAux = dfAux.drop(i, inplace=False)\n",
        "\n",
        "  #normalizando indices\n",
        "  dfAux = dfAux.reset_index(drop=True)\n",
        "\n",
        "  return dfAux"
      ],
      "metadata": {
        "id": "7Oul2aCC0-B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#carregando dados do Modelo\n",
        "\n",
        "# load model\n",
        "processor = DetrImageProcessor.from_pretrained(\"TahaDouaji/detr-doc-table-detection\")\n",
        "model = DetrForObjectDetection.from_pretrained(\"TahaDouaji/detr-doc-table-detection\")"
      ],
      "metadata": {
        "id": "fMhhqsuwAtAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Código MAIN para Rodar em Lote"
      ],
      "metadata": {
        "id": "PwlyxKHm1FGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################### FUNCAO PRINCIPAL - GERAR ARQUIVOS DE METADADOS #######################\n",
        "\n",
        "dfArq = listFiles(CERT_PATH, None) # carregar a lista de arquivos no DATAFRAME\n",
        "\n",
        "lstDF = None\n",
        "df = None\n",
        "dfAux = None\n",
        "# Definir o fuso horário de SP\n",
        "fuso_horario_brasilia = pytz.timezone('America/Sao_Paulo')\n",
        "\n",
        "pathLab = DIROUT_DETR + \"/\" + LAB_PATH + \"/\"\n",
        "\n",
        "if(os.path.exists(pathLab)):\n",
        "  print('Removendo arquivos gerados anteriormente.., caminho:', pathLab)\n",
        "  deleteFiles2(pathLab) #deletando os arquivos anteriores\n",
        "\n",
        "for filepath, pages in zip(dfArq[\"PATH\"], dfArq[\"QTDPAGES\"]):\n",
        "\n",
        "  arrcurfile = filepath.split(\"/\")\n",
        "  curFile = arrcurfile[len(arrcurfile)-1]\n",
        "  labName = arrcurfile[len(arrcurfile)-2]\n",
        "\n",
        "  #inicializando variaveis GT\n",
        "  listFilesGT = []\n",
        "  listTablesInfoGT = []\n",
        "\n",
        "  #coletando dados das tabelas GT para comparacao e gerar o ID da imagem correto\n",
        "  GT_LAB_OUT = GT_PATH + labName + \"/\"\n",
        "  print(\"GT_LAB_OUT\", GT_LAB_OUT)\n",
        "\n",
        "  #varrendo cada pagina do arquivo\n",
        "  for i in range(int(pages)):\n",
        "\n",
        "    page = i+1\n",
        "    #verificando a quantidade de tabelas por pagina\n",
        "\n",
        "    #verificando se a pasta do laboratorio existe, caso negativo, cria\n",
        "    labDirOut = DIROUT_DETR + \"/\" + labName + \"/\"\n",
        "    if not os.path.exists(labDirOut):\n",
        "      os.makedirs(labDirOut)\n",
        "\n",
        "    #definindo variaveis para gravacao do arquivo de saida\n",
        "    noExtension = curFile.replace(\".pdf\",\"\")\n",
        "    #arquivo de PDF de leitura\n",
        "    path_pdf_in = filepath\n",
        "    #caminho para arquivo BMP convertido\n",
        "    path_bmp_noextension =  labDirOut + \"/\" + noExtension\n",
        "\n",
        "    #nova pagina, carrega a lista de referencia (GT) da pagina em questao\n",
        "    listFilesGT = getListFilesGTInfo(noExtension, page, GT_LAB_OUT)\n",
        "    listTablesInfoGT = getListTablesInfo(GT_LAB_OUT,listFilesGT)\n",
        "    qtdTabelasGT = len(listTablesInfoGT)\n",
        "\n",
        "    print(\"Arquivo: [\", path_pdf_in , \"] / qtd de tabelas para ler da pagina[\" + str(page) + \"]: \", len(listTablesInfoGT))\n",
        "\n",
        "    #temos tabelas para processar....\n",
        "    if(qtdTabelasGT >0):\n",
        "\n",
        "      ####1 - converter pdf para png  ####\n",
        "      #convert_pdf_to_bmp(path_pdf_in, path_bmp_noextension, page)\n",
        "\n",
        "      output_png = path_bmp_noextension + \"_\" + str(page) + \".png\"\n",
        "\n",
        "      print(\"output_png = \", output_png)\n",
        "      pdf_page_to_png(path_pdf_in, page, output_png)\n",
        "\n",
        "      image_pil = Image.open(output_png)\n",
        "\n",
        "      # perform inference (predição)\n",
        "      inputs = processor(images=image_pil, return_tensors=\"pt\")\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "      # convert outputs (bounding boxes and class logits) to COCO API\n",
        "      # let's only keep detections with score > 0.9\n",
        "      target_sizes = torch.tensor([image_pil.size[::-1]])\n",
        "      results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
        "\n",
        "      #carregar os dataframes pra uma lista de dataframes lstDF\n",
        "      # Iterar sobre as bounding boxes e imprimir as coordenadas\n",
        "      lstDF = []\n",
        "      print(\"quantidade de boxes reconhecidos = \", len(results[\"boxes\"]))\n",
        "      for box in results[\"boxes\"]:\n",
        "\n",
        "        box = [round(i, 2) for i in box.tolist()]\n",
        "        x1 = box[0]\n",
        "        y1 = box[1]\n",
        "        x2 = box[2]\n",
        "        y2 = box[3]\n",
        "        table_image_cropped = image_pil.crop((box[0], box[1], box[2], box[3]))\n",
        "        #coletar o conteúdo de uma imagem para string\n",
        "        table_content = pytesseract.image_to_string(table_image_cropped)\n",
        "\n",
        "        # Dividir o texto em linhas\n",
        "        lines = table_content.split('\\n')\n",
        "\n",
        "        # Dividir cada linha em palavras\n",
        "        data = [line.split() for line in lines if line.strip() != '']\n",
        "\n",
        "        # Criar um DataFrame\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        #normalizar dataFrame\n",
        "        df = normalizeDataframe2(df)\n",
        "\n",
        "        lstDF.append(df)\n",
        "\n",
        "      #print(\"lstDF = \",lstDF)\n",
        "      #break\n",
        "\n",
        "      qtdlinhasLstDF = 0\n",
        "      qtdColunasLstDF = 0\n",
        "      if len(lstDF) > 0:\n",
        "        qtdlinhasLstDF = len(lstDF)\n",
        "        qtdColunasLstDF = len(lstDF[0].columns)\n",
        "\n",
        "      qtdlinhasGT = 0\n",
        "      qtdColunasGT = 0\n",
        "      if len(listTablesInfoGT) > 0:\n",
        "        qtdlinhasGT = len(listTablesInfoGT)\n",
        "        qtdColunasGT = len(listTablesInfoGT[0])\n",
        "\n",
        "      print(\"=============>Tabela pagina {0} caminho: {1} \".format(page,path_pdf_in))\n",
        "      print(\"Dimensão lstDF {0} x {1} \".format(qtdlinhasLstDF, qtdColunasLstDF))\n",
        "      print(\"Dimensão listTablesInfoGT {0} x {1} \".format(qtdlinhasGT, qtdColunasGT))\n",
        "\n",
        "      print(\"len(lstDF) e len(listTablesInfoGT) com tamanho > 1 - CASO COMPLEXO \")\n",
        "      #para cada tabela (dataframe) da lista de dataframes\n",
        "      for df in lstDF:\n",
        "\n",
        "        #print (\"df original = \", df)\n",
        "        qtdlinhasDF = 0\n",
        "        qtdColunasDF = 0\n",
        "        if len(lstDF) > 0:\n",
        "          qtdlinhasDF = len(df)\n",
        "          qtdColunasDF = len(df.columns)\n",
        "\n",
        "        dfOriginal = copy.deepcopy(df) #guarda o DF original sem correcoes para gravar no HTML (calculo do TED)\n",
        "        #verificando possivel ajuste de dimensao do dataframe com GT para comparação\n",
        "\n",
        "        qtdLinhasGT = 0\n",
        "        qtdColsGT = 0\n",
        "        if listTablesInfoGT is not None and len(listTablesInfoGT) > 0:\n",
        "\n",
        "          #ajustar o tamanho do dataframe, caso necessário\n",
        "          qtdLinhasGT = int(listTablesInfoGT[0][\"DIMENSION\"].split(\"X\")[0])\n",
        "          qtdColsGT = int(listTablesInfoGT[0][\"DIMENSION\"].split(\"X\")[1])\n",
        "          if (qtdColunasDF != qtdColsGT or qtdlinhasDF != qtdLinhasGT):\n",
        "            print(\"Ajustando dataframe para dimensão, dimensão atual DF = \", qtdlinhasDF,\"x\",qtdColunasDF)\n",
        "            print(\"Ajustando dataframe para dimensão do GT, nova dimensão = \", qtdLinhasGT,\"x\",qtdColsGT)\n",
        "            df = ajustDataframe(df, qtdLinhasGT, qtdColsGT)\n",
        "\n",
        "        #carregando o objeto dicionário INFO da tabela de análise\n",
        "        bbox = [x1, y1, x2, y2]\n",
        "        lstHead = [] if len(df) == 0 else df.iloc[0].tolist()\n",
        "        lstFirst = [] if len(df) == 0 or len(df) == 1 else df.iloc[1].tolist()\n",
        "        dicTable = getDicTableInfo(labName, noExtension, page, qtdLinhasGT, qtdColsGT, bbox, lstHead, lstFirst)\n",
        "        print(\"dicTable HEAD\", dicTable[\"HEAD\"])\n",
        "        print(\"dicTable FIRST_LINE\", dicTable[\"FIRST_LINE\"])\n",
        "\n",
        "        #verificando qual tabela de maior similaridade\n",
        "        #apenas uma tabela GT existente na pagina (df recebe o TABLEID existente)\n",
        "        if len(listTablesInfoGT) ==1:\n",
        "          print(\"Apenas uma tabela GT na página, logo, TABLEID a ser utilizado para dicTable = \",listTablesInfoGT[0][\"TABLEID\"])\n",
        "          dicTable[\"TABLEID\"] = listTablesInfoGT[0][\"TABLEID\"]\n",
        "        else:\n",
        "          print(\"listTablesInfoGT > 1, verificar similaridade...\")\n",
        "          dicTable[\"TABLEID\"] = getMaiorSimilaridade(dicTable, listTablesInfoGT)\n",
        "\n",
        "        listFileGT = getGTInfo(dicTable[\"TABLEID\"], noExtension, page, GT_LAB_OUT)\n",
        "        listTableInfoGT = getListTablesInfo(GT_LAB_OUT,listFileGT)\n",
        "        print(\"TABLEID de maior similaridade = \", dicTable[\"TABLEID\"])\n",
        "\n",
        "        #gravar o arquivo INFO da tabela para comparacao com GT\n",
        "        #encontrou tabela identica ou similar\n",
        "        if dicTable[\"TABLEID\"] != \"TBD\" and dicTable[\"TABLEID\"] != \"\":\n",
        "          filePath = labDirOut + dicTable[\"TABLEID\"] + \"|\" + noExtension + \"|\" + str(page) + \"_INFO.info\"\n",
        "          print(\"gerando arquivo INFO de resultado - SUCESSO: \", filePath)\n",
        "          SaveDicTableInfo(filePath, dicTable)\n",
        "        else:\n",
        "          #no caso de nao ter encontrado tabela similar ao GT para comparação, registrar arquivo de erro\n",
        "          filePath = labDirOut + noExtension + \"|\" + str(page) + \"|\" + \"_INFO_ERRO.error\"\n",
        "          dicTable[\"OBS\"] = \"ERRO - TABLEID NÃO ENCONTRADO\"\n",
        "          print(\"gerando arquivo INFO de resultado - ERRO: \", filePath)\n",
        "          SaveDicTableInfo(filePath, dicTable)\n",
        "\n",
        "        #modelo do dicmetada\n",
        "        dicMetaData = {\n",
        "        \"filename\": 0,\n",
        "        \"split\": \"train\",\n",
        "        \"imgid\": \"\",\n",
        "        \"html\": {\n",
        "          \"cells\": 0,\n",
        "          \"structure\": 0\n",
        "                }\n",
        "        }\n",
        "\n",
        "        #inicializando a lista\n",
        "        dicMetaData[\"filename\"] = path_pdf_in\n",
        "        dicMetaData[\"imgid\"] = dicTable[\"TABLEID\"]\n",
        "\n",
        "        #gravando no arquivo as informacoes de METADADOS, BBOX E HTML\n",
        "        lstCells = []\n",
        "        lstStructure = []\n",
        "\n",
        "        #print(\"df = \", df)\n",
        "        #print(\"df shape \", df.shape)\n",
        "        #print(\"df.at[0,0] \", df.at[0,0])\n",
        "        #carrega e concatena a lista de tokens das celulas e bbox calculando como referencia as coordenadas da tabela principal\n",
        "        lstCells.extend(noteListTokensBbox(None, df, None)) # no futuro alterar para inserir bbox\n",
        "        lstStructure.extend(noteTokensHTML(dfOriginal)) #carrega e concatena a lista de tokens html (df original sem ajustes)\n",
        "        dicMetaData[\"html\"][\"cells\"] = lstCells\n",
        "        dicMetaData[\"html\"][\"structure\"] = lstStructure\n",
        "\n",
        "        print(\"Gravando Metadados: \", DIROUT_DETR)\n",
        "        saveAnnotationFile(dicMetaData, DIROUT_DETR , page)\n",
        "        saveElementMetadata(dicMetaData, \"BBOX\", DIROUT_DETR, page)\n",
        "        saveElementMetadata(dicMetaData, \"HTML\", DIROUT_DETR, page)\n",
        "        #saveElementMetadata(dicMetaData, \"HTML_PRETTY\", DIROUT_DETR, page)\n",
        "\n",
        "        #break #end for lstDF\n",
        "\n",
        "      #end if temos paginas para processar\n",
        "\n",
        "    #break #end of pages\n",
        "\n",
        "  #break # end for files\n",
        "\n",
        "#removendo arquivos BMP\n",
        "print(\"Removendo arquivos png temporários...\")\n",
        "deleteFiles(labDirOut, \"png\")\n",
        "\n",
        "# Obter a data e hora corrente\n",
        "data_e_hora_corrente = datetime.now(fuso_horario_brasilia)\n",
        "# Formatar a data e hora corrente para o formato desejado\n",
        "data_e_hora_formatadas = data_e_hora_corrente.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "#MAXFILES = 1 #apenas para testes, delimitar a quantidade de certificados a ler\n",
        "print('FIM DO PROCESSAMENTO ', data_e_hora_formatadas)"
      ],
      "metadata": {
        "id": "LVTFkzen1lTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEDS - CALCULO PARA O MODELO"
      ],
      "metadata": {
        "id": "27lgz0_sUvFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install distance\n",
        "!pip install apted\n",
        "!pip install lxml\n",
        "!pip install tqdm\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "6UlAkHSrUvxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import distance\n",
        "from apted import APTED, Config\n",
        "from apted.helpers import Tree\n",
        "from lxml import etree, html\n",
        "from collections import deque\n",
        "#from parallel import parallel_process\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TableTree(Tree):\n",
        "    def __init__(self, tag, colspan=None, rowspan=None, content=None, *children):\n",
        "        self.tag = tag\n",
        "        self.colspan = colspan\n",
        "        self.rowspan = rowspan\n",
        "        self.content = content\n",
        "        self.children = list(children)\n",
        "\n",
        "    def bracket(self):\n",
        "        \"\"\"Show tree using brackets notation\"\"\"\n",
        "        if self.tag == 'td':\n",
        "            result = '\"tag\": %s, \"colspan\": %d, \"rowspan\": %d, \"text\": %s' % \\\n",
        "                     (self.tag, self.colspan, self.rowspan, self.content)\n",
        "        else:\n",
        "            result = '\"tag\": %s' % self.tag\n",
        "        for child in self.children:\n",
        "            result += child.bracket()\n",
        "        return \"{{{}}}\".format(result)\n",
        "\n",
        "\n",
        "class CustomConfig(Config):\n",
        "    @staticmethod\n",
        "    def maximum(*sequences):\n",
        "        \"\"\"Get maximum possible value\n",
        "        \"\"\"\n",
        "        return max(map(len, sequences))\n",
        "\n",
        "    def normalized_distance(self, *sequences):\n",
        "        \"\"\"Get distance from 0 to 1\n",
        "        \"\"\"\n",
        "        return float(distance.levenshtein(*sequences)) / self.maximum(*sequences)\n",
        "\n",
        "    def rename(self, node1, node2):\n",
        "        \"\"\"Compares attributes of trees\"\"\"\n",
        "        if (node1.tag != node2.tag) or (node1.colspan != node2.colspan) or (node1.rowspan != node2.rowspan):\n",
        "            return 1.\n",
        "        if node1.tag == 'td':\n",
        "            if node1.content or node2.content:\n",
        "                return self.normalized_distance(node1.content, node2.content)\n",
        "        return 0.\n",
        "\n",
        "\n",
        "class TEDS(object):\n",
        "    ''' Tree Edit Distance basead Similarity\n",
        "    '''\n",
        "    def __init__(self, structure_only=False, n_jobs=1, ignore_nodes=None):\n",
        "        assert isinstance(n_jobs, int) and (n_jobs >= 1), 'n_jobs must be an integer greather than 1'\n",
        "        self.structure_only = structure_only\n",
        "        self.n_jobs = n_jobs\n",
        "        self.ignore_nodes = ignore_nodes\n",
        "        self.__tokens__ = []\n",
        "\n",
        "    def tokenize(self, node):\n",
        "        ''' Tokenizes table cells\n",
        "        '''\n",
        "        self.__tokens__.append('<%s>' % node.tag)\n",
        "        if node.text is not None:\n",
        "            self.__tokens__ += list(node.text)\n",
        "        for n in node.getchildren():\n",
        "            self.tokenize(n)\n",
        "        if node.tag != 'unk':\n",
        "            self.__tokens__.append('</%s>' % node.tag)\n",
        "        if node.tag != 'td' and node.tail is not None:\n",
        "            self.__tokens__ += list(node.tail)\n",
        "\n",
        "    def load_html_tree(self, node, parent=None):\n",
        "        ''' Converts HTML tree to the format required by apted\n",
        "        '''\n",
        "        global __tokens__\n",
        "        if node.tag == 'td':\n",
        "            if self.structure_only:\n",
        "                cell = []\n",
        "            else:\n",
        "                self.__tokens__ = []\n",
        "                self.tokenize(node)\n",
        "                cell = self.__tokens__[1:-1].copy()\n",
        "            new_node = TableTree(node.tag,\n",
        "                                 int(node.attrib.get('colspan', '1')),\n",
        "                                 int(node.attrib.get('rowspan', '1')),\n",
        "                                 cell, *deque())\n",
        "        else:\n",
        "            new_node = TableTree(node.tag, None, None, None, *deque())\n",
        "        if parent is not None:\n",
        "            parent.children.append(new_node)\n",
        "        if node.tag != 'td':\n",
        "            for n in node.getchildren():\n",
        "                self.load_html_tree(n, new_node)\n",
        "        if parent is None:\n",
        "            return new_node\n",
        "\n",
        "    def evaluate(self, pred, true):\n",
        "        ''' Computes TEDS score between the prediction and the ground truth of a\n",
        "            given sample\n",
        "        '''\n",
        "        if (not pred) or (not true):\n",
        "            return 0.0\n",
        "        parser = html.HTMLParser(remove_comments=True, encoding='utf-8')\n",
        "        pred = html.fromstring(pred, parser=parser)\n",
        "        true = html.fromstring(true, parser=parser)\n",
        "        if pred.xpath('body/table') and true.xpath('body/table'):\n",
        "            pred = pred.xpath('body/table')[0]\n",
        "            true = true.xpath('body/table')[0]\n",
        "            if self.ignore_nodes:\n",
        "                etree.strip_tags(pred, *self.ignore_nodes)\n",
        "                etree.strip_tags(true, *self.ignore_nodes)\n",
        "            n_nodes_pred = len(pred.xpath(\".//*\"))\n",
        "            n_nodes_true = len(true.xpath(\".//*\"))\n",
        "            n_nodes = max(n_nodes_pred, n_nodes_true)\n",
        "            tree_pred = self.load_html_tree(pred)\n",
        "            tree_true = self.load_html_tree(true)\n",
        "            distance = APTED(tree_pred, tree_true, CustomConfig()).compute_edit_distance()\n",
        "            return 1.0 - (float(distance) / n_nodes)\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def batch_evaluate(self, pred_json, true_json):\n",
        "        ''' Computes TEDS score between the prediction and the ground truth of\n",
        "            a batch of samples\n",
        "            @params pred_json: {'FILENAME': 'HTML CODE', ...}\n",
        "            @params true_json: {'FILENAME': {'html': 'HTML CODE'}, ...}\n",
        "            @output: {'FILENAME': 'TEDS SCORE', ...}\n",
        "        '''\n",
        "        samples = true_json.keys()\n",
        "        if self.n_jobs == 1:\n",
        "            scores = [self.evaluate(pred_json.get(filename, ''), true_json[filename]['html']) for filename in tqdm(samples)]\n",
        "        else:\n",
        "            inputs = [{'pred': pred_json.get(filename, ''), 'true': true_json[filename]['html']} for filename in samples]\n",
        "            scores = parallel_process(inputs, self.evaluate, use_kwargs=True, n_jobs=self.n_jobs, front_num=1)\n",
        "        scores = dict(zip(samples, scores))\n",
        "        return scores"
      ],
      "metadata": {
        "id": "TM2tHRRAU-2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função MAIN para gerar estatísticas (ARQUIVOS GT VERSUS DETR)"
      ],
      "metadata": {
        "id": "IDWHZPKjUCHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRGnlpj701q6",
        "outputId": "bd856e01-6a21-4d28-82ca-93a474c66d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "#coletar as informacoes da tabela do arquivo _INFO e retornar para uma lista\n",
        "def getListTablesInfo(path, listFiles):\n",
        "\n",
        "  listTablesInfo = []\n",
        "  for fileName in listFiles:\n",
        "    with open(path + fileName, 'r') as file:\n",
        "      conteudo = file.read()\n",
        "      listTablesInfo.append(eval(conteudo))\n",
        "\n",
        "  return listTablesInfo\n",
        "\n",
        "def sortList(lst, len):\n",
        "    def personList(item):\n",
        "        # Extrai o número após 'CTM' e converte para inteiro\n",
        "        return int(item[len:])\n",
        "\n",
        "    return sorted(lst, key=personList)\n",
        "\n",
        "#coletar arquivo de acordo com premissas (prefixo e sufixo)\n",
        "def getFileByPrefix(path, prefix, sufix):\n",
        "\n",
        "  for fileName in os.listdir(path):\n",
        "      if prefix in fileName and fileName.endswith(sufix):\n",
        "          return fileName\n",
        "  return \"\"\n",
        "\n",
        "def getFiles(folderDir, ext):\n",
        "  # Construir o padrão de busca usando a extensão fornecida\n",
        "  pattern = os.path.join(folderDir, f\"*.{ext}\")\n",
        "\n",
        "  files = []\n",
        "  arrPath = folderDir.split(\"/\")\n",
        "\n",
        "  #print(\"arrPath \", arrPath)\n",
        "  if len(arrPath) >0 and len(arrPath[len(arrPath)-1].split(\"_\")) >0:\n",
        "    #print(folderDir)\n",
        "    lab = arrPath[len(arrPath)-1].split(\"_\")[2]\n",
        "    # Usar a função glob para encontrar os arquivos correspondentes ao padrão\n",
        "    files = glob.glob(pattern)\n",
        "\n",
        "  # Retornar a lista de arquivos encontrados\n",
        "  return sorted(files)\n",
        "\n",
        "def getInfoFiles(list):\n",
        "\n",
        "  lstInfoFiles = []\n",
        "  for path in list:\n",
        "\n",
        "    arrPath = path.split(\"/\")\n",
        "    file = arrPath[len(arrPath)-1]\n",
        "    tableId = file.split(\"|\")[0]\n",
        "    fileName = file.split(\"|\")[1]\n",
        "    page = file.split(\"|\")[2].split(\"_\")[0]\n",
        "\n",
        "    lstInfoFiles.append({\"TABLEID\":tableId, \"TABLEID\":tableId, \"FILE\":fileName, \"PAGE\":page})\n",
        "\n",
        "  return lstInfoFiles\n",
        "\n",
        "#files = getFiles(\"/content/drive/MyDrive/DataSets/Certificados/Out/img2table/LAB_01_CTM\", \"info\")\n",
        "#lstInfoFiles = getInfoFiles (files)\n",
        "\n",
        "#funcao que compara o valor de duas listas e calcula a media do percentual de similaridade entre eles\n",
        "#(para calcular o valor do bbox das tabelas e células das tabelas)\n",
        "def calcPercSimValueLists(lista1, lista2):\n",
        "  if len(lista1) != len(lista2):\n",
        "      print(\"calcPercSimValueLists, listas de tamanhos diferentes, lista1=\",lista1,\"/ lista2 = \",lista2)\n",
        "      raise ValueError(\"As listas devem ter o mesmo comprimento.\")\n",
        "\n",
        "  percSim = [ (1 / (1 + (abs(num1 - num2)))) * 100 for num1, num2 in zip(lista1, lista2)]\n",
        "  #print(\"percSim \", percSim)\n",
        "  #print(\"result percSim \", sum(percSim) / len (percSim))\n",
        "  return sum(percSim) / len (percSim)\n",
        "\n",
        "#(para calcular o percentual de similaridade entre dois números\n",
        "def calcPercSimValueNums(num1, num2):\n",
        "\n",
        "  percSim = (1 / (1 + (abs(num1 - num2)))) * 100\n",
        "  #print (\" similaridade entre os numeros {0} e {1}: {2}\".format(num1, num2, percSim))\n",
        "  return percSim\n",
        "\n",
        "#similaridade de strings conhecido como \"Distância de Levenshtein\"\n",
        "def calcPercSimStrings(str1, str2):\n",
        "\n",
        "  #retirando quebra de linhas da string\n",
        "  str1 = str1.replace(\"\\n\", \" \")\n",
        "  str2 = str2.replace(\"\\n\", \" \")\n",
        "\n",
        "  tamanho_str1 = len(str1)\n",
        "  tamanho_str2 = len(str2)\n",
        "\n",
        "  matriz = [[0] * (tamanho_str2 + 1) for _ in range(tamanho_str1 + 1)]\n",
        "\n",
        "  for i in range(tamanho_str1 + 1):\n",
        "    matriz[i][0] = i\n",
        "\n",
        "  for j in range(tamanho_str2 + 1):\n",
        "    matriz[0][j] = j\n",
        "\n",
        "  for i in range(1, tamanho_str1 + 1):\n",
        "    for j in range(1, tamanho_str2 + 1):\n",
        "        if str1[i - 1] == str2[j - 1]:\n",
        "            custo_substituicao = 0\n",
        "        else:\n",
        "            custo_substituicao = 1\n",
        "        matriz[i][j] = min(matriz[i - 1][j] + 1,       # Deletar\n",
        "                            matriz[i][j - 1] + 1,       # Inserir\n",
        "                              matriz[i - 1][j - 1] + custo_substituicao)  # Substituir\n",
        "\n",
        "  distancia = matriz[tamanho_str1][tamanho_str2]\n",
        "  maximo_tamanho = max(tamanho_str1, tamanho_str2)\n",
        "\n",
        "  similaridade = 0\n",
        "  if maximo_tamanho > 0:\n",
        "    similaridade = (maximo_tamanho - distancia) / maximo_tamanho\n",
        "  #print (\" similaridade entre {0} e {1}: {2}\".format(str1, str2, similaridade * 100))\n",
        "  return similaridade * 100\n",
        "\n",
        "#numero de ocorrencias de um numero em uma lista\n",
        "def numTimes(list, num):\n",
        "\n",
        "  cont = 0\n",
        "  for valor in list:\n",
        "    if valor == num:\n",
        "      cont+=1\n",
        "\n",
        "  return cont\n",
        "\n",
        "#numero de ocorrencias de um numero ser maior ou igual que um numero\n",
        "def numTimesMoreThen(list, num):\n",
        "\n",
        "  cont = 0\n",
        "  for valor in list:\n",
        "    if valor >= num and valor <100:\n",
        "      cont+=1\n",
        "\n",
        "  return cont\n",
        "\n",
        "#calcular similaridades em valores das celulas bbox e tokens de duas listas de tabelas\n",
        "def calStatsTablesValues(lstTable1, lstTable2):\n",
        "\n",
        "  lstResTokens = []\n",
        "  lstResBbox = []\n",
        "  for item1, item2 in zip(lstTable1, lstTable2):\n",
        "\n",
        "    listaBbox1 = item1[\"bbox\"]\n",
        "    listaBbox2 = item2[\"bbox\"]\n",
        "    str1 = \"\".join(item1[\"tokens\"])\n",
        "    str2 = \"\".join(item2[\"tokens\"])\n",
        "\n",
        "    if len(listaBbox1) >0 and len(listaBbox2) >0:\n",
        "      lstResBbox.append(round(calcPercSimValueLists(listaBbox1, listaBbox2),2))\n",
        "    else:\n",
        "      lstResBbox.append(0)\n",
        "\n",
        "    strDec1 = str1.replace(\",\", \".\").strip()\n",
        "    strDec2 = str2.replace(\",\", \".\").strip()\n",
        "    #se os valores forem numeros converter para float para calcular similiaridade com maior exatidao\n",
        "    if (isDecimal(strDec1) and isDecimal(strDec2)):\n",
        "      lstResTokens.append( round( calcPercSimValueNums(float(strDec1), float(strDec2) ),2) )\n",
        "    #no caso de string\n",
        "    else:\n",
        "      lstResTokens.append(round(calcPercSimStrings(str1, str2),2))\n",
        "\n",
        "  return lstResTokens, lstResBbox\n",
        "\n",
        "#calcular similaridades em valores das celulas bbox das tabelas detectadas (arquivo INFO)\n",
        "def calStatsBboxInfo(lstTable1, lstTable2):\n",
        "\n",
        "  lstBboxInfo = []\n",
        "  listaBbox1 = [] if len(lstTable1) ==0 else lstTable1[\"BBOX\"]\n",
        "  listaBbox2 = [] if len(lstTable2) ==0 else lstTable2[\"BBOX\"]\n",
        "\n",
        "  if len(listaBbox1) >0 and len(listaBbox2) >0:\n",
        "    lstBboxInfo.append(round(calcPercSimValueLists(listaBbox1, listaBbox2),2))\n",
        "  else:\n",
        "    lstBboxInfo.append(0)\n",
        "\n",
        "  return lstBboxInfo\n",
        "\n",
        "#calcular quantidade de valores não lidos pelo modelo (NAN ou 999999)\n",
        "def calStatsNAN(lstTable):\n",
        "\n",
        "  qtdNANToken = 0\n",
        "  qtdNANBbox = 0\n",
        "\n",
        "  for item in lstTable:\n",
        "    listaBbox = item[\"bbox\"]\n",
        "    token = \"\".join(item[\"tokens\"])\n",
        "\n",
        "    if numTimes(listaBbox, 999999) ==4:\n",
        "      qtdNANBbox+=1\n",
        "\n",
        "    if token == \"NAN\":\n",
        "      qtdNANToken+=1\n",
        "\n",
        "  return qtdNANToken, qtdNANBbox\n",
        "\n",
        "def removeSpecialChars(strTexto):\n",
        "  # Remover os caracteres especiais\n",
        "  speChars = [\"\\\\\", ]\n",
        "\n",
        "  strTexto = strTexto.replace(speChars, \"\")\n",
        "\n",
        "  return strTexto\n",
        "\n",
        "def readFile(filePath):\n",
        "  try:\n",
        "    with open(filePath, 'r') as arquivo:\n",
        "        conteudo = arquivo.read()\n",
        "        conteudo = conteudo.replace(\"[nan, nan, nan, nan]\", \"[999999,999999,999999,999999]\")\n",
        "        #conteudo = conteudo.replace(\"\\'\", \"\")\n",
        "        conteudo = conteudo.replace(\"\\n\", \" \")\n",
        "    return conteudo\n",
        "  except FileNotFoundError:\n",
        "    print(f'O arquivo \"{filePath}\" não foi encontrado.')\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print(f'Ocorreu um erro ao ler o arquivo: {e}')\n",
        "    return None\n",
        "\n",
        "def SaveFileStats (filePath, dicTableStats):\n",
        "\n",
        "  strFile = json.dumps(dicTableStats)\n",
        "  strFile = strFile.replace(\",\",\",\\n\")\n",
        "\n",
        "  print(\"Salvando arquivo de estatísticas: \",filePath)\n",
        "  with open(filePath, 'w') as arquivo:\n",
        "    arquivo.write(strFile)\n",
        "\n",
        "def strInDic(dicionario, string):\n",
        "\n",
        "  for chave, valor in dicionario.items():\n",
        "      if isinstance(valor, str) and string in valor:\n",
        "          return True\n",
        "  return False\n",
        "\n",
        "def strInList(lstDic, string):\n",
        "\n",
        "  for dic in lstDic:\n",
        "    for chave, valor in dic.items():\n",
        "        if isinstance(valor, str) and string == valor:\n",
        "            return True\n",
        "  return False\n",
        "\n",
        "def isDecimal(valor):\n",
        "  try:\n",
        "\n",
        "    if valor == \"NAN\" or valor == \"nan\" or valor is None:\n",
        "      return False\n",
        "    else:\n",
        "      valor = valor.replace(\",\",\".\")\n",
        "      float(valor)\n",
        "      return True\n",
        "  except ValueError:\n",
        "      return False\n",
        "\n",
        "def ExportCSVSummary(ToolPath, GTPath):\n",
        "  #coletando os diretorios dos laboratorios\n",
        "  dirLabs = [nome for nome in os.listdir(ToolPath)]\n",
        "\n",
        "  listStats = []\n",
        "  filesSTATS = []\n",
        "\n",
        "  lstTableIdGT = []\n",
        "  #colentando os arquivos de statisticas dos laboratorios\n",
        "  for dirLab in dirLabs:\n",
        "\n",
        "    labPath =  ToolPath + dirLab\n",
        "    filesSTATS = getFiles(labPath, \"stats\")\n",
        "\n",
        "    #print(\"filesSTATS \", filesSTATS)\n",
        "    #coletando arquivos de estatisticas\n",
        "    for fileSTATS in filesSTATS:\n",
        "      dicStats = ast.literal_eval(readFile(fileSTATS))\n",
        "      listStats.append(dicStats)\n",
        "\n",
        "    #coletando os tablesID do GT para comparacao\n",
        "    filesGTInfo = GTPath + dirLab\n",
        "    filesInfo = getFiles(filesGTInfo, \"info\")\n",
        "\n",
        "    #print(\"filesInfo \", filesInfo)\n",
        "\n",
        "    for fileInfo in filesInfo:\n",
        "      arrFile = fileInfo.split(\"/\")\n",
        "      tableId = arrFile[len(arrFile)-1].split(\"|\")[0]\n",
        "      lstTableIdGT.append(dirLab+\"|\"+tableId)\n",
        "\n",
        "  lstErros = []\n",
        "\n",
        "  for item in lstTableIdGT:\n",
        "\n",
        "    #print(item)\n",
        "    lab = item.split(\"|\")[0]\n",
        "    tableId = item.split(\"|\")[1]\n",
        "\n",
        "    if not strInList(listStats, tableId): #não encontrou, adicionar ao erro\n",
        "\n",
        "      print(\"Não encontrou TABLEID \", tableId, \", adicionando....\")\n",
        "      #print(\"GTPath \", GTPath)\n",
        "      #print(\"lab \", lab)\n",
        "      #print(\"tableId \", tableId)\n",
        "      #print(\"fileInfo \", fileInfo)\n",
        "\n",
        "      #coletando informacoes do statsInfo\n",
        "      #print(\"parametros a carregar na funcao getFileByPrefix\", GTPath + lab, tableId+\"|\", \"info\")\n",
        "      fileInfo = getFileByPrefix(GTPath + lab, tableId+\"|\", \"info\")\n",
        "      #pathInfo = GTPath + dirLab + fileInfo\n",
        "      lstFile = [fileInfo]\n",
        "      #print(\"parametros a carregar na funcao getListTablesInfo\", GTPath + lab + \"/\", lstFile)\n",
        "      lstInfo = getListTablesInfo(GTPath + lab + \"/\", lstFile)\n",
        "\n",
        "      dicTableStats = {}\n",
        "      dicTableStats[\"LAB\"] = lstInfo[0][\"LAB\"]\n",
        "      dicTableStats[\"FILE\"] = lstInfo[0][\"FILE\"]\n",
        "      dicTableStats[\"PAGE\"] = lstInfo[0][\"PAGE\"]\n",
        "      dicTableStats[\"TABLEID\"] = lstInfo[0][\"TABLEID\"]\n",
        "      dicTableStats[\"DIMENSION\"] = lstInfo[0][\"DIMENSION\"]\n",
        "      dicTableStats[\"QTDCELLS\"] = 0\n",
        "      dicTableStats[\"QTDACERTOSCELLS\"] = 0\n",
        "      dicTableStats[\"PERCACERTOSCELLS\"] = 0\n",
        "      dicTableStats[\"PERCACERTOSBBOXINFO\"] = 0\n",
        "      dicTableStats[\"PERCACERTOSBBOX\"] = 0\n",
        "      dicTableStats[\"QTDNAOLIDOSBBOX\"] = 0\n",
        "      dicTableStats[\"PERCNAOLIDOSBBOX\"] = 0\n",
        "      dicTableStats[\"QTDNAOLIDOSTOKEN\"] = 0\n",
        "      dicTableStats[\"PERCNAOLIDOSTOKEN\"] = 0\n",
        "      dicTableStats[\"TEDS\"] = 0\n",
        "\n",
        "      lstErros.append(dicTableStats)\n",
        "\n",
        "  lstTotal = listStats + lstErros\n",
        "\n",
        "  dtStats = pd.DataFrame(lstTotal)\n",
        "  dtStatsORD = dtStats.sort_values(by=['LAB','FILE', 'PAGE'])\n",
        "  #print(dtStatsORD)\n",
        "  print(\"Arquivo de sumário gerado \", ToolPath + \"Summary.xlsx\")\n",
        "  dtStatsORD.to_excel(ToolPath + \"Summary.xlsx\", index=False)  # index=False para não incluir o índice do DataFrame\n",
        "  return dtStatsORD"
      ],
      "metadata": {
        "id": "nndIJMzbUBrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gerando estatísticas\n",
        "\n",
        "import os\n",
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "#diretorio do img2tableDir\n",
        "Detr = \"/content/drive/MyDrive/DataSets/Certificados/Out/DETR/\"\n",
        "#diretorio de referencia GT\n",
        "GTDir = \"/content/drive/MyDrive/DataSets/Certificados/Out/GT/\"\n",
        "\n",
        "dirLabs = [] #INFORMAR AQUI A LISTA DAS PASTAS DE ARQUIVOS POR LABORATÓRIO\n",
        "\n",
        "for dirLab in dirLabs:\n",
        "\n",
        "  if not os.path.isfile(dirLab):\n",
        "    labPath =  Detr + dirLab\n",
        "    labPathGT =  GTDir + dirLab\n",
        "    #coleta os arquivos info para analise dos tablesID\n",
        "    filesINFO = getFiles(labPath, \"info\")\n",
        "    #coleta os tablesID para comparacao\n",
        "    lstInfoFiles = getInfoFiles(filesINFO)\n",
        "\n",
        "    #para cada tableID, gerar estatísticas\n",
        "    for dicInfoFile in lstInfoFiles:\n",
        "\n",
        "      print(\"dicInfoFile\", dicInfoFile)\n",
        "      fileInfo = dicInfoFile[\"TABLEID\"] + \"|\" + dicInfoFile[\"FILE\"] + \"|\" + dicInfoFile[\"PAGE\"] + \"_INFO.info\"\n",
        "      fileTokenBbox = dicInfoFile[\"TABLEID\"] + \"|\" + dicInfoFile[\"FILE\"] + \"|\" + dicInfoFile[\"PAGE\"] + \"_BBOX.bbox\"\n",
        "      fileHTML = dicInfoFile[\"TABLEID\"] + \"|\" + dicInfoFile[\"FILE\"] + \"|\" + dicInfoFile[\"PAGE\"] + \"_HTML.html\"\n",
        "\n",
        "      pathFileInfo = labPath + \"/\" + fileInfo\n",
        "      pathFileTokenBbox = labPath + \"/\" + fileTokenBbox\n",
        "      pathFileHTML = labPath + \"/\" + fileHTML\n",
        "\n",
        "      pathFileInfoGT = labPathGT + \"/\" + fileInfo\n",
        "      pathFileTokenBboxGT = labPathGT + \"/\" + fileTokenBbox\n",
        "      pathFileHTMLGT = labPathGT + \"/\" + fileHTML\n",
        "      print(\"pathFileInfo =\", pathFileInfo)\n",
        "\n",
        "      #gerando estatisticas do token e bbox\n",
        "      if os.path.exists(pathFileHTMLGT) and os.path.exists(pathFileTokenBbox) and os.path.exists(pathFileTokenBboxGT):\n",
        "\n",
        "        #print(\"pathFileHTML\", readFile(pathFileHTML).replace(\"\\n\", \" \"))\n",
        "        #print(\"pathFileHTMLGT\", readFile(pathFileHTMLGT).replace(\"\\n\", \" \"))\n",
        "\n",
        "        #carrega lista de tokens/bbox e HTMLs para comparacao do arquivo corrente com GT\n",
        "\n",
        "        #arquivo info analise (.info)\n",
        "        stringFileInfo = readFile(pathFileInfo)\n",
        "        stringFileInfo = stringFileInfo.replace(\"nan\", \"'NAN'\")\n",
        "        lstInfo = ast.literal_eval(stringFileInfo)\n",
        "        #arquivo info GT (.info)\n",
        "        lstInfoGT = ast.literal_eval(readFile(pathFileInfoGT))\n",
        "        #arquivo bbox (.bbox)\n",
        "        #print(\"pathFileTokenBbox\", pathFileTokenBbox)\n",
        "        lstTkBox = ast.literal_eval(readFile(pathFileTokenBbox))\n",
        "        #arquivo bbox GT (.bbox)\n",
        "        lstTkBoxGT = ast.literal_eval(readFile(pathFileTokenBboxGT))\n",
        "        #arquivo html (.html)\n",
        "        #print(\"pathFileHTML \", pathFileHTML)\n",
        "        strHTML = \"\".join(ast.literal_eval(readFile(pathFileHTML).replace(\"\\n\", \" \")))\n",
        "        #arquivo html GT(.html)\n",
        "        strHTMLGT = \"\".join(ast.literal_eval(readFile(pathFileHTMLGT).replace(\"\\n\", \" \")))\n",
        "        #break\n",
        "\n",
        "        #TEDS apenas funciona se tiver na estrutura html as tags html e body\n",
        "        if \"<body>\" not in strHTML:\n",
        "          strHTML = \"<body>\" + strHTML + \"</body>\"\n",
        "        if \"<html>\" not in strHTML:\n",
        "          strHTML = \"<html>\" + strHTML + \"</html>\"\n",
        "        if \"<body>\" not in strHTMLGT:\n",
        "          strHTMLGT = \"<body>\" + strHTMLGT + \"</body>\"\n",
        "        if \"<html>\" not in strHTMLGT:\n",
        "          strHTMLGT = \"<html>\" + strHTMLGT + \"</html>\"\n",
        "\n",
        "        qtdLinhas = int(lstInfoGT[\"DIMENSION\"].split(\"X\")[0])\n",
        "        qtdColunas = int(lstInfoGT[\"DIMENSION\"].split(\"X\")[1])\n",
        "        print(\"qtdLinhas \", qtdLinhas)\n",
        "        print(\"qtdColunas \", qtdColunas)\n",
        "        qtdCells = qtdLinhas * qtdColunas\n",
        "        print(\"qtdCells \", qtdCells)\n",
        "\n",
        "        #print(\"lstInfo\" , lstInfo)\n",
        "        #print(\"lstInfoGT\" , lstInfoGT)\n",
        "\n",
        "        lstBboxInfo = calStatsBboxInfo(lstInfo, lstInfoGT)\n",
        "        percAcertosBboxInfo = round((sum(lstBboxInfo) / len(lstBboxInfo))/100, 2)\n",
        "\n",
        "        #print(\"lstTkBox\",lstTkBox)\n",
        "        #print(\"lstTkBoxGT\",lstTkBoxGT)\n",
        "        lstStatsTokens, lstStatsBbox = calStatsTablesValues(lstTkBox, lstTkBoxGT)\n",
        "\n",
        "        qtdNANToken, qtdNANBbox = calStatsNAN(lstTkBox)\n",
        "\n",
        "        #calcula qtd de acertos (com similaridade 100% entre os valores dos tokens e tabelas)\n",
        "        qtdAcertosTokens = numTimes(lstStatsTokens, 100.0)\n",
        "        #qtdAcertosBbox = numTimes(lstStatsBbox, 100.0)\n",
        "        percAcertosBbox = round((sum(lstStatsBbox) / len(lstStatsBbox))/100, 2)\n",
        "\n",
        "        #calcula similidade maior que 95%\n",
        "        qtdTokensMaior95 = numTimesMoreThen(lstStatsTokens, 95)\n",
        "        qtdBboxMaior95 = numTimesMoreThen(lstStatsBbox, 95)\n",
        "\n",
        "        #calcula TEDS entre as estruturas HTMLs (img2table VS GT)\n",
        "        teds = TEDS()\n",
        "\n",
        "        scoreTEDS = round(teds.evaluate(strHTML, strHTMLGT), 6)\n",
        "\n",
        "        #salvado arquivo de estatística\n",
        "        fileStats = dicInfoFile[\"TABLEID\"] + \"|\" + dicInfoFile[\"FILE\"] + \"|\" + dicInfoFile[\"PAGE\"] + \"_STATS.stats\"\n",
        "        pathFileStats = labPath + \"/\" + fileStats\n",
        "\n",
        "        percAcertosTokens = round((qtdAcertosTokens / qtdCells), 2)\n",
        "        percNaoLidosBbox = round((qtdNANBbox / qtdCells), 2)\n",
        "        percNaoLidosToken = round((qtdNANToken / qtdCells), 2)\n",
        "\n",
        "        dicTableStats = {}\n",
        "        dicTableStats[\"LAB\"] = lstInfo[\"LAB\"]\n",
        "        dicTableStats[\"FILE\"] = dicInfoFile[\"FILE\"]\n",
        "        dicTableStats[\"PAGE\"] = dicInfoFile[\"PAGE\"]\n",
        "        dicTableStats[\"TABLEID\"] = dicInfoFile[\"TABLEID\"]\n",
        "        dicTableStats[\"DIMENSION\"] = lstInfo[\"DIMENSION\"]\n",
        "        dicTableStats[\"QTDCELLS\"] = qtdCells\n",
        "        dicTableStats[\"QTDACERTOSCELLS\"] = qtdAcertosTokens\n",
        "        dicTableStats[\"PERCACERTOSCELLS\"] = percAcertosTokens\n",
        "        dicTableStats[\"PERCACERTOSBBOXINFO\"] = percAcertosBboxInfo\n",
        "        dicTableStats[\"PERCACERTOSBBOX\"] = percAcertosBbox\n",
        "        dicTableStats[\"QTDNAOLIDOSBBOX\"] = qtdNANBbox\n",
        "        dicTableStats[\"PERCNAOLIDOSBBOX\"] = percNaoLidosBbox\n",
        "        dicTableStats[\"QTDNAOLIDOSTOKEN\"] = qtdNANToken\n",
        "        dicTableStats[\"PERCNAOLIDOSTOKEN\"] = percNaoLidosToken\n",
        "        #dicTableStats[\"QTDCELLSMAIOR95\"] = qtdTokensMaior95\n",
        "        #dicTableStats[\"QTDABBOXMAIOR95\"] = qtdBboxMaior95\n",
        "        dicTableStats[\"TEDS\"] = scoreTEDS\n",
        "\n",
        "        SaveFileStats (pathFileStats, dicTableStats)\n",
        "\n",
        "        print(\"Dimensão da tabela \", lstInfo[\"DIMENSION\"], \", total \", str(qtdCells), \" células\" )\n",
        "        print(\"qtdCells:\",qtdCells)\n",
        "        print(\"qtdAcertosTokens:\",qtdAcertosTokens)\n",
        "        print(\"percAcertosTokens:\",percAcertosTokens,\"/\",(percAcertosTokens*100),\"%\")\n",
        "        print(\"percAcertosBboxInfo:\",percAcertosBboxInfo,\"/\",(percAcertosBboxInfo*100),\"%\")\n",
        "        print(\"percAcertosBbox:\",percAcertosBbox,\"/\",(percAcertosBbox*100),\"%\")\n",
        "        print(\"qtdNaoLidosBbox:\",qtdNANBbox)\n",
        "        print(\"percNaoLidosBbox:\",percNaoLidosBbox,\"/\",(percNaoLidosBbox*100),\"%\")\n",
        "        print(\"qtdNaoLidosTokens:\",qtdNANToken)\n",
        "        print(\"percNaoLidosToken:\",percNaoLidosToken,\"/\",(percNaoLidosToken*100),\"%\")\n",
        "        #print(\"qtdAcertosTokens>95:\",qtdTokensMaior95)\n",
        "        #print(\"qtdAcertosBbox>95:\",qtdBboxMaior95)\n",
        "        print('TEDS score:', scoreTEDS,\"/\",round((scoreTEDS*100),2),\"%\")\n",
        "        #break # fim primeiro for\n",
        "\n",
        "  #break # fim segundo for\n",
        "#depois de gerar arquivos de estatística, montar sumário no EXCEL\n",
        "ExportCSVSummary(pubTables, GTDir)"
      ],
      "metadata": {
        "id": "vmjZNooPUkFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}