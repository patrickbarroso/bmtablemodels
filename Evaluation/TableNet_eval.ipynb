{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6ekQBe5ANgT"
      },
      "source": [
        "# Modelo Tablenet (Paliwal, 2019) - ajustado em abril/maio 2024\n",
        "Artigo: Tablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images\n",
        "https://arxiv.org/pdf/2001.01469.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Hr2pToBejA"
      },
      "source": [
        "# Carregando bibliotecas do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8olchVEC_zH"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall pillow==9.0.0 #rodar essa linha primeiro (que força reiniciar a sessão)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__mgPzZacW_T"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install pytorch_lightning\n",
        "!pip install pytesseract\n",
        "!pip install --upgrade --force-reinstall --no-deps albumentations\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pymupdf\n",
        "!pip install pdf2image\n",
        "!pip install --upgrade pillow==9.0.0\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_VujWSdrUjf"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "import shutil\n",
        "import pytz\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "try:\n",
        "    from PIL import Image #esse problema de import da classe Image deve ser considerado para demais modelos\n",
        "except ImportError:\n",
        "    import Image\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VMraYR9dXds"
      },
      "outputs": [],
      "source": [
        "#download do modelo (best_modelo.ckpt) - nao precisa carregar de novo pois o modelo já foi baixado\n",
        "!gdown --id 19nNp42l0rN1epa9AwEQEqGX5fbImMV6s\n",
        "#download de exemplos de imagens para predição (samples.rar)\n",
        "!gdown --id 1xiEUoeV_L8kpd2BwHyD8Grz2OCRMGjCn\n",
        "!unrar x /content/samples.rar\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySqemd3_cVBv"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision.models import vgg19, vgg19_bn #(COMENTEI POIS DEU ERRO)\n",
        "from collections import OrderedDict\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from albumentations import Compose\n",
        "from PIL import Image\n",
        "from pytesseract import image_to_string\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage.segmentation import clear_border\n",
        "from skimage.measure import label, regionprops\n",
        "from skimage.morphology import closing, square, convex_hull_image\n",
        "from skimage.transform import resize\n",
        "from skimage.util import invert\n",
        "import cv2\n",
        "#from tablenet import TableNetModule\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "#funcao para converter PDF para png\n",
        "from pdf2image import convert_from_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT0HyMW7BX_E"
      },
      "source": [
        "# Classe e Funções do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mc30sLpVOyS"
      },
      "outputs": [],
      "source": [
        "\"\"\"TableNet Module.\"\"\"\n",
        "\n",
        "EPSILON = 1e-15\n",
        "\n",
        "\n",
        "class TableNetModule(pl.LightningModule):\n",
        "    \"\"\"Pytorch Lightning Module for TableNet.\"\"\"\n",
        "\n",
        "    def __init__(self, num_class: int = 1, batch_norm: bool = False):\n",
        "        \"\"\"Initialize TableNet Module.\n",
        "\n",
        "        Args:\n",
        "            num_class (int): Number of classes per point.\n",
        "            batch_norm (bool): Select VGG with or without batch normalization.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = TableNet(num_class, batch_norm)\n",
        "        self.num_class = num_class\n",
        "        self.dice_loss = DiceLoss()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        \"\"\"Perform forward-pass.\n",
        "\n",
        "        Args:\n",
        "            batch (tensor): Batch of images to perform forward-pass.\n",
        "\n",
        "        Returns (Tuple[tensor, tensor]): Table, Column prediction.\n",
        "        \"\"\"\n",
        "        return self.model(batch)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"Get training step.\n",
        "\n",
        "        Args:\n",
        "            batch (List[Tensor]): Data for training.\n",
        "            batch_idx (int): batch index.\n",
        "\n",
        "        Returns: Tensor\n",
        "        \"\"\"\n",
        "        samples, labels_table, labels_column = batch\n",
        "        output_table, output_column = self.forward(samples)\n",
        "\n",
        "        loss_table = self.dice_loss(output_table, labels_table)\n",
        "        loss_column = self.dice_loss(output_column, labels_column)\n",
        "\n",
        "        self.log('train_loss_table', loss_table)\n",
        "        self.log('train_loss_column', loss_column)\n",
        "        self.log('train_loss', loss_column + loss_table)\n",
        "        return loss_table + loss_column\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"Get validation step.\n",
        "\n",
        "        Args:\n",
        "            batch (List[Tensor]): Data for training.\n",
        "            batch_idx (int): batch index.\n",
        "\n",
        "        Returns: Tensor\n",
        "        \"\"\"\n",
        "        samples, labels_table, labels_column = batch\n",
        "        output_table, output_column = self.forward(samples)\n",
        "\n",
        "        loss_table = self.dice_loss(output_table, labels_table)\n",
        "        loss_column = self.dice_loss(output_column, labels_column)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            self._log_images(\"validation\", samples, labels_table, labels_column, output_table, output_column)\n",
        "\n",
        "        self.log('valid_loss_table', loss_table, on_epoch=True)\n",
        "        self.log('valid_loss_column', loss_column, on_epoch=True)\n",
        "        self.log('validation_loss', loss_column + loss_table, on_epoch=True)\n",
        "        self.log('validation_iou_table', binary_mean_iou(output_table, labels_table), on_epoch=True)\n",
        "        self.log('validation_iou_column', binary_mean_iou(output_column, labels_column), on_epoch=True)\n",
        "        return loss_table + loss_column\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"Get test step.\n",
        "\n",
        "        Args:\n",
        "            batch (List[Tensor]): Data for training.\n",
        "            batch_idx (int): batch index.\n",
        "\n",
        "        Returns: Tensor\n",
        "        \"\"\"\n",
        "        samples, labels_table, labels_column = batch\n",
        "        output_table, output_column = self.forward(samples)\n",
        "\n",
        "        loss_table = self.dice_loss(output_table, labels_table)\n",
        "        loss_column = self.dice_loss(output_column, labels_column)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            self._log_images(\"test\", samples, labels_table, labels_column, output_table, output_column)\n",
        "\n",
        "        self.log('test_loss_table', loss_table, on_epoch=True)\n",
        "        self.log('test_loss_column', loss_column, on_epoch=True)\n",
        "        self.log('test_loss', loss_column + loss_table, on_epoch=True)\n",
        "        self.log('test_iou_table', binary_mean_iou(output_table, labels_table), on_epoch=True)\n",
        "        self.log('test_iou_column', binary_mean_iou(output_column, labels_column), on_epoch=True)\n",
        "        return loss_table + loss_column\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Configure optimizer for pytorch lighting.\n",
        "\n",
        "        Returns: optimizer and scheduler for pytorch lighting.\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = optim.SGD(self.parameters(), lr=0.0001)\n",
        "        scheduler = {\n",
        "            'scheduler': optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                       max_lr=0.0001, steps_per_epoch=204, epochs=500, pct_start=0.1),\n",
        "            'interval': 'step',\n",
        "        }\n",
        "\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def _log_images(self, mode, samples, labels_table, labels_column, output_table, output_column):\n",
        "        \"\"\"Log image on to logger.\"\"\"\n",
        "        self.logger.experiment.add_images(f'{mode}_generated_images', samples[0:4], self.current_epoch)\n",
        "        self.logger.experiment.add_images(f'{mode}_labels_table', labels_table[0:4], self.current_epoch)\n",
        "        self.logger.experiment.add_images(f'{mode}_labels_column', labels_column[0:4], self.current_epoch)\n",
        "        self.logger.experiment.add_images(f'{mode}_output_table', output_table[0:4], self.current_epoch)\n",
        "        self.logger.experiment.add_images(f'{mode}_output_column', output_column[0:4], self.current_epoch)\n",
        "\n",
        "\n",
        "class TableNet(nn.Module):\n",
        "    \"\"\"TableNet.\"\"\"\n",
        "\n",
        "    def __init__(self, num_class: int, batch_norm: bool = False):\n",
        "        \"\"\"Initialize TableNet.\n",
        "\n",
        "        Args:\n",
        "            num_class (int): Number of classes per point.\n",
        "            batch_norm (bool): Select VGG with or without batch normalization.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vgg = vgg19(pretrained=True).features if not batch_norm else vgg19_bn(pretrained=True).features\n",
        "        self.layers = [18, 27] if not batch_norm else [26, 39]\n",
        "        self.model = nn.Sequential(nn.Conv2d(512, 512, kernel_size=1),\n",
        "                                   nn.ReLU(inplace=True),\n",
        "                                   nn.Dropout(0.8),\n",
        "                                   nn.Conv2d(512, 512, kernel_size=1),\n",
        "                                   nn.ReLU(inplace=True),\n",
        "                                   nn.Dropout(0.8))\n",
        "        self.table_decoder = TableDecoder(num_class)\n",
        "        self.column_decoder = ColumnDecoder(num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (tensor): Batch of images to perform forward-pass.\n",
        "\n",
        "        Returns (Tuple[tensor, tensor]): Table, Column prediction.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for i, layer in enumerate(self.vgg):\n",
        "            x = layer(x)\n",
        "            if i in self.layers:\n",
        "                results.append(x)\n",
        "        x_table = self.table_decoder(x, results)\n",
        "        x_column = self.column_decoder(x, results)\n",
        "        return torch.sigmoid(x_table), torch.sigmoid(x_column)\n",
        "\n",
        "\n",
        "class ColumnDecoder(nn.Module):\n",
        "    \"\"\"Column Decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int):\n",
        "        \"\"\"Initialize Column Decoder.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes per point.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.8),\n",
        "            nn.Conv2d(512, 512, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer = nn.ConvTranspose2d(1280, num_classes, kernel_size=2, stride=2, dilation=1)\n",
        "\n",
        "    def forward(self, x, pools):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (tensor): Batch of images to perform forward-pass.\n",
        "            pools (Tuple[tensor, tensor]): The 3 and 4 pooling layer from VGG-19.\n",
        "\n",
        "        Returns (tensor): Forward-pass result tensor.\n",
        "\n",
        "        \"\"\"\n",
        "        pool_3, pool_4 = pools\n",
        "        x = self.decoder(x)\n",
        "        x = F.interpolate(x, scale_factor=2)\n",
        "        x = torch.cat([x, pool_4], dim=1)\n",
        "        x = F.interpolate(x, scale_factor=2)\n",
        "        x = torch.cat([x, pool_3], dim=1)\n",
        "        x = F.interpolate(x, scale_factor=2)\n",
        "        x = F.interpolate(x, scale_factor=2)\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class TableDecoder(ColumnDecoder):\n",
        "    \"\"\"Table Decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        \"\"\"Initialize Table decoder.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes per point.\n",
        "        \"\"\"\n",
        "        super().__init__(num_classes)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Dice Loss.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \"\"\"Calculate loss.\n",
        "\n",
        "        Args:\n",
        "            inputs (tensor): Output from the forward pass.\n",
        "            targets (tensor): Labels.\n",
        "            smooth (float): Value to smooth the loss.\n",
        "\n",
        "        Returns (tensor): Dice loss.\n",
        "\n",
        "        \"\"\"\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "\n",
        "def binary_mean_iou(inputs, targets):\n",
        "    \"\"\"Calculate binary mean intersection over union.\n",
        "\n",
        "    Args:\n",
        "        inputs (tensor): Output from the forward pass.\n",
        "        targets (tensor): Labels.\n",
        "\n",
        "    Returns (tensor): Intersection over union value.\n",
        "    \"\"\"\n",
        "    output = (inputs > 0).int()\n",
        "\n",
        "    if output.shape != targets.shape:\n",
        "        targets = torch.squeeze(targets, 1)\n",
        "\n",
        "    intersection = (targets * output).sum()\n",
        "\n",
        "    union = targets.sum() + output.sum() - intersection\n",
        "\n",
        "    result = (intersection + EPSILON) / (union + EPSILON)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YswXxRl2VXT-"
      },
      "outputs": [],
      "source": [
        "\"\"\"Marmot Dataset Module.\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from albumentations import Compose\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class MarmotDataset(Dataset):\n",
        "    \"\"\"Marmot Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data: List[Path], transforms: Compose = None) -> None:\n",
        "        \"\"\"Marmot Dataset initialization.\n",
        "\n",
        "        Args:\n",
        "            data (List[Path]): A list of Path.\n",
        "            transforms (Optional[Compose]): Compose object from albumentations.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Dataset Length.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"Get sample data.\n",
        "\n",
        "        Args:\n",
        "            item (int): sample id.\n",
        "\n",
        "        Returns (Tuple[tensor, tensor, tensor]): Image, Table Mask, Column Mask\n",
        "        \"\"\"\n",
        "        sample_id = self.data[item].stem\n",
        "\n",
        "        image_path = self.data[item]\n",
        "        table_path = self.data[item].parent.parent.joinpath(\"table_mask\", sample_id + \".bmp\")\n",
        "        column_path = self.data[item].parent.parent.joinpath(\"column_mask\", sample_id + \".bmp\")\n",
        "\n",
        "        image = np.array(Image.open(image_path))\n",
        "        table_mask = np.expand_dims(np.array(Image.open(table_path)), axis=2)\n",
        "        column_mask = np.expand_dims(np.array(Image.open(column_path)), axis=2)\n",
        "        mask = np.concatenate([table_mask, column_mask], axis=2) / 255\n",
        "        sample = {\"image\": image, \"mask\": mask}\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image=image, mask=mask)\n",
        "\n",
        "        image = sample[\"image\"]\n",
        "        mask_table = sample[\"mask\"][:, :, 0].unsqueeze(0)\n",
        "        mask_column = sample[\"mask\"][:, :, 1].unsqueeze(0)\n",
        "        return image, mask_table, mask_column\n",
        "\n",
        "\n",
        "class MarmotDataModule(pl.LightningDataModule):\n",
        "    \"\"\"Pytorch Lightning Data Module for Marmot.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str = \"./data\", transforms_preprocessing: Compose = None,\n",
        "                 transforms_augmentation: Compose = None, batch_size: int = 8, num_workers: int = 4):\n",
        "        \"\"\"Marmot Data Module initialization.\n",
        "\n",
        "        Args:\n",
        "            data_dir (str): Dataset directory.\n",
        "            transforms_preprocessing (Optional[Compose]): Compose object from albumentations applied\n",
        "             on validation an test dataset.\n",
        "            transforms_augmentation (Optional[Compose]): Compose object from albumentations applied\n",
        "             on training dataset.\n",
        "            batch_size (int): Define batch size.\n",
        "            num_workers (int): Define number of workers to process data.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.data = list(Path(data_dir).rglob(\"*.bmp\"))\n",
        "        self.transforms_preprocessing = transforms_preprocessing\n",
        "        self.transforms_augmentation = transforms_augmentation\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage: str = None) -> None:\n",
        "        \"\"\"Start training, validation and test datasets.\n",
        "\n",
        "        Args:\n",
        "            stage (Optional[str]): Used to separate setup logic for trainer.fit and trainer.test.\n",
        "        \"\"\"\n",
        "        n_samples = len(self.data)\n",
        "        self.data.sort()\n",
        "        train_slice = slice(0, int(n_samples * 0.8))\n",
        "        val_slice = slice(int(n_samples * 0.8), int(n_samples * 0.9))\n",
        "        test_slice = slice(int(n_samples * 0.9), n_samples)\n",
        "\n",
        "        self.complaint_train = MarmotDataset(self.data[train_slice], transforms=self.transforms_augmentation)\n",
        "        self.complaint_val = MarmotDataset(self.data[val_slice], transforms=self.transforms_preprocessing)\n",
        "        self.complaint_test = MarmotDataset(self.data[test_slice], transforms=self.transforms_preprocessing)\n",
        "\n",
        "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
        "        \"\"\"Create Dataloader.\n",
        "\n",
        "        Returns: DataLoader\n",
        "        \"\"\"\n",
        "        return DataLoader(self.complaint_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self, *args, **kwargs) -> DataLoader:\n",
        "        \"\"\"Create Dataloader.\n",
        "\n",
        "        Returns: DataLoader\n",
        "        \"\"\"\n",
        "        return DataLoader(self.complaint_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self, *args, **kwargs) -> DataLoader:\n",
        "        \"\"\"Create Dataloader.\n",
        "\n",
        "        Returns: DataLoader\n",
        "        \"\"\"\n",
        "        return DataLoader(self.complaint_test, batch_size=self.batch_size, num_workers=self.num_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpCIMtnpcx-P"
      },
      "outputs": [],
      "source": [
        "class Predict:\n",
        "    \"\"\"Predict images using pre-trained model.\"\"\"\n",
        "\n",
        "    def __init__(self, checkpoint_path: str, transforms: Compose, threshold: float = 0.5, per: float = 0.005):\n",
        "        \"\"\"Predict images using pre-trained TableNet model.\n",
        "        Args:\n",
        "            checkpoint_path (str): model weights path.\n",
        "            transforms (Optional[Compose]): Compose object from albumentations used for pre-processing.\n",
        "            threshold (float): threshold to consider the value as correctly classified.\n",
        "            per (float): Minimum area for tables and columns to be considered.\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "        self.threshold = threshold\n",
        "        self.per = per\n",
        "\n",
        "        self.model = TableNetModule.load_from_checkpoint(checkpoint_path)\n",
        "        self.model.eval()\n",
        "        self.model.requires_grad_(False)\n",
        "        self.inp_img = \"\"\n",
        "\n",
        "        #====>ALTERACAO 1\n",
        "        # Move o modelo para a GPU, se disponível\n",
        "        if torch.cuda.is_available():\n",
        "          self.model.cuda()\n",
        "\n",
        "    def predict(self, image: Image) -> List[pd.DataFrame]:\n",
        "        \"\"\"Predict a image table values.\n",
        "        Args:\n",
        "            image (Image): PIL.Image to\n",
        "        Returns (List[pd.DataFrame]): Tables in pandas DataFrame format.\n",
        "        \"\"\"\n",
        "        processed_image = self.transforms(image=np.array(image))[\"image\"]\n",
        "\n",
        "        #====>ALTERACAO 2\n",
        "        # Move a imagem processada para a GPU, se disponível\n",
        "        if torch.cuda.is_available():\n",
        "          processed_image = processed_image.cuda()\n",
        "\n",
        "        self.inp_img = cv2.resize(np.array(image), (896, 896))\n",
        "        #cv2_imshow(self.inp_img) #AQUI EXIBE AS IMAGENS TRABALHADAS 1 DE 3 (COMENTEI PARA NAO FICAR PESADO)\n",
        "\n",
        "        #====>ALTERACAO 3\n",
        "        # Faz a predição na GPU\n",
        "        with torch.no_grad():\n",
        "          table_mask, column_mask = self.model.forward(processed_image.unsqueeze(0))\n",
        "\n",
        "        #====>ALTERACAO 4\n",
        "        # Move as máscaras para a CPU para processamento adicional\n",
        "        table_mask = table_mask.cpu()\n",
        "        column_mask = column_mask.cpu()\n",
        "\n",
        "        table_mask, column_mask = self.model.forward(processed_image.unsqueeze(0))\n",
        "\n",
        "        table_mask = self._apply_threshold(table_mask)\n",
        "        column_mask = self._apply_threshold(column_mask)\n",
        "        # print(type(table_mask))\n",
        "        # print(type(column_mask))\n",
        "        # print(table_mask.shape)\n",
        "        # print(np.unique(table_mask))\n",
        "        # print(column_mask.shape)\n",
        "        # print(np.unique(column_mask))\n",
        "        # cv2_imshow((table_mask*255).astype(np.uint8))\n",
        "        # cv2_imshow((column_mask*255).astype(np.uint8))\n",
        "\n",
        "        segmented_tables = self._process_tables(self._segment_image(table_mask))\n",
        "        # print(type(segmented_tables[0]))\n",
        "        # print(segmented_tables[0].shape)\n",
        "        # for i in range(len(segmented_tables)):\n",
        "        #     cv2_imshow((segmented_tables[i]*255).astype(np.uint8))\n",
        "\n",
        "        # for table in segmented_tables:\n",
        "        #     abc = self._segment_image(column_mask*table)\n",
        "        #     cv2_imshow(abc*255)\n",
        "        #cv2_imshow(self.inp_img*cv2.merge((table_mask, table_mask, table_mask))) #AQUI EXIBE AS IMAGENS 2 DE 3 TRABALHADAS (COMENTEI PARA NAO FICAR PESADO)\n",
        "        #cv2_imshow(self.inp_img*cv2.merge((column_mask, column_mask, column_mask))) #AQUI EXIBE AS IMAGENS 3 DE 3 TRABALHADAS (COMENTEI PARA NAO FICAR PESADO)\n",
        "        tables = []\n",
        "        for table in segmented_tables:\n",
        "            segmented_columns = self._process_columns(self._segment_image(column_mask * table))\n",
        "            if segmented_columns:\n",
        "                cols = []\n",
        "                for column in segmented_columns.values():\n",
        "                    #cv2_imshow(self.inp_img*cv2.merge((column, column, column)))\n",
        "                    cols.append(self._column_to_dataframe(column, image))\n",
        "                tables.append(pd.concat(cols, ignore_index=True, axis=1))\n",
        "        return tables\n",
        "\n",
        "    def _apply_threshold(self, mask):\n",
        "\n",
        "        #mask = mask.squeeze(0).squeeze(0).numpy() > self.threshold\n",
        "\n",
        "        #ALTERACAO 5\n",
        "        mask = mask.cpu().squeeze(0).squeeze(0).numpy() > self.threshold\n",
        "\n",
        "        return mask.astype(int)\n",
        "\n",
        "    def _process_tables(self, segmented_tables):\n",
        "        width, height = segmented_tables.shape\n",
        "        tables = []\n",
        "        for i in np.unique(segmented_tables)[1:]:\n",
        "            table = np.where(segmented_tables == i, 1, 0)\n",
        "            if table.sum() > height * width * self.per:\n",
        "                tables.append(convex_hull_image(table))\n",
        "\n",
        "        return tables\n",
        "\n",
        "    def _process_columns(self, segmented_columns):\n",
        "        width, height = segmented_columns.shape\n",
        "        cols = {}\n",
        "        for j in np.unique(segmented_columns)[1:]:\n",
        "            column = np.where(segmented_columns == j, 1, 0)\n",
        "            column = column.astype(int)\n",
        "\n",
        "            if column.sum() > width * height * self.per:\n",
        "                position = regionprops(column)[0].centroid[1]\n",
        "                cols[position] = column\n",
        "        return OrderedDict(sorted(cols.items()))\n",
        "\n",
        "    @staticmethod\n",
        "    def _segment_image(image):\n",
        "        thresh = threshold_otsu(image)\n",
        "        bw = closing(image > thresh, square(2))\n",
        "        cleared = clear_border(bw)\n",
        "        label_image = label(cleared)\n",
        "        return label_image\n",
        "\n",
        "    @staticmethod\n",
        "    def _column_to_dataframe(column, image):\n",
        "        width, height = image.size\n",
        "        column = resize(np.expand_dims(column, axis=2), (height, width), preserve_range=True) > 0.01\n",
        "\n",
        "        crop = column * image\n",
        "        white = np.ones(column.shape) * invert(column) * 255\n",
        "        crop = crop + white\n",
        "        ocr = image_to_string(Image.fromarray(crop.astype(np.uint8)))\n",
        "        return pd.DataFrame({\"col\": [value for value in ocr.split(\"\\n\") if len(value) > 0]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMg9Ng46dNHp"
      },
      "outputs": [],
      "source": [
        "def predict(image_path: str, model_weights: str) -> List[pd.DataFrame]:\n",
        "    \"\"\"Predict table content.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): image path.\n",
        "        model_weights (str): model weights path.\n",
        "\n",
        "    Returns (List[pd.DataFrame]): Tables in pandas DataFrame format.\n",
        "    \"\"\"\n",
        "    import albumentations as album\n",
        "    from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "    transforms = album.Compose([\n",
        "        album.Resize(896, 896, always_apply=True),\n",
        "        album.Normalize(),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    pred = Predict(model_weights, transforms)\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    x = pred.predict(image)\n",
        "    #print(pred.predict(image))\n",
        "\n",
        "    #ajustei aqui para retornar a lista de dataframes\n",
        "    return (x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKGS77PRCwhd"
      },
      "source": [
        "# Funções Pré-processamento para MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44NmJPwEX3M9",
        "outputId": "1bb4e2e4-f770-456b-aaae-6a2c8bd89ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9As8t_6GV5jv"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install pymupdf\n",
        "!pip install pdf2image\n",
        "!apt-get install poppler-utils\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGVLM43bV5-W"
      },
      "outputs": [],
      "source": [
        "#Bibliotecas\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from huggingface_hub import hf_hub_download\n",
        "import fitz\n",
        "import os\n",
        "import pandas as pd\n",
        "from pdf2image import convert_from_path\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv\n",
        "from tqdm.auto import tqdm\n",
        "import cv2\n",
        "from bs4 import BeautifulSoup as bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X4KbnXAV-mQ"
      },
      "outputs": [],
      "source": [
        "#caminho dos certificados para analise\n",
        "CERT_PATH = \"/content/drive/MyDrive/DataSets/Certificados/In/\"\n",
        "\n",
        "#caminho de saida para geração das anotações das tabelas\n",
        "OUT_PATH = \"/content/drive/MyDrive/DataSets/Certificados/Out/Test/\"\n",
        "\n",
        "#diretorio dos arquivos do GT - Ground Truth (REFERENCIA para comparacao das tabelas)\n",
        "GT_PATH = \"/content/drive/MyDrive/DataSets/Certificados/Out/GT/\"\n",
        "\n",
        "#diretorio de escrita de arquivos\n",
        "DIROUT_TABLENET = \"/content/drive/MyDrive/DataSets/Certificados/Out/TableNet/\"\n",
        "\n",
        "#caminho do modelo TABLENET\n",
        "TABLENET_MODEL_PATH = '/content/drive/MyDrive/DataSets/Tablenet/Tablenet.ckpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bglnAFc6WKcs"
      },
      "outputs": [],
      "source": [
        "#funcao para varrer os arquivos PDF dos laboratórios nas pastas e retorna um dataframe\n",
        "#! o método está bem lento (deve verificar o motivo posteriormente)\n",
        "def InfoPDF(pathPDF):\n",
        "\n",
        "  lenPages = 0\n",
        "  doc = fitz.open(pathPDF)\n",
        "\n",
        "  lenPages = len(doc)\n",
        "  for pagina in doc:\n",
        "    isText = bool(pagina.get_text())\n",
        "    break\n",
        "\n",
        "  typeArq = [\"TEXT\" if isText else \"IMAGE\"]\n",
        "\n",
        "  return typeArq, lenPages\n",
        "\n",
        "def listFiles_OLD(CERT_PATH):\n",
        "\n",
        "  dfArq = pd.DataFrame()\n",
        "  dirs = [nome for nome in os.listdir(CERT_PATH) if os.path.isdir(os.path.join(CERT_PATH, nome))]\n",
        "\n",
        "  i = 0\n",
        "  for dir in dirs:\n",
        "\n",
        "    #coletando dados do diretorio (id, laboratorio)\n",
        "    lstDir = dir.split(\"_\")\n",
        "\n",
        "    if(len(lstDir)==3):\n",
        "\n",
        "      for file in os.listdir(os.path.join(CERT_PATH, dir)):\n",
        "\n",
        "        #é arquivo PDF\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "\n",
        "          pathFile = CERT_PATH + dir + \"/\" + file\n",
        "          dfArq.at[i,\"LAB\"] = lstDir[2]\n",
        "          dfArq.at[i,'PATH'] = pathFile\n",
        "\n",
        "          typeArq, qtdPages = InfoPDF(pathFile)\n",
        "          dfArq.at[i,'TYPE'] = typeArq\n",
        "          dfArq.at[i,'QTDPAGES'] = qtdPages\n",
        "\n",
        "          i = i + 1\n",
        "\n",
        "  return dfArq\n",
        "\n",
        "def listFiles(CERT_PATH, LAB_PATH):\n",
        "\n",
        "  dfArq = pd.DataFrame()\n",
        "\n",
        "  if LAB_PATH is None:\n",
        "    dirs = [nome for nome in os.listdir(CERT_PATH) if os.path.isdir(os.path.join(CERT_PATH, nome))]\n",
        "  else:\n",
        "    dirs = [LAB_PATH]\n",
        "\n",
        "  i = 0\n",
        "  for dir in dirs:\n",
        "\n",
        "    #coletando dados do diretorio (id, laboratorio)\n",
        "    lstDir = dir.split(\"_\")\n",
        "    print(\"lstDir \", lstDir);\n",
        "\n",
        "    if(len(lstDir)==3):\n",
        "\n",
        "      for file in os.listdir(os.path.join(CERT_PATH, dir)):\n",
        "\n",
        "        #é arquivo PDF\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "\n",
        "          pathFile = CERT_PATH + dir + \"/\" + file\n",
        "          print(\"pathFile \", pathFile);\n",
        "          dfArq.at[i,\"LAB\"] = lstDir[2]\n",
        "          dfArq.at[i,'PATH'] = pathFile\n",
        "\n",
        "          typeArq, qtdPages = InfoPDF(pathFile)\n",
        "          dfArq.at[i,'TYPE'] = typeArq\n",
        "          dfArq.at[i,'QTDPAGES'] = qtdPages\n",
        "\n",
        "          i = i + 1\n",
        "\n",
        "  return dfArq\n",
        "\n",
        "#dfArq = listFiles(CERT_PATH)\n",
        "\n",
        "def deleteFiles2(dirpath):\n",
        "  # Obtém a lista de arquivos no diretório\n",
        "  files = os.listdir(dirpath)\n",
        "\n",
        "  # Itera sobre os arquivos e os remove\n",
        "  for file in files:\n",
        "    filepath = os.path.join(dirpath, file)\n",
        "    if os.path.isfile(filepath):\n",
        "      os.remove(filepath)\n",
        "\n",
        "def is_image_by_extension(file_path):\n",
        "  image_extensions = ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'webp']  # Adicione outras extensões se necessário\n",
        "  file_extension = file_path.lower().split('.')[-1]\n",
        "  return file_extension in image_extensions\n",
        "\n",
        "def is_image(file_path):\n",
        "    try:\n",
        "        # Tenta abrir o arquivo como uma imagem\n",
        "        Image(file_path)\n",
        "        return True\n",
        "    except IOError:\n",
        "        # Se não for possível abrir como uma imagem, retorna False\n",
        "        return False\n",
        "\n",
        "def pdf_page_to_png(pdf_path, page_number, output_path):\n",
        "  # Convertendo a página do PDF para uma lista de imagens\n",
        "  images = convert_from_path(pdf_path, first_page=page_number, last_page=page_number)\n",
        "\n",
        "  # Salvando a imagem como PNG\n",
        "  images[0].save(output_path, 'PNG')\n",
        "\n",
        "class MaxResize(object):\n",
        "  def __init__(self, max_size=800):\n",
        "      self.max_size = max_size\n",
        "\n",
        "  def __call__(self, image):\n",
        "      width, height = image.size\n",
        "      current_max_size = max(width, height)\n",
        "      scale = self.max_size / current_max_size\n",
        "      resized_image = image.resize((int(round(scale*width)), int(round(scale*height))))\n",
        "\n",
        "      return resized_image\n",
        "\n",
        "def get_cell_coordinates_by_row(table_data):\n",
        "  # Extract rows and columns\n",
        "  rows = [entry for entry in table_data if entry['label'] == 'table row']\n",
        "  columns = [entry for entry in table_data if entry['label'] == 'table column']\n",
        "\n",
        "  # Sort rows and columns by their Y and X coordinates, respectively\n",
        "  rows.sort(key=lambda x: x['bbox'][1])\n",
        "  columns.sort(key=lambda x: x['bbox'][0])\n",
        "\n",
        "  # Function to find cell coordinates\n",
        "  def find_cell_coordinates(row, column):\n",
        "      cell_bbox = [column['bbox'][0], row['bbox'][1], column['bbox'][2], row['bbox'][3]]\n",
        "      return cell_bbox\n",
        "\n",
        "  # Generate cell coordinates and count cells in each row\n",
        "  cell_coordinates = []\n",
        "\n",
        "  for row in rows:\n",
        "      row_cells = []\n",
        "      for column in columns:\n",
        "          cell_bbox = find_cell_coordinates(row, column)\n",
        "          row_cells.append({'column': column['bbox'], 'cell': cell_bbox})\n",
        "\n",
        "      # Sort cells in the row by X coordinate\n",
        "      row_cells.sort(key=lambda x: x['column'][0])\n",
        "\n",
        "      # Append row information to cell_coordinates\n",
        "      cell_coordinates.append({'row': row['bbox'], 'cells': row_cells, 'cell_count': len(row_cells)})\n",
        "\n",
        "  # Sort rows from top to bottom\n",
        "  cell_coordinates.sort(key=lambda x: x['row'][1])\n",
        "\n",
        "  return cell_coordinates\n",
        "\n",
        "def aumentar_qualidade_e_contraste(imagem_path, fator_contraste, fator_brilho):\n",
        "  # Carregar a imagem\n",
        "  imagem = cv2.imread(imagem_path)\n",
        "\n",
        "  # Converter a imagem para o espaço de cores LAB (Luminância, Azul, Vermelho)\n",
        "  lab = cv2.cvtColor(imagem, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "  # Separar os canais L, A, B\n",
        "  l, a, b = cv2.split(lab)\n",
        "\n",
        "  # Aplicar o aumento de contraste na imagem L (luminância)\n",
        "  l = cv2.add(l, fator_brilho)\n",
        "  l = cv2.multiply(l, fator_contraste)\n",
        "\n",
        "  # Mesclar novamente os canais LAB\n",
        "  lab = cv2.merge((l, a, b))\n",
        "\n",
        "  # Converter a imagem de volta para o espaço de cores BGR\n",
        "  imagem_contraste = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "  return imagem_contraste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rje0AIpiWLVL"
      },
      "outputs": [],
      "source": [
        "#FUNÇÕES DE MANIPULACAO DE IMAGENS E ARQUIVOS\n",
        "\n",
        "#ler dados da celula\n",
        "def apply_ocr(cell_coordinates):\n",
        "    # let's OCR row by row\n",
        "    data = dict()\n",
        "    max_num_columns = 0\n",
        "    for idx, row in enumerate(tqdm(cell_coordinates)):\n",
        "      row_text = []\n",
        "      for cell in row[\"cells\"]:\n",
        "        # crop cell out of image\n",
        "        cell_image = np.array(cropped_table.crop(cell[\"cell\"]))\n",
        "        # apply OCR\n",
        "        result = reader.readtext(np.array(cell_image))\n",
        "        if len(result) > 0:\n",
        "          # print([x[1] for x in list(result)])\n",
        "          text = \" \".join([x[1] for x in result])\n",
        "          row_text.append(text)\n",
        "\n",
        "      if len(row_text) > max_num_columns:\n",
        "          max_num_columns = len(row_text)\n",
        "\n",
        "      data[idx] = row_text\n",
        "\n",
        "    #print(\"Max number of columns:\", max_num_columns)\n",
        "\n",
        "    # pad rows which don't have max_num_columns elements\n",
        "    # to make sure all rows have the same number of columns\n",
        "    for row, row_data in data.copy().items():\n",
        "        if len(row_data) != max_num_columns:\n",
        "          row_data = row_data + [\"\" for _ in range(max_num_columns - len(row_data))]\n",
        "        data[row] = row_data\n",
        "\n",
        "    return data\n",
        "\n",
        "#similaridade de strings conhecido como \"Distância de Levenshtein\"\n",
        "def calcPercSimStrings(str1, str2):\n",
        "\n",
        "  #retirando quebra de linhas da string\n",
        "  str1 = str1.replace(\"\\n\", \" \")\n",
        "  str2 = str2.replace(\"\\n\", \" \")\n",
        "\n",
        "  tamanho_str1 = len(str1)\n",
        "  tamanho_str2 = len(str2)\n",
        "\n",
        "  matriz = [[0] * (tamanho_str2 + 1) for _ in range(tamanho_str1 + 1)]\n",
        "\n",
        "  for i in range(tamanho_str1 + 1):\n",
        "    matriz[i][0] = i\n",
        "\n",
        "  for j in range(tamanho_str2 + 1):\n",
        "    matriz[0][j] = j\n",
        "\n",
        "  for i in range(1, tamanho_str1 + 1):\n",
        "    for j in range(1, tamanho_str2 + 1):\n",
        "        if str1[i - 1] == str2[j - 1]:\n",
        "            custo_substituicao = 0\n",
        "        else:\n",
        "            custo_substituicao = 1\n",
        "        matriz[i][j] = min(matriz[i - 1][j] + 1,       # Deletar\n",
        "                            matriz[i][j - 1] + 1,       # Inserir\n",
        "                              matriz[i - 1][j - 1] + custo_substituicao)  # Substituir\n",
        "\n",
        "  distancia = matriz[tamanho_str1][tamanho_str2]\n",
        "  maximo_tamanho = max(tamanho_str1, tamanho_str2)\n",
        "\n",
        "  similaridade = 0\n",
        "  if maximo_tamanho > 0:\n",
        "    similaridade = (maximo_tamanho - distancia) / maximo_tamanho\n",
        "  #print (\" similaridade entre {0} e {1}: {2}\".format(str1, str2, similaridade * 100))\n",
        "  return similaridade * 100\n",
        "\n",
        "#funcao que compara o valor de duas listas e calcula a media do percentual de similaridade entre eles\n",
        "#(para calcular o valor do bbox das tabelas e células das tabelas)\n",
        "\n",
        "def calcPercSimValueLists(lista1, lista2):\n",
        "  if len(lista1) != len(lista2):\n",
        "      print(\"calcPercSimValueLists, listas de tamanhos diferentes, lista1=\",lista1,\"/ lista2 = \",lista2)\n",
        "      raise ValueError(\"As listas devem ter o mesmo comprimento.\")\n",
        "\n",
        "  percSim = [ (1 / (1 + (abs(num1 - num2)))) * 100 for num1, num2 in zip(lista1, lista2)]\n",
        "\n",
        "\n",
        "  return sum(percSim) / len (percSim)\n",
        "\n",
        "#(para calcular o percentual de similaridade entre dois números\n",
        "def calcPercSimValueNums(num1, num2):\n",
        "\n",
        "  percSim = (1 / (1 + (abs(num1 - num2)))) * 100\n",
        "  #print (\" similaridade entre os numeros {0} e {1}: {2}\".format(num1, num2, percSim))\n",
        "  return percSim\n",
        "\n",
        "#coletar os arquivos de acordo com premissas (prefixo e sufixo)\n",
        "def getFilesByPrefix(path, prefix, sufix):\n",
        "  lstFiles = []\n",
        "  for fileName in os.listdir(path):\n",
        "      if prefix in fileName and fileName.endswith(sufix):\n",
        "          lstFiles.append(fileName)\n",
        "  return lstFiles\n",
        "\n",
        "#coletar no arquivo do GT dados das tabelas de uma determinada pagina\n",
        "def getListFilesGTInfo(curfile, page, path):\n",
        "\n",
        "  prefix = curfile + \"|\" + str(page)\n",
        "  sufix = \"_INFO.info\"\n",
        "  lstTables = getFilesByPrefix (path, prefix, sufix)\n",
        "\n",
        "  return lstTables\n",
        "\n",
        "def getGTInfo(tableID, curfile, page, path):\n",
        "\n",
        "  prefix = tableID + \"|\" + curfile + \"|\" + str(page)\n",
        "  sufix = \"_INFO.info\"\n",
        "  lstTables = getFilesByPrefix (path, prefix, sufix)\n",
        "\n",
        "  return lstTables\n",
        "\n",
        "#coletar as informacoes da tabela do arquivo _INFO e retornar para uma lista\n",
        "def getListTablesInfo(path, listFiles):\n",
        "\n",
        "  listTablesInfo = []\n",
        "  for fileName in listFiles:\n",
        "    with open(path + fileName, 'r') as file:\n",
        "      conteudo = file.read()\n",
        "      listTablesInfo.append(eval(conteudo))\n",
        "\n",
        "  return listTablesInfo\n",
        "\n",
        "# verificar se a estrutura dos dois dicionários INFO são similares\n",
        "def checkDimensionINFO(dicTableGT, dicTable):\n",
        "\n",
        "  msgErro = \"0 - SUCESSO\"\n",
        "  isIdentical = True\n",
        "\n",
        "  #se o tamanho das chaves dos dicionários são diferentes\n",
        "  if(dicTableGT.keys() != dicTable.keys()):\n",
        "    print(\"Dicionários não possuem o mesmo indice\")\n",
        "    msgErro = \"1 - Dicionários não possuem o mesmo indice\"\n",
        "    isIdentical = False\n",
        "\n",
        "  #se não tiver a mesma dimensao já descarta\n",
        "  if dicTableGT[\"DIMENSION\"] != dicTable[\"DIMENSION\"]:\n",
        "    msgErro = \"2 - Dicionários não possuem a mesma dimensão\"\n",
        "    isIdentical = False\n",
        "\n",
        "  #se o tamanho das colunas HEAD e FIRST_LINE não batem\n",
        "  if(len(dicTableGT[\"HEAD\"]) != len(dicTable[\"HEAD\"])):\n",
        "    print(\"Dicionários não possuem o mesmo tamanho da chave HEAD\")\n",
        "    msgErro = \"3 - Dicionários não possuem o mesmo tamanho da chave HEAD\"\n",
        "    isIdentical = False\n",
        "\n",
        "  if(len(dicTableGT[\"FIRST_LINE\"]) != len(dicTable[\"FIRST_LINE\"])):\n",
        "    msgErro = \"4 - Dicionários não possuem o mesmo tamanho da chave FIRST_LINE\"\n",
        "    isIdentical = False\n",
        "\n",
        "  #return msgErro, isSimilar\n",
        "  return isIdentical\n",
        "\n",
        "#calcular similaridade da string das células dos dicionários INFO de mesma dimensão\n",
        "#(para determinar se dois dicionarios INFO sao similares)\n",
        "def getPercSimTablesINFO(dicTableGT, dicTable , percTolerancia):\n",
        "\n",
        "  i = 0\n",
        "  for textTableGT in dicTableGT[\"HEAD\"]:\n",
        "    textTable = dicTable[\"HEAD\"][i]\n",
        "    if (calcPercSimStrings(textTableGT, textTable) < percTolerancia):\n",
        "      print(\"Tolerancia entre string \", textTableGT, \"e \", textTable,  \" menor que \", percTolerancia)\n",
        "      return False\n",
        "    i+=1\n",
        "\n",
        "  i = 0\n",
        "  for textTableGT in dicTableGT[\"FIRST_LINE\"]:\n",
        "    textTable = dicTable[\"FIRST_LINE\"][i]\n",
        "    if (calcPercSimStrings(textTableGT, textTable) < percTolerancia):\n",
        "      print(\"Tolerancia entre string \", textTableGT, \"e \", textTable,  \" menor que \", percTolerancia)\n",
        "      return False\n",
        "    i+=1\n",
        "\n",
        "  return True\n",
        "\n",
        "\n",
        "#verifica se a dimensão dos dicionários INFO possuem tamanhos parecidos (maximo 1 de diferença)\n",
        "def isAlmostSimilarINFO(dicTableGT, dicTable):\n",
        "\n",
        "  #devem possuir a mesma quantidade de linhas e quantidade semelhante de colunas (maximo no modulo 1)\n",
        "\n",
        "  #colunas\n",
        "  qtdColGT = len(dicTableGT[\"HEAD\"])\n",
        "  qtdColTab = len(dicTable[\"HEAD\"])\n",
        "\n",
        "  #linhas\n",
        "  qtdLinhasGT = dicTableGT[\"DIMENSION\"].split(\"X\")[0]\n",
        "  qtdLinhas = dicTable[\"DIMENSION\"].split(\"X\")[0]\n",
        "\n",
        "  if abs(qtdColGT-qtdColTab) <=1 and qtdLinhasGT == qtdLinhas:\n",
        "    return True\n",
        "\n",
        "  return False\n",
        "\n",
        "#verificar maior valor na lista\n",
        "def maiorValor(lista):\n",
        "\n",
        "  maiorValor = 0\n",
        "  for item in lista:\n",
        "    if item > maiorValor:\n",
        "        maiorValor = item\n",
        "\n",
        "  return maiorValor\n",
        "\n",
        "#verificar possivel similaridade no cabeçalho dos dicionários INFO\n",
        "def checkAVGSimilaritiesINFO(dicTableGT, dicTable, percTolerancia):\n",
        "\n",
        "  qtdColGT = len(dicTableGT[\"HEAD\"])\n",
        "  qtdColTab = len(dicTable[\"HEAD\"])\n",
        "\n",
        "  arrSim = []\n",
        "  arrSummary = []\n",
        "  limit = 4\n",
        "\n",
        "  #verificar similaridade nas 3 primeiras colunas\n",
        "  for i in range(qtdColTab):\n",
        "\n",
        "    for j in range(qtdColGT):\n",
        "      strTab = dicTable[\"HEAD\"][i]\n",
        "      strGT = dicTableGT[\"HEAD\"][j]\n",
        "      arrSim.append(calcPercSimStrings(strTab, strGT))\n",
        "\n",
        "    arrSummary.append(maiorValor(arrSim))\n",
        "    arrSim = []\n",
        "\n",
        "    if(i>= limit):\n",
        "      break\n",
        "\n",
        "  #print(arrSummary)\n",
        "  avgPerc = sum(arrSummary) / len(arrSummary)\n",
        "  return avgPerc >= percTolerancia\n",
        "\n",
        "\n",
        "#verificar possivel similaridade no cabeçalho dos dicionários FIRST_LINE\n",
        "def checkAVGSimilarities2INFO(dicTableGT, dicTable, percTolerancia):\n",
        "\n",
        "  qtdColGT = len(dicTableGT[\"FIRST_LINE\"])\n",
        "  qtdColTab = len(dicTable[\"FIRST_LINE\"])\n",
        "\n",
        "  arrSim = []\n",
        "  arrSummary = []\n",
        "  limit = 3\n",
        "\n",
        "  #verificar similaridade nas 3 primeiras colunas\n",
        "  for i in range(qtdColTab):\n",
        "\n",
        "    for j in range(qtdColGT):\n",
        "      strTab = dicTable[\"FIRST_LINE\"][i]\n",
        "      strGT = dicTableGT[\"FIRST_LINE\"][j]\n",
        "      arrSim.append(calcPercSimStrings(strTab, strGT))\n",
        "\n",
        "    arrSummary.append(maiorValor(arrSim))\n",
        "    arrSim = []\n",
        "\n",
        "    if(i>= limit):\n",
        "      break\n",
        "\n",
        "  #print(arrSummary)\n",
        "  avgPerc = sum(arrSummary) / len(arrSummary)\n",
        "  return avgPerc >= percTolerancia\n",
        "\n",
        "def getDicTableInfo(labName, curfile, page, qtdlinhas, qtdcolunas, bbox, head, firstLine):\n",
        "  dicTableInfo = {}\n",
        "\n",
        "  dicTableInfo[\"LAB\"] = labName\n",
        "  dicTableInfo[\"FILE\"] = curfile\n",
        "  dicTableInfo[\"PAGE\"] = page\n",
        "  dicTableInfo[\"TABLEID\"] = \"TBD\"\n",
        "  dicTableInfo[\"DIMENSION\"] = str(qtdlinhas) + \"X\" + str(qtdcolunas)\n",
        "  dicTableInfo[\"BBOX\"] = bbox\n",
        "  dicTableInfo[\"HEAD\"] = head\n",
        "  dicTableInfo[\"FIRST_LINE\"] = firstLine\n",
        "\n",
        "  return dicTableInfo\n",
        "\n",
        "def SaveDicTableInfo (filePath, dicTableInfo):\n",
        "\n",
        "  strFile = \"{\"\n",
        "  lenDic = len(dicTableInfo.items())\n",
        "  #print(lenDic)\n",
        "  i = 0\n",
        "  for chave, valor in dicTableInfo.items():\n",
        "    vir = \",\" if i < lenDic-1 else \"\"\n",
        "    if type(valor) == str:\n",
        "      strFile += \"'\" + str(chave)+ \"':'\" + str(valor) + \"'\" + vir + \"\\n\"\n",
        "    else:\n",
        "      strFile += \"'\" + str(chave) + \"':\" + str(valor) + vir + \"\\n\"\n",
        "    i = i + 1\n",
        "\n",
        "  strFile += \"}\"\n",
        "  with open(filePath, 'w') as arquivo:\n",
        "    arquivo.write(strFile)\n",
        "\n",
        "#numero de ocorrencias de um numero em uma lista\n",
        "def numTimes(list, num):\n",
        "\n",
        "  cont = 0\n",
        "  for valor in list:\n",
        "    if valor == num:\n",
        "      cont+=1\n",
        "\n",
        "  return cont\n",
        "\n",
        "def isDecimal(valor):\n",
        "  try:\n",
        "\n",
        "    if valor == \"NAN\" or valor == \"nan\":\n",
        "      return False\n",
        "    else:\n",
        "      float(valor)\n",
        "      return True\n",
        "  except ValueError:\n",
        "      return False\n",
        "\n",
        "def noteTokensHTML(df):\n",
        "\n",
        "  lsthead = []\n",
        "  lsttd = []\n",
        "  hasheader = False\n",
        "  hasdata = False\n",
        "  dict_tokens_html = {}\n",
        "  lsthead\n",
        "\n",
        "  #percorre o dataframe para construir a estrutura html de colunas\n",
        "  for i in range(len(df)):\n",
        "    primeiraColuna = True\n",
        "    j = 0\n",
        "    for column in df.columns:\n",
        "\n",
        "        value = \"NAN\"\n",
        "        if not df.empty and not pd.isna(df.at[i, column]):\n",
        "          # Value exists, access it\n",
        "          value = str(df.at[i, column])\n",
        "          value = value.replace(\"'\",\"\")\n",
        "          value = value.replace(\"\\\\\",\"\")\n",
        "\n",
        "        #primeira linha sao os cabeçalhos\n",
        "        if i == 0:\n",
        "          hasheader = True\n",
        "          lsthead.append(\"<td>\")\n",
        "          #lsthead.append(value) #apenas para teste, comentar depois\n",
        "          lsthead.append(\"</td>\")\n",
        "        else:\n",
        "          hasdata = True\n",
        "          if(primeiraColuna):\n",
        "            primeiraColuna = False\n",
        "            #a partir da 3a linha fecha a linha anterior </tr>\n",
        "            if(j==0 and i > 1):\n",
        "              lsttd.append(\"</tr>\")\n",
        "            lsttd.append(\"<tr>\")\n",
        "          lsttd.append(\"<td>\")\n",
        "          #lsttd.append(value) #apenas para teste, comentar depois\n",
        "          lsttd.append(\"</td>\")\n",
        "\n",
        "        primeiraColuna = False\n",
        "        j = j + 1\n",
        "\n",
        "  if(hasheader):\n",
        "    lsthead.insert(0,\"<thead>\")\n",
        "    lsthead.insert(1,\"<tr>\")\n",
        "    lsthead.append(\"</tr>\")\n",
        "    lsthead.append(\"</thead>\")\n",
        "\n",
        "  if(hasdata):\n",
        "    lsttd.insert(0,\"<tbody>\")\n",
        "    lsttd.append(\"</tbody>\")\n",
        "\n",
        "  #se a estrutura tiver completa, adiciona no dicionario tokens\n",
        "  if(hasheader and hasdata):\n",
        "    lsthead.extend(lsttd)\n",
        "  else:\n",
        "    dict_tokens_html = {\"tokens\": \"vazio\"}\n",
        "    lsthead.extend(dict_tokens_html)\n",
        "\n",
        "  lsthead.insert(0,\"<table>\")\n",
        "  lsthead.extend(\"</table>\")\n",
        "  return lsthead\n",
        "\n",
        "\n",
        "def noteListTokensBbox(cell_coordinates, lstData, lstTableRef):\n",
        "  list_tokens_bbox = []\n",
        "\n",
        "  #dimensao dos dados\n",
        "  qtdRowData = len(lstData)\n",
        "  qtdColData = len(lstData[0])\n",
        "  print(\"dimensao Data {0} x {1} \".format(qtdRowData, qtdColData))\n",
        "\n",
        "  #carregando BBOX de cada celula por linha para uma lista\n",
        "  qtdlinhasBbox = len(cell_coordinates)\n",
        "  qtdcolunasBbox = len(cell_coordinates[0][\"cells\"])\n",
        "  print(\"dimensao Bbox {0} x {1} \".format(qtdlinhasBbox, qtdcolunasBbox))\n",
        "\n",
        "\n",
        "  i = 0\n",
        "  for row in cell_coordinates:\n",
        "    j = 0\n",
        "    for bbox in row[\"cells\"]:\n",
        "      x1 = round(bbox[\"cell\"][0])\n",
        "      y1 = round(bbox[\"cell\"][1])\n",
        "      x2 = round(bbox[\"cell\"][2])\n",
        "      y2 = round(bbox[\"cell\"][3])\n",
        "\n",
        "      if(lstTableRef is None):\n",
        "        dict_tokens_bbox = {'tokens': list(lstData[i][j]), 'bbox': [x1, y1, x2, y2]}\n",
        "      else:\n",
        "        dict_tokens_bbox = {'tokens': list(lstData[i][j]), 'bbox': [x1 - lstTableRef[0], y1 - lstTableRef[1], x2 - lstTableRef[0], y2 - lstTableRef[1]]}\n",
        "      list_tokens_bbox.append(dict_tokens_bbox)\n",
        "      j+=1\n",
        "    i+=1\n",
        "\n",
        "  return list_tokens_bbox\n",
        "\n",
        "def noteListTokensBbox(lstBbox, df, lstTableRef):\n",
        "  list_tokens_bbox = []\n",
        "\n",
        "  #dimensao dos dados\n",
        "  qtdRowDf = 0 if df.empty else df.shape[0]\n",
        "  qtdColDf = 0 if df.empty else df.shape[1]\n",
        "  print(\"noteListTokensBbox - dimensao df {0} x {1} \".format(qtdRowDf, qtdColDf))\n",
        "\n",
        "  #carregando BBOX de cada celula por linha para uma lista\n",
        "  #qtdlinhasBbox = 0 if len(lstBbox) == 0 else len(lstBbox)\n",
        "  #qtdcolunasBbox = 0 if len(lstBbox) == 0 and len(lstBbox[0]) else len(lstBbox)\n",
        "\n",
        "  for i in range(qtdRowDf):\n",
        "    for j in range(qtdColDf):\n",
        "      #bbox = lstBbox[i][j]\n",
        "      #x1 = round(bbox[0])\n",
        "      #y1 = round(bbox[1])\n",
        "      #x2 = round(bbox[2])\n",
        "      #y2 = round(bbox[3])\n",
        "      x1 = np.nan\n",
        "      y1 = np.nan\n",
        "      x2 = np.nan\n",
        "      y2 = np.nan\n",
        "\n",
        "      value = \"NAN\"\n",
        "      if not df.empty and not pd.isna(df.at[i, j]):\n",
        "        # Value exists, access it\n",
        "        value = str(df.at[i, j])\n",
        "\n",
        "      if(lstTableRef is None):\n",
        "        dict_tokens_bbox = {'tokens': list(value), 'bbox': [x1, y1, x2, y2]}\n",
        "      else:\n",
        "        dict_tokens_bbox = {'tokens': list(value), 'bbox': [x1, y1, x2, y2]}\n",
        "        #dict_tokens_bbox = {'tokens': list(df.at[i,j]), 'bbox': [x1 - lstTableRef[0], y1 - lstTableRef[1], x2 - lstTableRef[0], y2 - lstTableRef[1]]}\n",
        "      list_tokens_bbox.append(dict_tokens_bbox)\n",
        "\n",
        "  return list_tokens_bbox\n",
        "\n",
        "def printMetaDados(dicMetaData):\n",
        "\n",
        "  strout = []\n",
        "  strout.append(\"{ \\n\")\n",
        "\n",
        "  if \"filename\" in dicMetaData:\n",
        "    strout.append(\"filename: '\" + str(dicMetaData[\"filename\"]) + \"',\\n\")\n",
        "  else:\n",
        "    strout[0] = \"chave filename não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if \"split\" in dicMetaData:\n",
        "    strout.append(\"split: '\" + str(dicMetaData[\"split\"]) + \"',\\n\")\n",
        "  else:\n",
        "    strout[0] = \"chave split não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if \"imgid\" in dicMetaData:\n",
        "    strout.append(\"'imgid': \" + str(dicMetaData[\"imgid\"]) + \",\\n\")\n",
        "  else:\n",
        "    strout[0] = \"chave imgid não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if \"html\" in dicMetaData:\n",
        "\n",
        "    strout.append(\"--INICIO HTML \\n\")\n",
        "    strout.append(\"'html': \\n {\")\n",
        "\n",
        "    if \"cells\" in dicMetaData[\"html\"] and \"structure\" in dicMetaData[\"html\"]:\n",
        "\n",
        "      if isinstance(dicMetaData[\"html\"][\"cells\"], list ) and isinstance(dicMetaData[\"html\"][\"structure\"], list ):\n",
        "\n",
        "        #varrendo o conteudo da lista dicMetaData[\"html\"][\"cells\"]\n",
        "        #que contem as duas sublistas tokens e bbox\n",
        "        strout.append(\"--INICIO CELLS \\n\")\n",
        "        strout.append(\"'cells': [\\n\")\n",
        "        i = 0\n",
        "        for arrcells in dicMetaData[\"html\"][\"cells\"]:\n",
        "\n",
        "           #print da estrutura dos dicionarios tokens e bbox\n",
        "           tokens =  arrcells[\"tokens\"]\n",
        "           bbox =  arrcells[\"bbox\"]\n",
        "           comma = \",\"\n",
        "           if i == len(dicMetaData[\"html\"][\"cells\"]) -1:\n",
        "            comma = \"\"\n",
        "           else:\n",
        "            comma = \",\"\n",
        "\n",
        "           strout.append(\"      {'tokens': \" + str(tokens) + \", 'bbox': \" + str(bbox) + \"}\" + comma + \" \\n\")\n",
        "           i = i + 1\n",
        "\n",
        "        strout.append(\"] --FIM CELLS\\n\")\n",
        "\n",
        "        #print da estrutura do dicionario structure\n",
        "        if(dicMetaData[\"html\"] is not None and dicMetaData[\"html\"][\"structure\"] is not None):\n",
        "          #strout.append(\"      'structure': [\" + str(\"','\".join(dicMetaData[\"html\"][\"structure\"])) + \"' \\n\")\n",
        "          strout.append(\"      'structure': ['\" + str(\"','\".join([x for x in dicMetaData[\"html\"][\"structure\"] if x is not None])) + \"' \\n\")\n",
        "\n",
        "        else:\n",
        "          strout.append(\"      'structure': ['None'] \\n\")\n",
        "\n",
        "\n",
        "        strout.append(\"] --FIM STRUCTURE \\n\")\n",
        "\n",
        "      else:\n",
        "        strout[0] = \"chave html/cells ou structure não existe na estrutura\"\n",
        "        return strout\n",
        "\n",
        "    strout.append(\"} --FIM HTML\\n\")\n",
        "  else:\n",
        "    strout[0] = \"chave html não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  strout.append(\"} \\n\")\n",
        "  return strout\n",
        "\n",
        "def saveAnnotationFile(dicMetaData, dirout , numPage):\n",
        "\n",
        "  vecArq = dicMetaData[\"filename\"].split(\"/\")\n",
        "\n",
        "  nomeArq = \"\"\n",
        "  if(len(vecArq)>0):\n",
        "\n",
        "    tableID = dicMetaData[\"imgid\"].replace(\"'\",\"\")\n",
        "    filename = vecArq[len(vecArq)-1].split(\".\")[0]\n",
        "    labname = vecArq[len(vecArq)-2].split(\".\")[0]\n",
        "\n",
        "    nomeArq = tableID + \"|\" + filename + \"|\" + str(numPage) + \"_METADADOS.mtd\"\n",
        "    print(\"gravando arquivo \", nomeArq)\n",
        "\n",
        "    strout = printMetaDados(dicMetaData)\n",
        "\n",
        "    labDirOut = dirout + \"/\" + labname + \"/\"\n",
        "    if not os.path.exists(labDirOut):\n",
        "      os.makedirs(labDirOut)\n",
        "\n",
        "    with open(labDirOut + nomeArq, 'w') as arquivo:\n",
        "      for linha in strout:\n",
        "            arquivo.write(linha)\n",
        "\n",
        "def getPos(lst, key):\n",
        "\n",
        "  for k, item in enumerate(lst):\n",
        "    if item == key:\n",
        "      return k\n",
        "\n",
        "  return -1\n",
        "\n",
        "#FUNCAO DE VERIFICA SE EXISTE O BUG DE TRUNCAR O VALOR ∞\n",
        "def isBUGInfinito(dicTable, dicTableGT):\n",
        "\n",
        "  char = \"\"\n",
        "  posInf = getPos(dicTableGT[\"FIRST_LINE\"], \"∞\")\n",
        "  posInfV = getPos(dicTableGT[\"FIRST_LINE\"], \"V\")\n",
        "\n",
        "  #possui valor ∞ na tabela? segue análise\n",
        "  if posInf >-1:\n",
        "    char = \"∞\"\n",
        "    print(\"Possui valor ∞ na tabela, posInf\", posInf)\n",
        "    #2 - possuem o mesmo valor de dimensao em DIMENSION\n",
        "    if dicTable[\"DIMENSION\"] == dicTableGT[\"DIMENSION\"]:\n",
        "      print(\"Valor DIMENSION iguais\")\n",
        "      #4 primeiras colunas das duas tabelas possuem o mesmo valor?\n",
        "      print(\"checkAVGSimilarities2INFO >=60 perc? \",checkAVGSimilarities2INFO(dicTableGT, dicTable, 80))\n",
        "      if checkAVGSimilarities2INFO(dicTableGT, dicTable, 60):\n",
        "        print(\"Quatro primeiras colunas similares\")\n",
        "        #4 - chave HEAD tem o tamanho um a menos que GT\n",
        "        if len(dicTable[\"HEAD\"]) == len(dicTableGT[\"HEAD\"])-1 and len(dicTable[\"FIRST_LINE\"]) == len(dicTableGT[\"FIRST_LINE\"])-1:\n",
        "          #dicTable[\"FIRST_LINE\"].insert(posInf, \"∞\")\n",
        "          return True, posInf, \"∞\"\n",
        "\n",
        "  #possui valor ∞ na tabela? segue análise\n",
        "  if posInfV >-1:\n",
        "    char = \"V\"\n",
        "    print(\"Possui valor V na tabela, posInf\", posInf)\n",
        "    #2 - possuem o mesmo valor de dimensao em DIMENSION\n",
        "    if dicTable[\"DIMENSION\"] == dicTableGT[\"DIMENSION\"]:\n",
        "      print(\"Valor DIMENSION iguais\")\n",
        "      #4 primeiras colunas das duas tabelas possuem o mesmo valor?\n",
        "      print(\"checkAVGSimilarities2INFO >=60 perc? \",checkAVGSimilarities2INFO(dicTableGT, dicTable, 80))\n",
        "      if checkAVGSimilarities2INFO(dicTableGT, dicTable, 60):\n",
        "        print(\"Quatro primeiras colunas similares\")\n",
        "        #4 - chave HEAD tem o tamanho um a menos que GT\n",
        "        if len(dicTable[\"HEAD\"]) == len(dicTableGT[\"HEAD\"])-1 and len(dicTable[\"FIRST_LINE\"]) == len(dicTableGT[\"FIRST_LINE\"])-1:\n",
        "          #dicTable[\"FIRST_LINE\"].insert(posInf, \"∞\")\n",
        "          return True, posInfV, \"V\"\n",
        "\n",
        "  return False, -1, \"\"\n",
        "\n",
        "import glob\n",
        "\n",
        "def deleteFiles(dir, ext):\n",
        "  # Obter todos os arquivos com a extensão especificada\n",
        "  files = glob.glob(os.path.join(dir, f'*.{ext}'))\n",
        "\n",
        "  # Remover cada arquivo encontrado\n",
        "  for file in files:\n",
        "      try:\n",
        "          os.remove(file)\n",
        "          print(f\"Arquivo {file} removido com sucesso.\")\n",
        "      except OSError as e:\n",
        "          print(f\"Erro ao remover o arquivo {file}: {e}\")\n",
        "\n",
        "def printHTML2(lst, tipo): #com TAB\n",
        "  # tipo: RAW (cru) ou PRETTY (html com identações)\n",
        "\n",
        "  strout = \"\"\n",
        "  #strres = ''.join(lst)\n",
        "  strres = ''.join([str(x) for x in lst])\n",
        "  if tipo == \"PRETTY\":\n",
        "    soup = bs(strres, 'html.parser')\n",
        "    strout = soup.prettify()\n",
        "    # Substituir espaços por TAB\n",
        "    strout = strout.replace(\"  \", \"\\t\")\n",
        "  else:\n",
        "    strout = strres\n",
        "\n",
        "  return strout\n",
        "\n",
        "def printElementMetaData(dicMetaData, elem):\n",
        "\n",
        "  strout = []\n",
        "  strout.append(\"[\")\n",
        "\n",
        "  if not \"filename\" in dicMetaData:\n",
        "    strout[0] = \"chave filename não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if not \"split\" in dicMetaData:\n",
        "    strout[0] = \"chave split não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if not \"imgid\" in dicMetaData:\n",
        "    strout[0] = \"chave imgid não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  if \"html\" in dicMetaData:\n",
        "    if \"cells\" in dicMetaData[\"html\"] and \"structure\" in dicMetaData[\"html\"]:\n",
        "      if isinstance(dicMetaData[\"html\"][\"cells\"], list ) and isinstance(dicMetaData[\"html\"][\"structure\"], list ):\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        if(elem == \"BBOX\"):\n",
        "          #strout.append(\"{\")\n",
        "          for arrcells in dicMetaData[\"html\"][\"cells\"]:\n",
        "\n",
        "            tokens =  arrcells[\"tokens\"]\n",
        "            bbox =  arrcells[\"bbox\"]\n",
        "            comma = \",\"\n",
        "            if i == len(dicMetaData[\"html\"][\"cells\"]) -1:\n",
        "              comma = \"\"\n",
        "            else:\n",
        "              comma = \",\"\n",
        "\n",
        "            strout.append(\"{'tokens': \" + str(tokens) + \", 'bbox': \" + str(bbox) + \"}\" + comma + \" \\n\")\n",
        "            i = i + 1\n",
        "\n",
        "        elif(elem == \"HTML_PRETTY\"):\n",
        "          strout.append(printHTML2(dicMetaData[\"html\"][\"structure\"], \"PRETTY\"))\n",
        "\n",
        "        else:\n",
        "          html = \"'\"\n",
        "          html = html + \"','\".join([element for element in dicMetaData[\"html\"][\"structure\"] if element]) + \"'\"\n",
        "          strout.append(html)\n",
        "\n",
        "      else:\n",
        "        strout[0] = \"chave html/cells ou structure não existe na estrutura\"\n",
        "        return strout\n",
        "\n",
        "  else:\n",
        "    strout[0] = \"chave html não existe na estrutura\"\n",
        "    return strout\n",
        "\n",
        "  strout.append(\"]\")\n",
        "  return strout\n",
        "\n",
        "def saveElementMetadata(dicMetaData, elem, dirout, numPage):\n",
        "\n",
        "  vecArq = dicMetaData[\"filename\"].split(\"/\")\n",
        "  #print(vecArq)\n",
        "  nomeArq = \"\"\n",
        "  if(len(vecArq)>0):\n",
        "\n",
        "    tableID = str(dicMetaData[\"imgid\"]).replace(\"'\",\"\")\n",
        "    filename = vecArq[len(vecArq)-1].split(\".\")[0]\n",
        "    labname = vecArq[len(vecArq)-2].split(\".\")[0]\n",
        "\n",
        "    nomeArq = tableID + \"|\" + filename + \"|\" + str(numPage) + \"_\" + elem + \".\" + elem.lower()\n",
        "    strout = printElementMetaData(dicMetaData, elem)\n",
        "\n",
        "    labDirOut = dirout + \"/\" + labname + \"/\"\n",
        "\n",
        "    print(\"Arquivo de anotação \", elem, \" gerado = \",  nomeArq)\n",
        "    with open(labDirOut + nomeArq, 'w') as arquivo:\n",
        "      for linha in strout:\n",
        "          arquivo.write(linha)\n",
        "\n",
        "#retorna o tableID de maior similiaridade entre os dicionarios GT de comparacao\n",
        "def getMaiorSimilaridade(dic, listasGT):\n",
        "\n",
        "  lstDicRes = []\n",
        "\n",
        "  for dicGT in listasGT:\n",
        "\n",
        "    lstRes = []\n",
        "\n",
        "    #ajusta caso necessário a dimensao entre as tabelas\n",
        "    #print(\"a tratar...\", dic[\"FIRST_LINE\"])\n",
        "    #print(type(dic[\"FIRST_LINE\"]))\n",
        "    dic[\"FIRST_LINE\"] = ajustColList(dic[\"FIRST_LINE\"], len(dicGT[\"FIRST_LINE\"]))\n",
        "    lenDic = len(dic[\"FIRST_LINE\"])\n",
        "\n",
        "    print(\"getMaiorSimilaridade, FIRST_LINE GT\", dicGT[\"FIRST_LINE\"])\n",
        "    print(\"getMaiorSimilaridade, FIRST_LINE ANALISE\", dic[\"FIRST_LINE\"])\n",
        "    for i in range(len(dicGT[\"FIRST_LINE\"])):\n",
        "\n",
        "      lenDicGT = len(dicGT[\"FIRST_LINE\"])\n",
        "\n",
        "      if lenDic != lenDicGT: #tamanhos diferentes, retornar vazio\n",
        "        return \"\"\n",
        "\n",
        "      str1 = str(dic[\"FIRST_LINE\"][i])\n",
        "      str2 = str(dicGT[\"FIRST_LINE\"][i])\n",
        "      strDec1 = str1.replace(\",\", \".\").strip()\n",
        "      strDec2 = str2.replace(\",\", \".\").strip()\n",
        "\n",
        "      #se os valores forem numeros converter para float para calcular similiaridade com maior exatidao\n",
        "      if (isDecimal(strDec1) and isDecimal(strDec2)):\n",
        "        #print(\" Similaridade entre dois numeros \", strDec1, \" e \", strDec2, \" = \", round(calcPercSimValueNums(float(strDec1), float(strDec2)), 2))\n",
        "        lstRes.append(round(calcPercSimValueNums(float(strDec1), float(strDec2)), 2))\n",
        "      #no caso de string\n",
        "      else:\n",
        "        #print(\" Similaridade entre duas strings \", strDec1, \" e \", strDec2, \" = \", round(calcPercSimStrings(str1, str2),4))\n",
        "        lstRes.append(round(calcPercSimStrings(str1, str2),2))\n",
        "\n",
        "    lstDicRes.append({\"TABLEID\": dicGT[\"TABLEID\"], \"RESULT\":lstRes})\n",
        "\n",
        "  #verificando maior media\n",
        "  tableId = \"\"\n",
        "  maiorMedia = 0\n",
        "  for dicRes in lstDicRes:\n",
        "    avg = sum(dicRes[\"RESULT\"]) / len(dicRes[\"RESULT\"])\n",
        "    print(\"Media \", avg)\n",
        "    if avg > maiorMedia:\n",
        "      tableId = dicRes[\"TABLEID\"]\n",
        "      maiorMedia = avg\n",
        "\n",
        "  return tableId\n",
        "\n",
        "def ajustColList(lst1, qtdColsRef):\n",
        "\n",
        "  # Calcula o número de colunas de cada lista\n",
        "  #num_cols_lstRef = len(lstRef)\n",
        "  num_cols_lst1 = len(lst1) if lst1 else 0\n",
        "\n",
        "  # Se lst1 tiver menos colunas que lstRef, preenche com NaN\n",
        "  if num_cols_lst1 < qtdColsRef:\n",
        "    # Calcula o número de colunas a serem adicionadas\n",
        "    num_cols_adicionais = qtdColsRef - num_cols_lst1\n",
        "    # Preenche lst2 com NaN nas novas colunas\n",
        "\n",
        "    #print(\"num_cols_adicionais \", num_cols_adicionais)\n",
        "    for i in range(num_cols_adicionais):\n",
        "      lst1.append(\"NAN\")\n",
        "\n",
        "  #se lst1 tiver mais coluna que lstRef, remove as colunas adicionais de lst1\n",
        "  elif num_cols_lst1 > qtdColsRef:\n",
        "    # Calcula o número de colunas a serem removidas\n",
        "    num_cols_adicionais = num_cols_lst1 - qtdColsRef\n",
        "    for i in range(num_cols_adicionais):\n",
        "      if (len(lst1)>0):\n",
        "        del(lst1[len(lst1)-1])\n",
        "\n",
        "  return lst1\n",
        "\n",
        "#funcao para ajustar uma lista de acordo com a quantidade de linhas e colunas de referencia\n",
        "# se tiver a mais linhas ou colunas, adiciona, se tiver menos, remove\n",
        "def ajustList(lst1, qtdRowsRef, qtdColsRef):\n",
        "\n",
        "  qtdRows = len(lst1)\n",
        "\n",
        "  #lsteste = eval(strdata)\n",
        "  lst1_ajust = []\n",
        "\n",
        "  #1 - ajustando as colunas\n",
        "  for lstRow in lst1:\n",
        "    lst1_ajust.append(ajustColList(lstRow, qtdColsRef))\n",
        "\n",
        "  #1 - ajustando as linhas\n",
        "  #se precisar adicionar linhas\n",
        "  if(qtdRowsRef > qtdRows):\n",
        "    qtdLinhasAdicionais = qtdRowsRef - qtdRows\n",
        "    for i in range(qtdLinhasAdicionais):\n",
        "      if len(lst1_ajust) >0 and len(lst1_ajust[0]) >0:\n",
        "        lst1_ajust.insert(len(lst1_ajust), [\"NAN\" for _ in range(len(lst1_ajust[0]))])\n",
        "  #se precisar remover linhas adicionais\n",
        "  elif(qtdRows > qtdRowsRef):\n",
        "    qtdLinhasAdicionais = qtdRows - qtdRowsRef\n",
        "    for i in range(qtdLinhasAdicionais):\n",
        "      del(lst1_ajust[len(lst1_ajust)-1])\n",
        "\n",
        "  return lst1_ajust\n",
        "\n",
        "#funcao para ajustar a estrutura cell_coordinates em relacao a referencia para possibilitar a comparacao e geracao de estatisticas\n",
        "def ajustCellCord (cellCord, qtdRowsRef, qtdColsRef):\n",
        "\n",
        "  lstCoord = [999999, 999999, 999999, 999999] #nova lista de coordenadas\n",
        "  dicNewCol = {'column': lstCoord, 'cell': lstCoord} #uma nova coluna (celula)\n",
        "  #nova linha da tabela\n",
        "  newLine =  \"{'row': [99999, 99999, 99999, 99999], \\\n",
        "              'cells': [], \\\n",
        "              'cell_count': 0}\"\n",
        "  dicNewLine = eval(newLine)\n",
        "\n",
        "  #verificando dimensao da estrutura atual\n",
        "  qtdRows = 0\n",
        "  qtdCols = 0\n",
        "  if cellCord is not None and len(cellCord) >0:\n",
        "    qtdRows = len(cellCord)\n",
        "    qtdCols = len(cellCord[0][\"cells\"])\n",
        "  #qtdRows = len(cellCord)\n",
        "  #qtdCols = len(cellCord[0][\"cells\"])\n",
        "\n",
        "  #adicionando estrutura inicial para cada quantidade de colunas de referencia\n",
        "  for i in range(qtdColsRef):\n",
        "    dicNewLine[\"cells\"].insert(i,dicNewCol)\n",
        "\n",
        "  #adiciona para cada coluna adicional necessária\n",
        "  if qtdCols < qtdColsRef:\n",
        "    qtdColAdicionais = qtdColsRef - qtdCols\n",
        "\n",
        "    print(\"Adicionando coluna, qtd = \", qtdColAdicionais)\n",
        "    for i in range(qtdColAdicionais):\n",
        "      for row in cellCord:\n",
        "        row['cells'].append(dicNewCol)\n",
        "\n",
        "  #removendo uma coluna para cada adicional\n",
        "  elif qtdCols > qtdColsRef:\n",
        "    qtdColAdicionais = qtdCols - qtdColsRef\n",
        "\n",
        "    print(\"Removendo coluna, qtd = \", qtdColAdicionais)\n",
        "    for i in range(qtdColAdicionais):\n",
        "      for row in cellCord:\n",
        "        row['cells'] = row['cells'][:-1]\n",
        "\n",
        "  #adiciona linha para cada linha adicional necessária\n",
        "  if qtdRows < qtdRowsRef:\n",
        "    qtdRowAdicionais = qtdRowsRef - qtdRows\n",
        "\n",
        "    print(\"Adicionando linha, qtd = \", qtdRowAdicionais)\n",
        "    for i in range(qtdRowAdicionais):\n",
        "      cellCord.append(dicNewLine)\n",
        "\n",
        "  #removendo linha para cada linha adicional necessária\n",
        "  elif qtdRows > qtdRowsRef:\n",
        "    qtdRowAdicionais = qtdRows - qtdRowsRef\n",
        "\n",
        "    print(\"Removendo linha, qtd = \", qtdRowAdicionais)\n",
        "    #removendo linha para cada adicional\n",
        "    for i in range(qtdRowAdicionais):\n",
        "      del(cellCord[len(cellCord)-1])\n",
        "\n",
        "  return cellCord\n",
        "\n",
        "def temRepeticoes(lista):\n",
        "    return len(lista) != len(set(lista))\n",
        "\n",
        "#ajustar df2 para incrementar mais linhas ou colunas em relacao a referencia (df1)\n",
        "def ajustDataframe(df1, df2):\n",
        "\n",
        "  # Verifica se o número de colunas de df2 é menor que o de df1\n",
        "  if df2.shape[1] < df1.shape[1]:\n",
        "      # Calcula quantas colunas precisam ser adicionadas\n",
        "      num_cols_adicionais = df1.shape[1] - df2.shape[1]\n",
        "      # Adiciona as colunas extras em df2 preenchidas com NaN\n",
        "      for i in range(num_cols_adicionais):\n",
        "          df2[f'C{i+1}'] = np.nan\n",
        "  #neste caso remove as colunas adicionais\n",
        "  elif df2.shape[1] > df1.shape[1]:\n",
        "    # Calcula quantas colunas precisam ser removidas\n",
        "    num_cols_remover = df2.shape[1] - df1.shape[1]\n",
        "    # Remove as colunas extras em df2 da direita para a esquerda\n",
        "    df2 = df2.iloc[:, :-num_cols_remover]\n",
        "\n",
        "  # Verifica se o número de linhas de df2 é menor que o de df1\n",
        "  if df2.shape[0] < df1.shape[0]:\n",
        "      # Calcula quantas linhas precisam ser adicionadas\n",
        "      num_linhas_adicionais = df1.shape[0] - df2.shape[0]\n",
        "      # Adiciona as linhas extras em df2 preenchidas com NaN\n",
        "      linhas_extras = pd.DataFrame(index=[f'L{i+1}' for i in range(num_linhas_adicionais)],\n",
        "                                    columns=df2.columns)\n",
        "      df2 = pd.concat([df2, linhas_extras])\n",
        "  #neste caso remove as linhas adicionais\n",
        "  elif df2.shape[0] > df1.shape[0]:\n",
        "    # Calcula quantas linhas precisam ser removidas\n",
        "    num_linhas_remover = df2.shape[0] - df1.shape[0]\n",
        "    # Remove as linhas extras em df2 de baixo para cima\n",
        "    df2 = df2.iloc[:-num_linhas_remover, :]\n",
        "\n",
        "  return df2\n",
        "\n",
        "import copy\n",
        "\n",
        "#ajustar df2 para incrementar mais linhas ou colunas em relacao a referencia (df1)\n",
        "def ajustDataframe(df1, qtdLinhasRef, qtdColunasRef):\n",
        "\n",
        "  dfAux = copy.deepcopy(df1)\n",
        "  # Verifica se o número de colunas de df1 é menor que qtdColunasRef\n",
        "  if df1.shape[1] < qtdColunasRef:\n",
        "      # Calcula quantas colunas precisam ser adicionadas\n",
        "      num_cols_adicionais = qtdColunasRef - df1.shape[1]\n",
        "      # Adiciona as colunas extras em df1 preenchidas com NaN\n",
        "      #print(\"adicionando qtd colunas\", num_cols_adicionais)\n",
        "      ultIndice = len(dfAux.columns) - 1\n",
        "      for i in range(num_cols_adicionais):\n",
        "          if (i==0):\n",
        "            novoIndice = ultIndice + 1\n",
        "          else:\n",
        "            novoIndice = ultIndice + (i+1)\n",
        "          #dfAux[f'C{i+1}'] = np.nan novoIndice\n",
        "          dfAux[novoIndice] = np.nan\n",
        "  #neste caso remove as colunas adicionais\n",
        "  elif qtdColunasRef < df1.shape[1]:\n",
        "    # Calcula quantas colunas precisam ser removidas\n",
        "    num_cols_remover = df1.shape[1] - qtdColunasRef\n",
        "    # Remove as colunas extras em df1 da direita para a esquerda\n",
        "    dfAux = dfAux.iloc[:, :-num_cols_remover]\n",
        "\n",
        "  # Verifica se o número de linhas de df1 é menor que qtdLinhasRef\n",
        "  if df1.shape[0] < qtdLinhasRef:\n",
        "      # Calcula quantas linhas precisam ser adicionadas\n",
        "      num_linhas_adicionais = qtdLinhasRef - df1.shape[0]\n",
        "      # Adiciona as linhas extras em df1 preenchidas com NaN\n",
        "      #linhas_extras = pd.DataFrame(index=[f'L{i+1}' for i in range(num_linhas_adicionais)],\n",
        "                                    #columns=df1.columns)\n",
        "      #print(\"adicionando qtd linhas\", num_linhas_adicionais)\n",
        "      ultIndice = len(dfAux.columns) - 1\n",
        "      linhas_extras = pd.DataFrame(index=[f'{ultIndice + i + 1}' for i in range(num_linhas_adicionais)],\n",
        "                             columns=dfAux.columns)\n",
        "      dfAux = pd.concat([dfAux, linhas_extras])\n",
        "  #neste caso remove as linhas adicionais\n",
        "  elif qtdLinhasRef < df1.shape[0]:\n",
        "    # Calcula quantas linhas precisam ser removidas\n",
        "    num_linhas_remover = df1.shape[0] - qtdLinhasRef\n",
        "    # Remove as linhas extras em df1 de baixo para cima\n",
        "    dfAux = dfAux.iloc[:-num_linhas_remover, :]\n",
        "\n",
        "  #normalizando indices\n",
        "  dfAux = dfAux.reset_index(drop=True)\n",
        "\n",
        "  return dfAux\n",
        "\n",
        "#funcao para converter PDF para png\n",
        "def pdf_page_to_png(pdf_path, page_number, output_path):\n",
        "  # Convertendo a página do PDF para uma lista de imagens\n",
        "  images = convert_from_path(pdf_path, first_page=page_number, last_page=page_number)\n",
        "\n",
        "  # Salvando a imagem como PNG\n",
        "  images[0].save(output_path, 'PNG')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj14GnSdC6T_"
      },
      "source": [
        "# Código MAIN para Rodar em Lote\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F5ALhlJTFUZ"
      },
      "outputs": [],
      "source": [
        "################################### FUNCAO PRINCIPAL - GERAR ARQUIVOS DE METADADOS #######################\n",
        "dfArq = listFiles(CERT_PATH, None) # carregar a lista de arquivos no DATAFRAME\n",
        "\n",
        "lstDF = None\n",
        "df = None\n",
        "# Definir o fuso horário de SP\n",
        "fuso_horario_brasilia = pytz.timezone('America/Sao_Paulo')\n",
        "\n",
        "pathLab = DIROUT_TABLENET + \"/\" + LAB_PATH + \"/\"\n",
        "\n",
        "if(os.path.exists(pathLab)):\n",
        "  print('Removendo arquivos gerados anteriormente.., caminho:', pathLab)\n",
        "  deleteFiles2(pathLab) #deletando os arquivos anteriores\n",
        "\n",
        "for filepath, pages in zip(dfArq[\"PATH\"], dfArq[\"QTDPAGES\"]):\n",
        "\n",
        "  arrcurfile = filepath.split(\"/\")\n",
        "  curFile = arrcurfile[len(arrcurfile)-1]\n",
        "  labName = arrcurfile[len(arrcurfile)-2]\n",
        "\n",
        "  #inicializando variaveis GT\n",
        "  listFilesGT = []\n",
        "  listTablesInfoGT = []\n",
        "\n",
        "  #coletando dados das tabelas GT para comparacao e gerar o ID da imagem correto\n",
        "  GT_LAB_OUT = GT_PATH + labName + \"/\"\n",
        "  print(\"GT_LAB_OUT\", GT_LAB_OUT)\n",
        "\n",
        "  #varrendo cada pagina do arquivo\n",
        "  for i in range(int(pages)):\n",
        "\n",
        "    page = i+1\n",
        "    #verificando a quantidade de tabelas por pagina\n",
        "\n",
        "    #verificando se a pasta do laboratorio existe, caso negativo, cria\n",
        "    labDirOut = DIROUT_TABLENET + \"/\" + labName + \"/\"\n",
        "    if not os.path.exists(labDirOut):\n",
        "      os.makedirs(labDirOut)\n",
        "\n",
        "    #definindo variaveis para gravacao do arquivo de saida\n",
        "    noExtension = curFile.replace(\".pdf\",\"\")\n",
        "    #arquivo de PDF de leitura\n",
        "    path_pdf_in = filepath\n",
        "    #caminho para arquivo BMP convertido\n",
        "    path_bmp_noextension =  labDirOut + \"/\" + noExtension\n",
        "\n",
        "    #nova pagina, carrega a lista de referencia (GT) da pagina em questao\n",
        "    listFilesGT = getListFilesGTInfo(noExtension, page, GT_LAB_OUT)\n",
        "    listTablesInfoGT = getListTablesInfo(GT_LAB_OUT,listFilesGT)\n",
        "    qtdTabelasGT = len(listTablesInfoGT)\n",
        "\n",
        "    print(\"Arquivo: [\", path_pdf_in , \"] / qtd de tabelas para ler da pagina[\" + str(page) + \"]: \", len(listTablesInfoGT))\n",
        "\n",
        "    #temos tabelas para processar....\n",
        "    if(qtdTabelasGT >0):\n",
        "\n",
        "      ####1 - converter pdf para png  ####\n",
        "      #convert_pdf_to_bmp(path_pdf_in, path_bmp_noextension, page)\n",
        "\n",
        "      output_png = path_bmp_noextension + \"_\" + str(page) + \".png\"\n",
        "\n",
        "      pdf_page_to_png(path_pdf_in, page, output_png)\n",
        "\n",
        "      #carregar a imagem do modelo para o dataframe\n",
        "      lstDF = predict(output_png, TABLENET_MODEL_PATH)\n",
        "      qtdlinhasLstDF = 0\n",
        "      qtdColunasLstDF = 0\n",
        "      if len(lstDF) > 0:\n",
        "        qtdlinhasLstDF = len(lstDF)\n",
        "        qtdColunasLstDF = len(lstDF[0].columns)\n",
        "\n",
        "      qtdlinhasGT = 0\n",
        "      qtdColunasGT = 0\n",
        "      if len(listTablesInfoGT) > 0:\n",
        "        qtdlinhasGT = len(listTablesInfoGT)\n",
        "        qtdColunasGT = len(listTablesInfoGT[0])\n",
        "\n",
        "      print(\"=============>Tabela pagina {0} caminho: {1} \".format(page,path_pdf_in))\n",
        "      print(\"Dimensão lstDF {0} x {1} \".format(qtdlinhasLstDF, qtdColunasLstDF))\n",
        "      print(\"Dimensão listTablesInfoGT {0} x {1} \".format(qtdlinhasGT, qtdColunasGT))\n",
        "\n",
        "      print(\"len(lstDF) e len(listTablesInfoGT) com tamanho > 1 - CASO COMPLEXO \")\n",
        "      #para cada tabela (dataframe) da lista de dataframes\n",
        "      for df in lstDF:\n",
        "\n",
        "        qtdlinhasDF = 0\n",
        "        qtdColunasDF = 0\n",
        "        if len(lstDF) > 0:\n",
        "          qtdlinhasDF = len(df)\n",
        "          qtdColunasDF = len(df.columns)\n",
        "\n",
        "        dfOriginal = copy.deepcopy(df) #guarda o DF original sem correcoes para gravar no HTML (calculo do TED)\n",
        "        #verificando possivel ajuste de dimensao do dataframe com GT para comparação\n",
        "        if listTablesInfoGT is not None and len(listTablesInfoGT) > 0:\n",
        "\n",
        "          #ajustar o tamanho do dataframe, caso necessário\n",
        "          qtdLinhasGT = int(listTablesInfoGT[0][\"DIMENSION\"].split(\"X\")[0])\n",
        "          qtdColsGT = int(listTablesInfoGT[0][\"DIMENSION\"].split(\"X\")[1])\n",
        "          if (qtdColunasDF != qtdColsGT or qtdlinhasDF != qtdLinhasGT):\n",
        "            print(\"Ajustando dataframe para dimensão, dimensão atual DF = \", qtdlinhasDF,\"x\",qtdColunasDF)\n",
        "            print(\"Ajustando dataframe para dimensão do GT, nova dimensão = \", qtdLinhasGT,\"x\",qtdColsGT)\n",
        "            df = ajustDataframe(df, qtdLinhasGT, qtdColsGT)\n",
        "\n",
        "        #carregando o objeto dicionário INFO da tabela de análise\n",
        "        bbox = [999999, 999999, 999999, 999999] #modelo nao traz informacao de coordenadas (ajustar)\n",
        "        lstHead = [] if len(df) == 0 else df.iloc[0].tolist()\n",
        "        lstFirst = [] if len(df) == 0 or len(df) == 1 else df.iloc[1].tolist()\n",
        "        dicTable = getDicTableInfo(labName, noExtension, page, qtdlinhasDF, qtdColunasDF, bbox, lstHead, lstFirst)\n",
        "\n",
        "        print(\"dicTable HEAD\", dicTable[\"HEAD\"])\n",
        "        print(\"dicTable FIRST_LINE\", dicTable[\"FIRST_LINE\"])\n",
        "\n",
        "        #verificando qual tabela de maior similaridade\n",
        "        #apenas uma tabela GT existente na pagina (df recebe o TABLEID existente)\n",
        "        if len(listTablesInfoGT) ==1:\n",
        "          print(\"Apenas uma tabela GT na página, logo, TABLEID a ser utilizado para dicTable = \",listTablesInfoGT[0][\"TABLEID\"])\n",
        "          dicTable[\"TABLEID\"] = listTablesInfoGT[0][\"TABLEID\"]\n",
        "        else:\n",
        "          print(\"listTablesInfoGT > 1, verificar similaridade...\")\n",
        "          dicTable[\"TABLEID\"] = getMaiorSimilaridade(dicTable, listTablesInfoGT)\n",
        "\n",
        "        listFileGT = getGTInfo(dicTable[\"TABLEID\"], noExtension, page, GT_LAB_OUT)\n",
        "        listTableInfoGT = getListTablesInfo(GT_LAB_OUT,listFileGT)\n",
        "        print(\"TABLEID de maior similaridade = \", dicTable[\"TABLEID\"])\n",
        "\n",
        "        #gravar o arquivo INFO da tabela para comparacao com GT\n",
        "        #encontrou tabela identica ou similar\n",
        "        if dicTable[\"TABLEID\"] != \"TBD\" and dicTable[\"TABLEID\"] != \"\":\n",
        "          filePath = labDirOut + dicTable[\"TABLEID\"] + \"|\" + noExtension + \"|\" + str(page) + \"_INFO.info\"\n",
        "          print(\"gerando arquivo INFO de resultado - SUCESSO: \", filePath)\n",
        "          SaveDicTableInfo(filePath, dicTable)\n",
        "        else:\n",
        "          #no caso de nao ter encontrado tabela similar ao GT para comparação, registrar arquivo de erro\n",
        "          filePath = labDirOut + noExtension + \"|\" + str(page) + \"|\" + \"_INFO_ERRO.error\"\n",
        "          dicTable[\"OBS\"] = \"ERRO - TABLEID NÃO ENCONTRADO\"\n",
        "          print(\"gerando arquivo INFO de resultado - ERRO: \", filePath)\n",
        "          SaveDicTableInfo(filePath, dicTable)\n",
        "\n",
        "        #modelo do dicmetada\n",
        "        dicMetaData = {\n",
        "        \"filename\": 0,\n",
        "        \"split\": \"train\",\n",
        "        \"imgid\": \"\",\n",
        "        \"html\": {\n",
        "          \"cells\": 0,\n",
        "          \"structure\": 0\n",
        "                }\n",
        "        }\n",
        "\n",
        "        #inicializando a lista\n",
        "        dicMetaData[\"filename\"] = path_pdf_in\n",
        "        dicMetaData[\"imgid\"] = dicTable[\"TABLEID\"]\n",
        "\n",
        "        #gravando no arquivo as informacoes de METADADOS, BBOX E HTML\n",
        "        lstCells = []\n",
        "        lstStructure = []\n",
        "\n",
        "        #print(\"df = \", df)\n",
        "        #print(\"df shape \", df.shape)\n",
        "        #print(\"df.at[0,0] \", df.at[0,0])\n",
        "        #carrega e concatena a lista de tokens das celulas e bbox calculando como referencia as coordenadas da tabela principal\n",
        "        lstCells.extend(noteListTokensBbox(None, df, None)) # no futuro alterar para inserir bbox\n",
        "        lstStructure.extend(noteTokensHTML(dfOriginal)) #carrega e concatena a lista de tokens html (df original sem ajustes)\n",
        "        dicMetaData[\"html\"][\"cells\"] = lstCells\n",
        "        dicMetaData[\"html\"][\"structure\"] = lstStructure\n",
        "\n",
        "        print(\"Gravando Metadados: \", DIROUT_TABLENET)\n",
        "        saveAnnotationFile(dicMetaData, DIROUT_TABLENET , page)\n",
        "        saveElementMetadata(dicMetaData, \"BBOX\", DIROUT_TABLENET, page)\n",
        "        saveElementMetadata(dicMetaData, \"HTML\", DIROUT_TABLENET, page)\n",
        "        saveElementMetadata(dicMetaData, \"HTML_PRETTY\", DIROUT_TABLENET, page)\n",
        "\n",
        "        #break #end for lstDF\n",
        "\n",
        "      #end if temos paginas para processar\n",
        "\n",
        "    #break # end for pages\n",
        "\n",
        "  #break # end for files\n",
        "\n",
        "#removendo arquivos BMP\n",
        "print(\"Removendo arquivos png temporários...\")\n",
        "deleteFiles(labDirOut, \"png\")\n",
        "\n",
        "# Obter a data e hora corrente\n",
        "data_e_hora_corrente = datetime.now(fuso_horario_brasilia)\n",
        "# Formatar a data e hora corrente para o formato desejado\n",
        "data_e_hora_formatadas = data_e_hora_corrente.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "#MAXFILES = 1 #apenas para testes, delimitar a quantidade de certificados a ler\n",
        "print('FIM DO PROCESSAMENTO ', data_e_hora_formatadas)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l6-skfIb6Oyg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzvzaxFyAMLn"
      },
      "source": [
        "# TEDS - CALCULO PARA O MODELO\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kpNXzMBXOqs"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install distance\n",
        "!pip install apted\n",
        "!pip install lxml\n",
        "!pip install tqdm\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4bM2faDBABZ"
      },
      "outputs": [],
      "source": [
        "import distance\n",
        "from apted import APTED, Config\n",
        "from apted.helpers import Tree\n",
        "from lxml import etree, html\n",
        "from collections import deque\n",
        "#from parallel import parallel_process\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TableTree(Tree):\n",
        "    def __init__(self, tag, colspan=None, rowspan=None, content=None, *children):\n",
        "        self.tag = tag\n",
        "        self.colspan = colspan\n",
        "        self.rowspan = rowspan\n",
        "        self.content = content\n",
        "        self.children = list(children)\n",
        "\n",
        "    def bracket(self):\n",
        "        \"\"\"Show tree using brackets notation\"\"\"\n",
        "        if self.tag == 'td':\n",
        "            result = '\"tag\": %s, \"colspan\": %d, \"rowspan\": %d, \"text\": %s' % \\\n",
        "                     (self.tag, self.colspan, self.rowspan, self.content)\n",
        "        else:\n",
        "            result = '\"tag\": %s' % self.tag\n",
        "        for child in self.children:\n",
        "            result += child.bracket()\n",
        "        return \"{{{}}}\".format(result)\n",
        "\n",
        "\n",
        "class CustomConfig(Config):\n",
        "    @staticmethod\n",
        "    def maximum(*sequences):\n",
        "        \"\"\"Get maximum possible value\n",
        "        \"\"\"\n",
        "        return max(map(len, sequences))\n",
        "\n",
        "    def normalized_distance(self, *sequences):\n",
        "        \"\"\"Get distance from 0 to 1\n",
        "        \"\"\"\n",
        "        return float(distance.levenshtein(*sequences)) / self.maximum(*sequences)\n",
        "\n",
        "    def rename(self, node1, node2):\n",
        "        \"\"\"Compares attributes of trees\"\"\"\n",
        "        if (node1.tag != node2.tag) or (node1.colspan != node2.colspan) or (node1.rowspan != node2.rowspan):\n",
        "            return 1.\n",
        "        if node1.tag == 'td':\n",
        "            if node1.content or node2.content:\n",
        "                return self.normalized_distance(node1.content, node2.content)\n",
        "        return 0.\n",
        "\n",
        "\n",
        "class TEDS(object):\n",
        "    ''' Tree Edit Distance basead Similarity\n",
        "    '''\n",
        "    def __init__(self, structure_only=False, n_jobs=1, ignore_nodes=None):\n",
        "        assert isinstance(n_jobs, int) and (n_jobs >= 1), 'n_jobs must be an integer greather than 1'\n",
        "        self.structure_only = structure_only\n",
        "        self.n_jobs = n_jobs\n",
        "        self.ignore_nodes = ignore_nodes\n",
        "        self.__tokens__ = []\n",
        "\n",
        "    def tokenize(self, node):\n",
        "        ''' Tokenizes table cells\n",
        "        '''\n",
        "        self.__tokens__.append('<%s>' % node.tag)\n",
        "        if node.text is not None:\n",
        "            self.__tokens__ += list(node.text)\n",
        "        for n in node.getchildren():\n",
        "            self.tokenize(n)\n",
        "        if node.tag != 'unk':\n",
        "            self.__tokens__.append('</%s>' % node.tag)\n",
        "        if node.tag != 'td' and node.tail is not None:\n",
        "            self.__tokens__ += list(node.tail)\n",
        "\n",
        "    def load_html_tree(self, node, parent=None):\n",
        "        ''' Converts HTML tree to the format required by apted\n",
        "        '''\n",
        "        global __tokens__\n",
        "        if node.tag == 'td':\n",
        "            if self.structure_only:\n",
        "                cell = []\n",
        "            else:\n",
        "                self.__tokens__ = []\n",
        "                self.tokenize(node)\n",
        "                cell = self.__tokens__[1:-1].copy()\n",
        "            new_node = TableTree(node.tag,\n",
        "                                 int(node.attrib.get('colspan', '1')),\n",
        "                                 int(node.attrib.get('rowspan', '1')),\n",
        "                                 cell, *deque())\n",
        "        else:\n",
        "            new_node = TableTree(node.tag, None, None, None, *deque())\n",
        "        if parent is not None:\n",
        "            parent.children.append(new_node)\n",
        "        if node.tag != 'td':\n",
        "            for n in node.getchildren():\n",
        "                self.load_html_tree(n, new_node)\n",
        "        if parent is None:\n",
        "            return new_node\n",
        "\n",
        "    def evaluate(self, pred, true):\n",
        "        ''' Computes TEDS score between the prediction and the ground truth of a\n",
        "            given sample\n",
        "        '''\n",
        "        if (not pred) or (not true):\n",
        "            return 0.0\n",
        "        parser = html.HTMLParser(remove_comments=True, encoding='utf-8')\n",
        "        pred = html.fromstring(pred, parser=parser)\n",
        "        true = html.fromstring(true, parser=parser)\n",
        "        if pred.xpath('body/table') and true.xpath('body/table'):\n",
        "            pred = pred.xpath('body/table')[0]\n",
        "            true = true.xpath('body/table')[0]\n",
        "            if self.ignore_nodes:\n",
        "                etree.strip_tags(pred, *self.ignore_nodes)\n",
        "                etree.strip_tags(true, *self.ignore_nodes)\n",
        "            n_nodes_pred = len(pred.xpath(\".//*\"))\n",
        "            n_nodes_true = len(true.xpath(\".//*\"))\n",
        "            n_nodes = max(n_nodes_pred, n_nodes_true)\n",
        "            tree_pred = self.load_html_tree(pred)\n",
        "            tree_true = self.load_html_tree(true)\n",
        "            distance = APTED(tree_pred, tree_true, CustomConfig()).compute_edit_distance()\n",
        "            return 1.0 - (float(distance) / n_nodes)\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def batch_evaluate(self, pred_json, true_json):\n",
        "        ''' Computes TEDS score between the prediction and the ground truth of\n",
        "            a batch of samples\n",
        "            @params pred_json: {'FILENAME': 'HTML CODE', ...}\n",
        "            @params true_json: {'FILENAME': {'html': 'HTML CODE'}, ...}\n",
        "            @output: {'FILENAME': 'TEDS SCORE', ...}\n",
        "        '''\n",
        "        samples = true_json.keys()\n",
        "        if self.n_jobs == 1:\n",
        "            scores = [self.evaluate(pred_json.get(filename, ''), true_json[filename]['html']) for filename in tqdm(samples)]\n",
        "        else:\n",
        "            inputs = [{'pred': pred_json.get(filename, ''), 'true': true_json[filename]['html']} for filename in samples]\n",
        "            scores = parallel_process(inputs, self.evaluate, use_kwargs=True, n_jobs=self.n_jobs, front_num=1)\n",
        "        scores = dict(zip(samples, scores))\n",
        "        return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-YYbwXFBGIR"
      },
      "source": [
        "# 5 - Função MAIN para gerar estatísticas (ARQUIVOS GT VERSUS TABLENET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a729lKKkBKKj",
        "outputId": "e1bc3e54-1587-43e9-dce9-b6b975431459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#montando o caminho para leitura dos arquivos (certificados, imagens, planilhas, etc)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUO8IzYDBKi8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "#coletar as informacoes da tabela do arquivo _INFO e retornar para uma lista\n",
        "def getListTablesInfo(path, listFiles):\n",
        "\n",
        "  listTablesInfo = []\n",
        "  for fileName in listFiles:\n",
        "    with open(path + fileName, 'r') as file:\n",
        "      conteudo = file.read()\n",
        "      listTablesInfo.append(eval(conteudo))\n",
        "\n",
        "  return listTablesInfo\n",
        "\n",
        "def sortList(lst, len):\n",
        "    def personList(item):\n",
        "        # Extrai o número após 'CTM' e converte para inteiro\n",
        "        return int(item[len:])\n",
        "\n",
        "    return sorted(lst, key=personList)\n",
        "\n",
        "#coletar arquivo de acordo com premissas (prefixo e sufixo)\n",
        "def getFileByPrefix(path, prefix, sufix):\n",
        "\n",
        "  for fileName in os.listdir(path):\n",
        "      if prefix in fileName and fileName.endswith(sufix):\n",
        "          return fileName\n",
        "  return \"\"\n",
        "\n",
        "def getFiles(folderDir, ext):\n",
        "  # Construir o padrão de busca usando a extensão fornecida\n",
        "  pattern = os.path.join(folderDir, f\"*.{ext}\")\n",
        "\n",
        "  files = []\n",
        "  arrPath = folderDir.split(\"/\")\n",
        "\n",
        "  #print(\"arrPath \", arrPath)\n",
        "  if len(arrPath) >0 and len(arrPath[len(arrPath)-1].split(\"_\")) >0:\n",
        "    #print(folderDir)\n",
        "    lab = arrPath[len(arrPath)-1].split(\"_\")[2]\n",
        "    # Usar a função glob para encontrar os arquivos correspondentes ao padrão\n",
        "    files = glob.glob(pattern)\n",
        "\n",
        "  # Retornar a lista de arquivos encontrados\n",
        "  return sorted(files)\n",
        "\n",
        "def getInfoFiles(list):\n",
        "\n",
        "  lstInfoFiles = []\n",
        "  for path in list:\n",
        "\n",
        "    arrPath = path.split(\"/\")\n",
        "    file = arrPath[len(arrPath)-1]\n",
        "    tableId = file.split(\"|\")[0]\n",
        "    fileName = file.split(\"|\")[1]\n",
        "    page = file.split(\"|\")[2].split(\"_\")[0]\n",
        "\n",
        "    lstInfoFiles.append({\"TABLEID\":tableId, \"TABLEID\":tableId, \"FILE\":fileName, \"PAGE\":page})\n",
        "\n",
        "  return lstInfoFiles\n",
        "\n",
        "#files = getFiles(\"/content/drive/MyDrive/DataSets/Certificados/Out/img2table/LAB_01_CTM\", \"info\")\n",
        "#lstInfoFiles = getInfoFiles (files)\n",
        "\n",
        "#funcao que compara o valor de duas listas e calcula a media do percentual de similaridade entre eles\n",
        "#(para calcular o valor do bbox das tabelas e células das tabelas)\n",
        "def calcPercSimValueLists(lista1, lista2):\n",
        "  if len(lista1) != len(lista2):\n",
        "      print(\"calcPercSimValueLists, listas de tamanhos diferentes, lista1=\",lista1,\"/ lista2 = \",lista2)\n",
        "      raise ValueError(\"As listas devem ter o mesmo comprimento.\")\n",
        "\n",
        "  percSim = [ (1 / (1 + (abs(num1 - num2)))) * 100 for num1, num2 in zip(lista1, lista2)]\n",
        "  #print(\"percSim \", percSim)\n",
        "  #print(\"result percSim \", sum(percSim) / len (percSim))\n",
        "  return sum(percSim) / len (percSim)\n",
        "\n",
        "#(para calcular o percentual de similaridade entre dois números\n",
        "def calcPercSimValueNums(num1, num2):\n",
        "\n",
        "  percSim = (1 / (1 + (abs(num1 - num2)))) * 100\n",
        "  #print (\" similaridade entre os numeros {0} e {1}: {2}\".format(num1, num2, percSim))\n",
        "  return percSim\n",
        "\n",
        "#similaridade de strings conhecido como \"Distância de Levenshtein\"\n",
        "def calcPercSimStrings(str1, str2):\n",
        "\n",
        "  #retirando quebra de linhas da string\n",
        "  str1 = str1.replace(\"\\n\", \" \")\n",
        "  str2 = str2.replace(\"\\n\", \" \")\n",
        "\n",
        "  tamanho_str1 = len(str1)\n",
        "  tamanho_str2 = len(str2)\n",
        "\n",
        "  matriz = [[0] * (tamanho_str2 + 1) for _ in range(tamanho_str1 + 1)]\n",
        "\n",
        "  for i in range(tamanho_str1 + 1):\n",
        "    matriz[i][0] = i\n",
        "\n",
        "  for j in range(tamanho_str2 + 1):\n",
        "    matriz[0][j] = j\n",
        "\n",
        "  for i in range(1, tamanho_str1 + 1):\n",
        "    for j in range(1, tamanho_str2 + 1):\n",
        "        if str1[i - 1] == str2[j - 1]:\n",
        "            custo_substituicao = 0\n",
        "        else:\n",
        "            custo_substituicao = 1\n",
        "        matriz[i][j] = min(matriz[i - 1][j] + 1,       # Deletar\n",
        "                            matriz[i][j - 1] + 1,       # Inserir\n",
        "                              matriz[i - 1][j - 1] + custo_substituicao)  # Substituir\n",
        "\n",
        "  distancia = matriz[tamanho_str1][tamanho_str2]\n",
        "  maximo_tamanho = max(tamanho_str1, tamanho_str2)\n",
        "\n",
        "  similaridade = 0\n",
        "  if maximo_tamanho > 0:\n",
        "    similaridade = (maximo_tamanho - distancia) / maximo_tamanho\n",
        "  #print (\" similaridade entre {0} e {1}: {2}\".format(str1, str2, similaridade * 100))\n",
        "  return similaridade * 100\n",
        "\n",
        "#numero de ocorrencias de um numero em uma lista\n",
        "def numTimes(list, num):\n",
        "\n",
        "  cont = 0\n",
        "  for valor in list:\n",
        "    if valor == num:\n",
        "      cont+=1\n",
        "\n",
        "  return cont\n",
        "\n",
        "#numero de ocorrencias de um numero ser maior ou igual que um numero\n",
        "def numTimesMoreThen(list, num):\n",
        "\n",
        "  cont = 0\n",
        "  for valor in list:\n",
        "    if valor >= num and valor <100:\n",
        "      cont+=1\n",
        "\n",
        "  return cont\n",
        "\n",
        "def isDecimal(valor):\n",
        "  try:\n",
        "      float(valor)\n",
        "      return True\n",
        "  except ValueError:\n",
        "      return False\n",
        "\n",
        "#calcular similaridades em valores das celulas bbox e tokens de duas listas de tabelas\n",
        "def calStatsTablesValues(lstTable1, lstTable2):\n",
        "\n",
        "  lstResTokens = []\n",
        "  lstResBbox = []\n",
        "  for item1, item2 in zip(lstTable1, lstTable2):\n",
        "    listaBbox1 = item1[\"bbox\"]\n",
        "    listaBbox2 = item2[\"bbox\"]\n",
        "    str1 = \"\".join(item1[\"tokens\"])\n",
        "    str2 = \"\".join(item2[\"tokens\"])\n",
        "\n",
        "    if len(listaBbox1) >0 and len(listaBbox2) >0:\n",
        "      lstResBbox.append(round(calcPercSimValueLists(listaBbox1, listaBbox2),2))\n",
        "    else:\n",
        "      lstResBbox.append(0)\n",
        "\n",
        "    strDec1 = str1.replace(\",\", \".\").strip()\n",
        "    strDec2 = str2.replace(\",\", \".\").strip()\n",
        "    #se os valores forem numeros converter para float para calcular similiaridade com maior exatidao\n",
        "    if (isDecimal(strDec1) and isDecimal(strDec2)):\n",
        "      lstResTokens.append( round( calcPercSimValueNums(float(strDec1), float(strDec2) ),2) )\n",
        "    #no caso de string\n",
        "    else:\n",
        "      lstResTokens.append(round(calcPercSimStrings(str1, str2),2))\n",
        "\n",
        "  return lstResTokens, lstResBbox\n",
        "\n",
        "#calcular quantidade de valores não lidos pelo modelo (NAN ou 999999)\n",
        "def calStatsNAN(lstTable):\n",
        "\n",
        "  qtdNANToken = 0\n",
        "  qtdNANBbox = 0\n",
        "\n",
        "  for item in lstTable:\n",
        "    listaBbox = item[\"bbox\"]\n",
        "    token = \"\".join(item[\"tokens\"])\n",
        "\n",
        "    if numTimes(listaBbox, 999999) ==4:\n",
        "      qtdNANBbox+=1\n",
        "\n",
        "    if token == \"NAN\":\n",
        "      qtdNANToken+=1\n",
        "\n",
        "  return qtdNANToken, qtdNANBbox\n",
        "\n",
        "def removeSpecialChars(strTexto):\n",
        "  # Remover os caracteres especiais\n",
        "  speChars = [\"\\\\\", ]\n",
        "\n",
        "  strTexto = strTexto.replace(speChars, \"\")\n",
        "\n",
        "  return strTexto\n",
        "\n",
        "def readFile(filePath):\n",
        "  try:\n",
        "    with open(filePath, 'r') as arquivo:\n",
        "        conteudo = arquivo.read()\n",
        "        conteudo = conteudo.replace(\"[nan, nan, nan, nan]\", \"[999999,999999,999999,999999]\")\n",
        "        #conteudo = conteudo.replace(\"\\'\", \"\")\n",
        "        conteudo = conteudo.replace(\"\\n\", \" \")\n",
        "    return conteudo\n",
        "  except FileNotFoundError:\n",
        "    print(f'O arquivo \"{filePath}\" não foi encontrado.')\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print(f'Ocorreu um erro ao ler o arquivo: {e}')\n",
        "    return None\n",
        "\n",
        "def SaveFileStats (filePath, dicTableStats):\n",
        "\n",
        "  strFile = json.dumps(dicTableStats)\n",
        "  strFile = strFile.replace(\",\",\",\\n\")\n",
        "\n",
        "  print(\"Salvando arquivo de estatísticas: \",filePath)\n",
        "  with open(filePath, 'w') as arquivo:\n",
        "    arquivo.write(strFile)\n",
        "\n",
        "def strInDic(dicionario, string):\n",
        "\n",
        "  for chave, valor in dicionario.items():\n",
        "      if isinstance(valor, str) and string in valor:\n",
        "          return True\n",
        "  return False\n",
        "\n",
        "def strInList(lstDic, string):\n",
        "\n",
        "  for dic in lstDic:\n",
        "    for chave, valor in dic.items():\n",
        "        if isinstance(valor, str) and string == valor:\n",
        "            return True\n",
        "  return False\n",
        "\n",
        "def ExportCSVSummary(ToolPath, GTPath):\n",
        "  #coletando os diretorios dos laboratorios\n",
        "  dirLabs = [nome for nome in os.listdir(ToolPath)]\n",
        "\n",
        "  listStats = []\n",
        "  filesSTATS = []\n",
        "\n",
        "  lstTableIdGT = []\n",
        "  #colentando os arquivos de statisticas dos laboratorios\n",
        "  for dirLab in dirLabs:\n",
        "\n",
        "    labPath =  ToolPath + dirLab\n",
        "    filesSTATS = getFiles(labPath, \"stats\")\n",
        "\n",
        "    #print(\"filesSTATS \", filesSTATS)\n",
        "    #coletando arquivos de estatisticas\n",
        "    for fileSTATS in filesSTATS:\n",
        "      dicStats = ast.literal_eval(readFile(fileSTATS))\n",
        "      listStats.append(dicStats)\n",
        "\n",
        "    #coletando os tablesID do GT para comparacao\n",
        "    filesGTInfo = GTPath + dirLab\n",
        "    filesInfo = getFiles(filesGTInfo, \"info\")\n",
        "\n",
        "    #print(\"filesInfo \", filesInfo)\n",
        "\n",
        "    for fileInfo in filesInfo:\n",
        "      arrFile = fileInfo.split(\"/\")\n",
        "      tableId = arrFile[len(arrFile)-1].split(\"|\")[0]\n",
        "      lstTableIdGT.append(dirLab+\"|\"+tableId)\n",
        "\n",
        "  lstErros = []\n",
        "\n",
        "  for item in lstTableIdGT:\n",
        "\n",
        "    #print(item)\n",
        "    lab = item.split(\"|\")[0]\n",
        "    tableId = item.split(\"|\")[1]\n",
        "\n",
        "    if not strInList(listStats, tableId): #não encontrou, adicionar ao erro\n",
        "\n",
        "      print(\"Não encontrou TABLEID \", tableId, \", adicionando....\")\n",
        "      #print(\"GTPath \", GTPath)\n",
        "      #print(\"lab \", lab)\n",
        "      #print(\"tableId \", tableId)\n",
        "      #print(\"fileInfo \", fileInfo)\n",
        "\n",
        "      #coletando informacoes do statsInfo\n",
        "      #print(\"parametros a carregar na funcao getFileByPrefix\", GTPath + lab, tableId+\"|\", \"info\")\n",
        "      fileInfo = getFileByPrefix(GTPath + lab, tableId+\"|\", \"info\")\n",
        "      #pathInfo = GTPath + dirLab + fileInfo\n",
        "      lstFile = [fileInfo]\n",
        "      #print(\"parametros a carregar na funcao getListTablesInfo\", GTPath + lab + \"/\", lstFile)\n",
        "      lstInfo = getListTablesInfo(GTPath + lab + \"/\", lstFile)\n",
        "\n",
        "      dicTableStats = {}\n",
        "      dicTableStats[\"LAB\"] = lstInfo[0][\"LAB\"]\n",
        "      dicTableStats[\"FILE\"] = lstInfo[0][\"FILE\"]\n",
        "      dicTableStats[\"PAGE\"] = lstInfo[0][\"PAGE\"]\n",
        "      dicTableStats[\"TABLEID\"] = lstInfo[0][\"TABLEID\"]\n",
        "      dicTableStats[\"DIMENSION\"] = lstInfo[0][\"DIMENSION\"]\n",
        "      dicTableStats[\"QTDCELLS\"] = 0\n",
        "      dicTableStats[\"QTDACERTOSCELLS\"] = 0\n",
        "      dicTableStats[\"PERCACERTOSCELLS\"] = 0\n",
        "      dicTableStats[\"PERCACERTOSBBOX\"] = 0\n",
        "      dicTableStats[\"QTDNAOLIDOSBBOX\"] = 0\n",
        "      dicTableStats[\"PERCNAOLIDOSBBOX\"] = 0\n",
        "      dicTableStats[\"QTDNAOLIDOSTOKEN\"] = 0\n",
        "      dicTableStats[\"PERCNAOLIDOSTOKEN\"] = 0\n",
        "      dicTableStats[\"TEDS\"] = 0\n",
        "\n",
        "      lstErros.append(dicTableStats)\n",
        "\n",
        "  lstTotal = listStats + lstErros\n",
        "\n",
        "  dtStats = pd.DataFrame(lstTotal)\n",
        "  dtStatsORD = dtStats.sort_values(by=['LAB','FILE', 'PAGE'])\n",
        "  #print(dtStatsORD)\n",
        "  print(\"Arquivo de sumário gerado \", ToolPath + \"Summary.xlsx\")\n",
        "  dtStatsORD.to_excel(ToolPath + \"Summary.xlsx\", index=False)  # index=False para não incluir o índice do DataFrame\n",
        "  return dtStatsORD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dO5dLfuBMpM"
      },
      "outputs": [],
      "source": [
        "#gerando estatísticas\n",
        "\n",
        "import os\n",
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "#diretorio do img2tableDir\n",
        "Tablenet = \"/content/drive/MyDrive/DataSets/Certificados/Out/TableNet/\"\n",
        "#diretorio de referencia GT\n",
        "GTDir = \"/content/drive/MyDrive/DataSets/Certificados/Out/GT/\"\n",
        "\n",
        "#coletando os diretorios dos laboratorios\n",
        "#dirLabs = [nome for nome in os.listdir(pubTables)]\n",
        "\n",
        "dirLabs = [] # informe aqui a relacao de pastas dos laboratorios\n",
        "\n",
        "for dirLab in dirLabs:\n",
        "\n",
        "  if not os.path.isfile(dirLab):\n",
        "    labPath =  Tablenet + dirLab\n",
        "    labPathGT =  GTDir + dirLab\n",
        "    #coleta os arquivos info para analise dos tablesID\n",
        "    filesINFO = getFiles(labPath, \"info\")\n",
        "    #coleta os tablesID para comparacao\n",
        "    lstInfoFiles = getInfoFiles(filesINFO)\n",
        "\n",
        "    #para cada tableID, gerar estatísticas\n",
        "    for dicInfoFile in lstInfoFiles:\n",
        "\n",
        "      print(\"dicInfoFile\", dicInfoFile)\n",
        "      fileInfo = dicInfoFile[\"TABLEID\"] + \"|\" + dicInfoFile[\"FILE\"] + \"|\" + dicInfoFile[\"PAGE\"] + \"_INFO.info\"\n",
        "      fileTokenBbox = dicInfoFile[\"TABLEID\"] + \"|\" + dicInfoFile[\"FILE\"] + \"|\" + dicInfoFile[\"PAGE\"] + \"_BBOX.bbox\"\n",
        "      fileHTML = dicInfoFile[\"TABLEID\"] + \"|\" + dicInfoFile[\"FILE\"] + \"|\" + dicInfoFile[\"PAGE\"] + \"_HTML.html\"\n",
        "\n",
        "      pathFileInfo = labPath + \"/\" + fileInfo\n",
        "      pathFileTokenBbox = labPath + \"/\" + fileTokenBbox\n",
        "      pathFileHTML = labPath + \"/\" + fileHTML\n",
        "\n",
        "      pathFileInfoGT = labPathGT + \"/\" + fileInfo\n",
        "      pathFileTokenBboxGT = labPathGT + \"/\" + fileTokenBbox\n",
        "      pathFileHTMLGT = labPathGT + \"/\" + fileHTML\n",
        "      print(\"pathFileInfo =\", pathFileInfo)\n",
        "\n",
        "      #gerando estatisticas do token e bbox\n",
        "      if os.path.exists(pathFileHTMLGT) and os.path.exists(pathFileTokenBbox) and os.path.exists(pathFileTokenBboxGT):\n",
        "\n",
        "        #carrega lista de tokens/bbox e HTMLs para comparacao do arquivo corrente com GT\n",
        "        #arquivo info GT (.info)\n",
        "        lstInfo = ast.literal_eval(readFile(pathFileInfoGT))\n",
        "        #arquivo bbox (.bbox)\n",
        "        #print(\"pathFileTokenBbox\", pathFileTokenBbox)\n",
        "        lstTkBox = ast.literal_eval(readFile(pathFileTokenBbox))\n",
        "        #arquivo bbox GT (.bbox)\n",
        "        lstTkBoxGT = ast.literal_eval(readFile(pathFileTokenBboxGT))\n",
        "        #arquivo html (.html)\n",
        "        #print(\"pathFileHTML \", pathFileHTML)\n",
        "        strHTML = \"\".join(ast.literal_eval(readFile(pathFileHTML).replace(\"\\n\", \" \")))\n",
        "        #arquivo html GT(.html)\n",
        "        strHTMLGT = \"\".join(ast.literal_eval(readFile(pathFileHTMLGT).replace(\"\\n\", \" \")))\n",
        "        #break\n",
        "\n",
        "        #TEDS apenas funciona se tiver na estrutura html as tags html e body\n",
        "        if \"<body>\" not in strHTML:\n",
        "          strHTML = \"<body>\" + strHTML + \"</body>\"\n",
        "        if \"<html>\" not in strHTML:\n",
        "          strHTML = \"<html>\" + strHTML + \"</html>\"\n",
        "        if \"<body>\" not in strHTMLGT:\n",
        "          strHTMLGT = \"<body>\" + strHTMLGT + \"</body>\"\n",
        "        if \"<html>\" not in strHTMLGT:\n",
        "          strHTMLGT = \"<html>\" + strHTMLGT + \"</html>\"\n",
        "\n",
        "        qtdLinhas = int(lstInfo[\"DIMENSION\"].split(\"X\")[0])\n",
        "        qtdColunas = int(lstInfo[\"DIMENSION\"].split(\"X\")[1])\n",
        "        qtdCells = qtdLinhas * qtdColunas\n",
        "\n",
        "        #print(\"lstTkBox\",lstTkBox)\n",
        "        #print(\"lstTkBoxGT\",lstTkBoxGT)\n",
        "        lstStatsTokens, lstStatsBbox = calStatsTablesValues(lstTkBox, lstTkBoxGT)\n",
        "\n",
        "        qtdNANToken, qtdNANBbox = calStatsNAN(lstTkBox)\n",
        "\n",
        "        #calcula qtd de acertos (com similaridade 100% entre os valores dos tokens e tabelas)\n",
        "        qtdAcertosTokens = numTimes(lstStatsTokens, 100.0)\n",
        "        #qtdAcertosBbox = numTimes(lstStatsBbox, 100.0)\n",
        "        percAcertosBbox = round((sum(lstStatsBbox) / len(lstStatsBbox))/100, 2)\n",
        "\n",
        "        #calcula similidade maior que 95%\n",
        "        qtdTokensMaior95 = numTimesMoreThen(lstStatsTokens, 95)\n",
        "        qtdBboxMaior95 = numTimesMoreThen(lstStatsBbox, 95)\n",
        "\n",
        "        #calcula TEDS entre as estruturas HTMLs (img2table VS GT)\n",
        "        teds = TEDS()\n",
        "\n",
        "        scoreTEDS = round(teds.evaluate(strHTML, strHTMLGT), 6)\n",
        "\n",
        "        #salvado arquivo de estatística\n",
        "        fileStats = dicInfoFile[\"TABLEID\"] + \"|\" + dicInfoFile[\"FILE\"] + \"|\" + dicInfoFile[\"PAGE\"] + \"_STATS.stats\"\n",
        "        pathFileStats = labPath + \"/\" + fileStats\n",
        "\n",
        "        percAcertosTokens = round((qtdAcertosTokens / qtdCells), 2)\n",
        "        percNaoLidosBbox = round((qtdNANBbox / qtdCells), 2)\n",
        "        percNaoLidosToken = round((qtdNANToken / qtdCells), 2)\n",
        "\n",
        "        dicTableStats = {}\n",
        "        dicTableStats[\"LAB\"] = lstInfo[\"LAB\"]\n",
        "        dicTableStats[\"FILE\"] = dicInfoFile[\"FILE\"]\n",
        "        dicTableStats[\"PAGE\"] = dicInfoFile[\"PAGE\"]\n",
        "        dicTableStats[\"TABLEID\"] = dicInfoFile[\"TABLEID\"]\n",
        "        dicTableStats[\"DIMENSION\"] = lstInfo[\"DIMENSION\"]\n",
        "        dicTableStats[\"QTDCELLS\"] = qtdCells\n",
        "        dicTableStats[\"QTDACERTOSCELLS\"] = qtdAcertosTokens\n",
        "        dicTableStats[\"PERCACERTOSCELLS\"] = percAcertosTokens\n",
        "        dicTableStats[\"PERCACERTOSBBOX\"] = percAcertosBbox\n",
        "        dicTableStats[\"QTDNAOLIDOSBBOX\"] = qtdNANBbox\n",
        "        dicTableStats[\"PERCNAOLIDOSBBOX\"] = percNaoLidosBbox\n",
        "        dicTableStats[\"QTDNAOLIDOSTOKEN\"] = qtdNANToken\n",
        "        dicTableStats[\"PERCNAOLIDOSTOKEN\"] = percNaoLidosToken\n",
        "        #dicTableStats[\"QTDCELLSMAIOR95\"] = qtdTokensMaior95\n",
        "        #dicTableStats[\"QTDABBOXMAIOR95\"] = qtdBboxMaior95\n",
        "        dicTableStats[\"TEDS\"] = scoreTEDS\n",
        "\n",
        "        SaveFileStats (pathFileStats, dicTableStats)\n",
        "\n",
        "        print(\"Dimensão da tabela \", lstInfo[\"DIMENSION\"], \", total \", str(qtdCells), \" células\" )\n",
        "        print(\"qtdCells:\",qtdCells)\n",
        "        print(\"qtdAcertosTokens:\",qtdAcertosTokens)\n",
        "        print(\"percAcertosTokens:\",percAcertosTokens,\"/\",(percAcertosTokens*100),\"%\")\n",
        "        print(\"percAcertosBbox:\",percAcertosBbox,\"/\",(percAcertosBbox*100),\"%\")\n",
        "        print(\"qtdNaoLidosBbox:\",qtdNANBbox)\n",
        "        print(\"percNaoLidosBbox:\",percNaoLidosBbox,\"/\",(percNaoLidosBbox*100),\"%\")\n",
        "        print(\"qtdNaoLidosTokens:\",qtdNANToken)\n",
        "        print(\"percNaoLidosToken:\",percNaoLidosToken,\"/\",(percNaoLidosToken*100),\"%\")\n",
        "        #print(\"qtdAcertosTokens>95:\",qtdTokensMaior95)\n",
        "        #print(\"qtdAcertosBbox>95:\",qtdBboxMaior95)\n",
        "        print('TEDS score:', scoreTEDS,\"/\",round((scoreTEDS*100),2),\"%\")\n",
        "        #break # fim primeiro for\n",
        "\n",
        "#gerar arquivos de estatística, montar sumário no EXCEL\n",
        "ExportCSVSummary(pubTables, GTDir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yyDKDvDKGsAb",
        "4VFVby8Q0Xrr"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}